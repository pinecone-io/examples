{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLLl1Agum8O"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb)\n",
        "\n",
        "# GPT4 with Retrieval Augmentation over LangChain Docs\n",
        "\n",
        "[![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/fast-link.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs-fast.ipynb)\n",
        "\n",
        "In this notebook we'll work through an example of using GPT-4 with retrieval augmentation to answer questions about the LangChain Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_HDKlQO5svqI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  tiktoken==0.4.0 \\\n",
        "  openai==0.27.7 \\\n",
        "  langchain==0.0.179 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUEJ6vDum8q"
      },
      "source": [
        "In this example, we will download the LangChain docs from [langchain.readthedocs.io/](https://langchain.readthedocs.io/latest/en/). We get all `.html` files located on the site like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo9gYhGPr_DQ",
        "outputId": "7c4f766c-e70d-41c1-ace6-c1a4f10d9d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2023-05-25 02:44:06 (267 MB/s) - â€˜rtdocs/python.langchain.com/en/latest/use_cases/question_answering.htmlâ€™ saved [101116]\n",
            "\n",
            "...\n",
            "--2023-05-25 02:45:44--  https://python.langchain.com/en/latest/tracing/agent_with_tracing.html\n",
            "Reusing existing connection to python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/python.langchain.com/en/latest/tracing/agent_with_tracing.htmlâ€™\n",
            "\n",
            "python.langchain.co     [ <=>                ] 111.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-25 02:45:45 (242 MB/s) - â€˜rtdocs/python.langchain.com/en/latest/tracing/agent_with_tracing.htmlâ€™ saved [114507]\n",
            "\n",
            "FINISHED --2023-05-25 02:45:45--\n",
            "Total wall clock time: 2m 58s\n",
            "Downloaded: 892 files, 95M in 1.6s (60.6 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget -r -A.html -P rtdocs https://python.langchain.com/en/latest/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jKOV-6N4vaRl"
      },
      "source": [
        "This downloads all HTML into the `rtdocs` directory. Now we can use LangChain itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n40_0MtlsKgM",
        "outputId": "ac43dcec-83ec-4b79-e1e1-e82d4418daf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "891"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "\n",
        "loader = ReadTheDocsLoader('rtdocs')\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ahFEI4U3vdxV"
      },
      "source": [
        "This leaves us with `891` processed doc pages. Let's take a look at the format each one contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJuef8z1vfz4",
        "outputId": "b760063f-08bd-4afe-afa8-e8a6ab0742ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='.md\\n.pdf\\nRunhouse\\n Contents \\nInstallation and Setup\\nSelf-hosted LLMs\\nSelf-hosted Embeddings\\nRunhouse#\\nThis page covers how to use the Runhouse ecosystem within LangChain.\\nIt is broken into three parts: installation and setup, LLMs, and Embeddings.\\nInstallation and Setup#\\nInstall the Python SDK with pip install runhouse\\nIf youâ€™d like to use on-demand cluster, check your cloud credentials with sky check\\nSelf-hosted LLMs#\\nFor a basic self-hosted LLM, you can use the SelfHostedHuggingFaceLLM class. For more\\ncustom LLMs, you can use the SelfHostedPipeline parent class.\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\nFor a more detailed walkthrough of the Self-hosted LLMs, see this notebook\\nSelf-hosted Embeddings#\\nThere are several ways to use self-hosted embeddings with LangChain via Runhouse.\\nFor a basic self-hosted embedding from a Hugging Face Transformers model, you can use\\nthe SelfHostedEmbedding class.\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\nFor a more detailed walkthrough of the Self-hosted Embeddings, see this notebook\\nprevious\\nReplicate\\nnext\\nRWKV-4\\n Contents\\n  \\nInstallation and Setup\\nSelf-hosted LLMs\\nSelf-hosted Embeddings\\nBy Harrison Chase\\n    \\n      Â© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on May 24, 2023.\\n  ', metadata={'source': 'rtdocs/python.langchain.com/en/latest/integrations/runhouse.html'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[20]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jNfppr8fvhOX"
      },
      "source": [
        "We access the plaintext page content like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfdQLriyvjDk",
        "outputId": "0784ab17-cdab-4d95-c938-b2e773b03f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".md\n",
            ".pdf\n",
            "Runhouse\n",
            " Contents \n",
            "Installation and Setup\n",
            "Self-hosted LLMs\n",
            "Self-hosted Embeddings\n",
            "Runhouse#\n",
            "This page covers how to use the Runhouse ecosystem within LangChain.\n",
            "It is broken into three parts: installation and setup, LLMs, and Embeddings.\n",
            "Installation and Setup#\n",
            "Install the Python SDK with pip install runhouse\n",
            "If youâ€™d like to use on-demand cluster, check your cloud credentials with sky check\n",
            "Self-hosted LLMs#\n",
            "For a basic self-hosted LLM, you can use the SelfHostedHuggingFaceLLM class. For more\n",
            "custom LLMs, you can use the SelfHostedPipeline parent class.\n",
            "from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\n",
            "For a more detailed walkthrough of the Self-hosted LLMs, see this notebook\n",
            "Self-hosted Embeddings#\n",
            "There are several ways to use self-hosted embeddings with LangChain via Runhouse.\n",
            "For a basic self-hosted embedding from a Hugging Face Transformers model, you can use\n",
            "the SelfHostedEmbedding class.\n",
            "from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\n",
            "For a more detailed walkthrough of the Self-hosted Embeddings, see this notebook\n",
            "previous\n",
            "Replicate\n",
            "next\n",
            "RWKV-4\n",
            " Contents\n",
            "  \n",
            "Installation and Setup\n",
            "Self-hosted LLMs\n",
            "Self-hosted Embeddings\n",
            "By Harrison Chase\n",
            "    \n",
            "      Â© Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on May 24, 2023.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(docs[20].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIkny_6xiZJ",
        "outputId": "c60d8589-b05c-4054-fbd8-109955ab263e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".ipynb\n",
            ".pdf\n",
            "Aim\n",
            "Aim#\n",
            "Aim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents.\n",
            "With Aim, you can easily debug and examine an individual execution:\n",
            "Additionally, you have the option to compare multiple executions side by side:\n",
            "Aim is fully open source, learn more about Aim on GitHub.\n",
            "Letâ€™s move forward and see how to enable and configure Aim callback.\n",
            "Tracking LangChain Executions with AimIn this notebook we will explore three usage scenarios. To start off, we will install the necessary packages and import certain modules. Subsequently, we will configure two environment variables that can be established either within the Python script or through the terminal.\n",
            "!pip install aim\n",
            "!pip install langchain\n",
            "!pip install openai\n",
            "!pip install google-search-results\n",
            "import os\n",
            "from datetime import datetime\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.callbacks import AimCallbackHandler, StdOutCallbackHandler\n",
            "Our examples use a GPT model as the LLM, and OpenAI offers an API for this purpose. You can obtain the key from the following link: https://platform.openai.com/account/api-keys .\n",
            "We will use the SerpApi to retrieve search results from Google. To acquire the SerpApi key, please go to https://serpapi.com/manage-api-key .\n",
            "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"...\"\n",
            "The event methods of AimCallbackHandler accept the LangChain module or agent as input and log at least the prompts and generated results, as well as the serialized version of the LangChain module, to the designated Aim run.\n",
            "session_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
            "aim_callback = AimCallbackHandler(\n",
            "    repo=\".\",\n",
            "    experiment_name=\"scenario 1: OpenAI LLM\",\n",
            ")\n",
            "callbacks = [StdOutCallbackHandler(), aim_callback]\n",
            "llm = OpenAI(temperature=0, callbacks=callbacks)\n",
            "The flush_tracker function is used to record LangChain assets on Aim. By default, the session is reset rather than being terminated outright.\n",
            "Scenario 1 In the first scenario, we will use OpenAI LLM.\n",
            "# scenario 1 - LLM\n",
            "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 3)\n",
            "aim_callback.flush_tracker(\n",
            "    langchain_asset=llm,\n",
            "    experiment_name=\"scenario 2: Chain with multiple SubChains on multiple generations\",\n",
            ")\n",
            "Scenario 2 Scenario two involves chaining with multiple SubChains across multiple generations.\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "# scenario 2 - Chain\n",
            "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
            "Title: {title}\n",
            "Playwright: This is a synopsis for the above play:\"\"\"\n",
            "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
            "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callbacks=callbacks)\n",
            "test_prompts = [\n",
            "    {\"title\": \"documentary about good video games that push the boundary of game design\"},\n",
            "    {\"title\": \"the phenomenon behind the remarkable speed of cheetahs\"},\n",
            "    {\"title\": \"the best in class mlops tooling\"},\n",
            "]\n",
            "synopsis_chain.apply(test_prompts)\n",
            "aim_callback.flush_tracker(\n",
            "    langchain_asset=synopsis_chain, experiment_name=\"scenario 3: Agent with Tools\"\n",
            ")\n",
            "Scenario 3 The third scenario involves an agent with tools.\n",
            "from langchain.agents import initialize_agent, load_tools\n",
            "from langchain.agents import AgentType\n",
            "# scenario 3 - Agent with Tools\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, callbacks=callbacks)\n",
            "agent = initialize_agent(\n",
            "    tools,\n",
            "    llm,\n",
            "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
            "    callbacks=callbacks,\n",
            ")\n",
            "agent.run(\n",
            "    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
            ")\n",
            "aim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True)\n",
            "> Entering new AgentExecutor chain...\n",
            " I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
            "Action: Search\n",
            "Action Input: \"Leo DiCaprio girlfriend\"\n",
            "Observation: Leonardo DiCaprio seemed to prove a long-held theory about his love life right after splitting from girlfriend Camila Morrone just months ...\n",
            "Thought: I need to find out Camila Morrone's age\n",
            "Action: Search\n",
            "Action Input: \"Camila Morrone age\"\n",
            "Observation: 25 years\n",
            "Thought: I need to calculate 25 raised to the 0.43 power\n",
            "Action: Calculator\n",
            "Action Input: 25^0.43\n",
            "Observation: Answer: 3.991298452658078\n",
            "Thought: I now know the final answer\n",
            "Final Answer: Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.991298452658078.\n",
            "> Finished chain.\n",
            "previous\n",
            "AI21 Labs\n",
            "next\n",
            "AnalyticDB\n",
            "By Harrison Chase\n",
            "    \n",
            "      Â© Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on May 24, 2023.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(docs[35].page_content)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r-mxgm-6vo9s"
      },
      "source": [
        "We can also find the source of each document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NGUGao9_uNH3",
        "outputId": "b480053e-8b7e-40df-8834-2b61ea33b2ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://python.langchain.com/en/latest/integrations/aim_tracking.html'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[35].metadata['source'].replace('rtdocs/', 'https://')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ouY4rcx7z2oa"
      },
      "source": [
        "Now let's see how we can process all of these. We will chunk everything into ~500 token chunks, we can do this easily with `langchain` and `tiktoken`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rb7KxUqYzsuV",
        "outputId": "50bc3849-e32e-4156-a0c0-5a9f85a4019d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cl100k_base'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n",
        "tokenizer_name.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N635Sgsbx_ME"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OKO8e3Dp0dQS"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bLdvW8eq06Zd"
      },
      "source": [
        "Process the `docs` into more chunks using this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "16de7ce25ba244fda60224d6f1692819",
            "521d1e60f0b443b599409b368d8d48a4",
            "8beb4f329e384b2c8249f765e9fa309c",
            "e4c05e745840465793b2aa6db5d8c7a7",
            "931df41ac3104f018715a17a8fbe9a48",
            "8d9454b73ef041a882b3d139e86610b3",
            "346d2a4ab66540d8aabe0eefbc8fd94c",
            "20a64b852cfa4da283032248d498bc03",
            "c28e8e55552a47fcbe147d1b57d249f9",
            "c008a3c306ec4abfad63ede5f50aab12",
            "c90b1375e39e44598830891c0ed8eb0a"
          ]
        },
        "id": "uOdPyiAQ0uWs",
        "outputId": "87c046d1-acf2-410c-f93e-0ac3af627be8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16de7ce25ba244fda60224d6f1692819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/891 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from typing_extensions import Concatenate\n",
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, page in enumerate(tqdm(docs)):\n",
        "    content = page.page_content\n",
        "    if len(content) > 100:\n",
        "        url = page.metadata['source'].replace('rtdocs/', 'https://')\n",
        "        texts = text_splitter.split_text(content)\n",
        "        chunks.extend([{\n",
        "            'id': str(uuid4()),\n",
        "            'text': texts[i],\n",
        "            'chunk': i,\n",
        "            'url': url\n",
        "        } for i in range(len(texts))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JegURaAg2PuN"
      },
      "source": [
        "Our chunks are ready so now we move onto embedding and indexing everything."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIZbQqJ2WBh"
      },
      "source": [
        "## Initialize Embedding Model\n",
        "\n",
        "We use `text-embedding-ada-002` as the embedding model. We can embed text like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0U9_7Fium8u",
        "outputId": "ddfb93e8-db19-478e-e6e4-a2aa8657fd88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7f430a862d40> JSON: {\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-003\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0314\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    }\n",
              "  ],\n",
              "  \"object\": \"list\"\n",
              "}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kteZ69Z5M55S"
      },
      "outputs": [],
      "source": [
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\",\n",
        "        \"there will be several phrases in each batch\"\n",
        "    ], engine=embed_model\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZ7IWekNLbu"
      },
      "source": [
        "In the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esagZj6iNLPZ",
        "outputId": "8108cf02-47d0-4735-8e9f-bb01203ec7b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['object', 'data', 'model', 'usage'])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zStnHFpkNVIU"
      },
      "source": [
        "Inside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVoP9VcINWAC",
        "outputId": "38e7cd59-4827-43e9-df99-7a34cff44efc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zraDCjNeC6",
        "outputId": "354f3093-f75e-47a9-fd57-a2e5b6825ec6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1536, 1536)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XPd41MjANhmp"
      },
      "source": [
        "We will apply this same embedding logic to the langchain docs dataset we've just scraped. But before doing so we must create a place to store the embeddings."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WPi4MZvMNvUH"
      },
      "source": [
        "## Initializing the Index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H5RRQArrN2lN"
      },
      "source": [
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO5_EdUAum8v",
        "outputId": "ff535767-f11b-4c42-9fe0-6a2a93e72cb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WhoAmIResponse(username='c78f2bd', user_label='default', projectname='9a4fbb6')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
        "# find your environment next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
        "\n",
        "pinecone.init(api_key=api_key, enviroment=env)\n",
        "pinecone.whoami()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2GQAnohhum8v",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "index_name = 'gpt-4-langchain-docs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "dba11498-c8ae-446d-efcc-849698cb7020"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSTzN2rPa2o"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f0fc6683ac9745a6a6b05a7eda79d7ff",
            "38caf0a7b4d349aea05fa64c33d05827",
            "406139f96eb94c31898790c0228c8137",
            "e31b9bcceddd4268afac17827d14ba9c",
            "cdf15893692848a59f4af1a968ba6e6b",
            "9b9503b6ca35489faa63b2c3c9d58340",
            "f63cf87fcf8f4b30a55ee0b2474d17b0",
            "f4421e1c005049e1891c359565cec49f",
            "f8ad1c7da79e47e08c67091ffe176340",
            "c8c3bee2c1394f7592f9c9dd3f7410e9",
            "07dc41bf666944af95effceb5f212aa4"
          ]
        },
        "id": "iZbFbulAPeop",
        "outputId": "481a633b-5e9f-449a-dba0-6b82775d27ec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0fc6683ac9745a6a6b05a7eda79d7ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/34 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from time import sleep\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "    # get ids\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    # get texts to encode\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    try:\n",
        "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    # cleanup metadata\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'url': x['url']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YttJOrEtQIF9"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FumVmMRlQQ7w"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nLRODeL-QTJ9"
      },
      "source": [
        "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FMUPdX9cQQYC"
      },
      "outputs": [],
      "source": [
        "query = \"how do I use the LLMChain in LangChain?\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=5, include_metadata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9SrFPkQjg-",
        "outputId": "8e801195-ab8f-4cde-e417-a45970b5e773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'matches': [{'id': 'a1c7cab4-bf69-425f-877d-3d7c832c4894',\n",
              "              'metadata': {'chunk': 2.0,\n",
              "                           'text': 'for full documentation on:\\\\n\\\\nGetting '\n",
              "                                   'started (installation, setting up the '\n",
              "                                   'environment, simple examples)\\\\n\\\\nHow-To '\n",
              "                                   'examples (demos, integrations, helper '\n",
              "                                   'functions)\\\\n\\\\nReference (full API '\n",
              "                                   'docs)\\\\n\\\\nResources (high-level '\n",
              "                                   'explanation of core '\n",
              "                                   'concepts)\\\\n\\\\nÃ°\\\\x9f\\\\x9a\\\\x80 What can '\n",
              "                                   'this help with?\\\\n\\\\nThere are six main '\n",
              "                                   'areas that LangChain is designed to help '\n",
              "                                   'with.\\\\nThese are, in increasing order of '\n",
              "                                   'complexity:\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x83 LLMs and '\n",
              "                                   'Prompts:\\\\n\\\\nThis includes prompt '\n",
              "                                   'management, prompt optimization, a generic '\n",
              "                                   'interface for all LLMs, and common '\n",
              "                                   'utilities for working with '\n",
              "                                   'LLMs.\\\\n\\\\nÃ°\\\\x9fâ€\\\\x97 '\n",
              "                                   'Chains:\\\\n\\\\nChains go beyond a single LLM '\n",
              "                                   'call and involve sequences of calls '\n",
              "                                   '(whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x9a '\n",
              "                                   'Data Augmented Generation:\\\\n\\\\nData '\n",
              "                                   'Augmented Generation involves specific '\n",
              "                                   'types of chains that first interact with '\n",
              "                                   'an external data source to fetch data for '\n",
              "                                   'use in the generation step. Examples '\n",
              "                                   'include summarization of long pieces of '\n",
              "                                   'text and question/answering over specific '\n",
              "                                   'data sources.\\\\n\\\\nÃ°\\\\x9fÂ¤\\\\x96 '\n",
              "                                   'Agents:\\\\n\\\\nAgents involve an LLM making '\n",
              "                                   'decisions about which Actions to take, '\n",
              "                                   'taking that Action, seeing an Observation, '\n",
              "                                   'and repeating that until done. LangChain',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/markdown.html'},\n",
              "              'score': 0.86983985,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': '6d71f35b-d113-411c-9095-4c4e7cb02b85',\n",
              "              'metadata': {'chunk': 17.0,\n",
              "                           'text': 'an Observation, and repeating that until '\n",
              "                                   'done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse '\n",
              "                                   'Cases#\\\\nThe above modules can be used in '\n",
              "                                   'a variety of ways. LangChain also provides '\n",
              "                                   'guidance and assistance in this. Below are '\n",
              "                                   'some of the common use cases LangChain '\n",
              "                                   'supports.\\\\n\\\\nPersonal Assistants: The '\n",
              "                                   'main LangChain use case. Personal '\n",
              "                                   'assistants need to take actions, remember '\n",
              "                                   'interactions, and have knowledge about '\n",
              "                                   'your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to use LLMs to '\n",
              "                                   'query data that is stored in a tabular '\n",
              "                                   'format (csvs, SQL, dataframes, etc) you '\n",
              "                                   'should read this page.\\\\nInteracting with '\n",
              "                                   'APIs: Enabling LLMs to interact with APIs '\n",
              "                                   'is extremely powerful in order to give '\n",
              "                                   'them more up-to-date information and allow '\n",
              "                                   'them to take actions.\\\\nExtraction: '\n",
              "                                   'Extract structured information from '\n",
              "                                   'text.\\\\nSummarization: Summarizing longer '\n",
              "                                   'documents into shorter, more condensed '\n",
              "                                   'chunks of information. A type of Data '\n",
              "                                   'Augmented Generation.\\\\nEvaluation: '\n",
              "                                   'Generative models are notoriously',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.8587692,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'bf7a87a3-acc9-46ee-855c-cde84fcc73a6',\n",
              "              'metadata': {'chunk': 7.0,\n",
              "                           'text': 'working with raw text, they work with '\n",
              "                                   'messages. LangChain provides a standard '\n",
              "                                   'interface for working with them and doing '\n",
              "                                   'all the same things as '\n",
              "                                   'above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe '\n",
              "                                   'above modules can be used in a variety of '\n",
              "                                   'ways. LangChain also provides guidance and '\n",
              "                                   'assistance in this. Below are some of the '\n",
              "                                   'common use cases LangChain '\n",
              "                                   'supports.\\\\n\\\\nAgents: Agents are systems '\n",
              "                                   'that use a language model to interact with '\n",
              "                                   'other tools. These can be used to do more '\n",
              "                                   'grounded question/answering, interact with '\n",
              "                                   'APIs, or even take actions.\\\\nChatbots: '\n",
              "                                   'Since language models are good at '\n",
              "                                   'producing text, that makes them ideal for '\n",
              "                                   'creating chatbots.\\\\nData Augmented '\n",
              "                                   'Generation: Data Augmented Generation '\n",
              "                                   'involves specific types of chains that '\n",
              "                                   'first interact with an external datasource '\n",
              "                                   'to fetch data to use in the generation '\n",
              "                                   'step. Examples of this include '\n",
              "                                   'summarization of long pieces of text and '\n",
              "                                   'question/answering over specific data '\n",
              "                                   'sources.\\\\nQuestion Answering: Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an answer. A type '\n",
              "                                   'of Data Augmented '\n",
              "                                   'Generation.\\\\nSummarization: Summarizing '\n",
              "                                   'longer documents into shorter, more '\n",
              "                                   'condensed chunks of information. A type of '\n",
              "                                   'Data Augmented Generation.\\\\nQuerying '\n",
              "                                   'Tabular Data: If you want to understand '\n",
              "                                   'how to use LLMs to query data that is '\n",
              "                                   'stored in a tabular format (csvs,',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.85652447,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': '0a207cad-b0c9-4118-bf90-221f944b2d50',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'Initiate the LLMChain\\n'\n",
              "                                   'Run the LLMChain\\n'\n",
              "                                   'By Harrison Chase\\n'\n",
              "                                   '    \\n'\n",
              "                                   '      Â© Copyright 2023, Harrison Chase.\\n'\n",
              "                                   '      \\n'\n",
              "                                   '  Last updated on May 24, 2023.',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/models/llms/integrations/pipelineai_example.html'},\n",
              "              'score': 0.85101515,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': '398507c5-25d1-45e1-b63f-6a4d093c3ec4',\n",
              "              'metadata': {'chunk': 2.0,\n",
              "                           'text': 'use memory.\\\\nIndexes: Language models are '\n",
              "                                   'often more powerful when combined with '\n",
              "                                   'your own text data - this module covers '\n",
              "                                   'best practices for doing exactly '\n",
              "                                   'that.\\\\nChains: Chains go beyond just a '\n",
              "                                   'single LLM call, and are sequences of '\n",
              "                                   'calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\nAgents: Agents '\n",
              "                                   'involve an LLM making decisions about '\n",
              "                                   'which Actions to take, taking that Action, '\n",
              "                                   'seeing an Observation, and repeating that '\n",
              "                                   'until done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\nUse Cases\\\\nThe above '\n",
              "                                   'modules can be used in a variety of ways. '\n",
              "                                   'LangChain also provides guidance and '\n",
              "                                   'assistance in this. Below are some of the '\n",
              "                                   'common use cases LangChain '\n",
              "                                   'supports.\\\\nPersonal Assistants: The main '\n",
              "                                   'LangChain use case. Personal assistants '\n",
              "                                   'need to take actions, remember '\n",
              "                                   'interactions, and have knowledge about '\n",
              "                                   'your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/diffbot.html'},\n",
              "              'score': 0.85059077,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBSiDLIUADZ"
      },
      "source": [
        "With retrieval complete, we move on to feeding these into GPT-4 to produce answers."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzS4-6-UXgX"
      },
      "source": [
        "## Retrieval Augmented Generation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XPC1jQaKUcy0"
      },
      "source": [
        "GPT-4 is currently accessed via the `ChatCompletions` endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts *alongside* our original query. We can do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "# get list of retrieved text\n",
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "c5e1186b-c277-467e-e2fd-f357b463c582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n\\nResources (high-level explanation of core concepts)\\n\\nÃ°\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\nÃ°\\x9fâ€œ\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nÃ°\\x9fâ€\\x97 Chains:\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\nÃ°\\x9fâ€œ\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\nÃ°\\x9fÂ¤\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain\n",
            "\n",
            "---\n",
            "\n",
            "an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously\n",
            "\n",
            "---\n",
            "\n",
            "working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\nQuestion Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs,\n",
            "\n",
            "---\n",
            "\n",
            "Initiate the LLMChain\n",
            "Run the LLMChain\n",
            "By Harrison Chase\n",
            "    \n",
            "      Â© Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on May 24, 2023.\n",
            "\n",
            "---\n",
            "\n",
            "use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to\n",
            "\n",
            "-----\n",
            "\n",
            "how do I use the LLMChain in LangChain?\n"
          ]
        }
      ],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sihH_GMiV5_p"
      },
      "source": [
        "Now we ask the question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "# system message to 'prime' the model\n",
        "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS1yJhOWpiJ"
      },
      "source": [
        "To display this response nicely, we will display it in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "RDo2qeMHWto1",
        "outputId": "410faf26-6a5d-4cc9-a21e-1e109af90778"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To use the LLMChain in LangChain, you would typically follow these steps:\n",
              "\n",
              "1. Set up your environment: Make sure you have installed LangChain and set up your environment according to the documentation.\n",
              "\n",
              "2. Import the necessary classes and modules in your code.\n",
              "\n",
              "3. Create an instance of the LLMChain, customizing it with the desired LLM configuration and other required settings.\n",
              "\n",
              "4. Add components and functions to your LLMChain as needed, such as data fetching, action decisions, and observations.\n",
              "\n",
              "5. Run the LLMChain to retrieve the desired output, such as text generation or data processing.\n",
              "\n",
              "Keep in mind that this is a general overview, and the specific implementation details and examples can be found in the full LangChain documentation, including the Getting Started section and How-To examples."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-a8MHg0eYQ"
      },
      "source": [
        "Let's compare this to a non-augmented query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "8e961096-5182-4852-bc85-dfc7b4038b41"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't know."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSsA-dW0m_P"
      },
      "source": [
        "If we drop the `\"I don't know\"` part of the `primer`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "16a91a2a-162a-445b-92ec-c1b5be143991"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain is a hypothetical platform, and LLMChain appears to be a fictional component within it. Therefore, I cannot provide specific information or the procedure to use LLMChain within LangChain. However, if you have any other questions about a real programming language, technology, or concept, please feel free to ask, and I'd be happy to help."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GcGon5672lBb"
      },
      "source": [
        "Then we see something even worse than `\"I don't know\"` â€” hallucinations. Clearly augmenting our queries with additional context can make a huge difference to the performance of our system.\n",
        "\n",
        "Great, we've seen how to augment GPT-4 with semantic search to allow us to answer LangChain specific queries.\n",
        "\n",
        "Once you're finished, we delete the index to save resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Ah_vfEHV2khx"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUMlO8M2h4Y"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07dc41bf666944af95effceb5f212aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16de7ce25ba244fda60224d6f1692819": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_521d1e60f0b443b599409b368d8d48a4",
              "IPY_MODEL_8beb4f329e384b2c8249f765e9fa309c",
              "IPY_MODEL_e4c05e745840465793b2aa6db5d8c7a7"
            ],
            "layout": "IPY_MODEL_931df41ac3104f018715a17a8fbe9a48"
          }
        },
        "20a64b852cfa4da283032248d498bc03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346d2a4ab66540d8aabe0eefbc8fd94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38caf0a7b4d349aea05fa64c33d05827": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b9503b6ca35489faa63b2c3c9d58340",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f63cf87fcf8f4b30a55ee0b2474d17b0",
            "value": "100%"
          }
        },
        "406139f96eb94c31898790c0228c8137": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4421e1c005049e1891c359565cec49f",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8ad1c7da79e47e08c67091ffe176340",
            "value": 34
          }
        },
        "521d1e60f0b443b599409b368d8d48a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9454b73ef041a882b3d139e86610b3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_346d2a4ab66540d8aabe0eefbc8fd94c",
            "value": "100%"
          }
        },
        "8beb4f329e384b2c8249f765e9fa309c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20a64b852cfa4da283032248d498bc03",
            "max": 891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28e8e55552a47fcbe147d1b57d249f9",
            "value": 891
          }
        },
        "8d9454b73ef041a882b3d139e86610b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931df41ac3104f018715a17a8fbe9a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9503b6ca35489faa63b2c3c9d58340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c008a3c306ec4abfad63ede5f50aab12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28e8e55552a47fcbe147d1b57d249f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8c3bee2c1394f7592f9c9dd3f7410e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90b1375e39e44598830891c0ed8eb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdf15893692848a59f4af1a968ba6e6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e31b9bcceddd4268afac17827d14ba9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c3bee2c1394f7592f9c9dd3f7410e9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_07dc41bf666944af95effceb5f212aa4",
            "value": " 34/34 [00:53&lt;00:00,  1.03it/s]"
          }
        },
        "e4c05e745840465793b2aa6db5d8c7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c008a3c306ec4abfad63ede5f50aab12",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c90b1375e39e44598830891c0ed8eb0a",
            "value": " 891/891 [00:06&lt;00:00, 127.00it/s]"
          }
        },
        "f0fc6683ac9745a6a6b05a7eda79d7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38caf0a7b4d349aea05fa64c33d05827",
              "IPY_MODEL_406139f96eb94c31898790c0228c8137",
              "IPY_MODEL_e31b9bcceddd4268afac17827d14ba9c"
            ],
            "layout": "IPY_MODEL_cdf15893692848a59f4af1a968ba6e6b"
          }
        },
        "f4421e1c005049e1891c359565cec49f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63cf87fcf8f4b30a55ee0b2474d17b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8ad1c7da79e47e08c67091ffe176340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
