{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLLl1Agum8O"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb)\n",
        "\n",
        "# GPT4 with Retrieval Augmentation over LangChain Docs\n",
        "\n",
        "[![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/fast-link.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs-fast.ipynb)\n",
        "\n",
        "In this notebook we'll work through an example of using GPT-4 with retrieval augmentation to answer questions about the LangChain Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_HDKlQO5svqI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  tiktoken==0.4.0 \\\n",
        "  openai==0.27.7 \\\n",
        "  langchain==0.0.179 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1EpQ-jq7SU"
      },
      "source": [
        "---\n",
        "\n",
        "ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUEJ6vDum8q"
      },
      "source": [
        "In this example, we will download the LangChain docs from [langchain.readthedocs.io/](https://langchain.readthedocs.io/latest/en/). We get all `.html` files located on the site like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo9gYhGPr_DQ",
        "outputId": "89dffc5f-7ef6-45f3-f13f-9c4f2269ecc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/llms.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   2.36M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-06-27 12:14:56 (312 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/llms.htmlâ€™ saved [2477496]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/retrievers.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/retrievers.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 272.60K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:56 (343 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/retrievers.htmlâ€™ saved [279143]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/vectorstores.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/vectorstores.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 878.29K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-06-27 12:14:56 (322 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/vectorstores.htmlâ€™ saved [899365]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/document_loaders.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/document_loaders.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 518.92K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-06-27 12:14:56 (249 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/document_loaders.htmlâ€™ saved [531379]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/tools.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/tools.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 677.84K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-06-27 12:14:56 (289 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/tools.htmlâ€™ saved [694108]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/example_selector.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/example_selector.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  46.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (360 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/example_selector.htmlâ€™ saved [47157]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/prompts.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/prompts.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 157.22K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:56 (229 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/prompts.htmlâ€™ saved [160996]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/embeddings.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/embeddings.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 241.95K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:56 (241 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/embeddings.htmlâ€™ saved [247759]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/agents.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/agents.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 362.20K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:56 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/agents.htmlâ€™ saved [370892]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/document_transformers.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/document_transformers.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  82.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (371 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/document_transformers.htmlâ€™ saved [84175]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/output_parsers.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/output_parsers.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  97.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (330 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/output_parsers.htmlâ€™ saved [99867]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/agent_toolkits.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/agent_toolkits.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 193.46K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:56 (369 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/agent_toolkits.htmlâ€™ saved [198103]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/modules/chat_models.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/modules/chat_models.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  89.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (387 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/modules/chat_models.htmlâ€™ saved [92069]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/_modules/langchain/schema.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/schema.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  67.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (308 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/schema.htmlâ€™ saved [69516]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/prompts.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/prompts.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:56 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/prompts.htmlâ€™ saved [9479]\n",
            "\n",
            "--2023-06-27 12:14:56--  https://api.python.langchain.com/en/stable/models.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/models.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/models.htmlâ€™ saved [9455]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/api/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/api/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (404 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/api/base.htmlâ€™ saved [31128]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (289 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/base.htmlâ€™ saved [29890]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/conversational_retrieval/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/conversational_retrieval/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  52.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (412 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/conversational_retrieval/base.htmlâ€™ saved [53681]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/constitutional_ai/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/constitutional_ai/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (370 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/constitutional_ai/base.htmlâ€™ saved [31165]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/conversation/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/conversation/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (199 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/conversation/base.htmlâ€™ saved [15838]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/flare/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/flare/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  41.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (278 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/flare/base.htmlâ€™ saved [42111]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/cypher.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/cypher.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (339 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/cypher.htmlâ€™ saved [26717]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (342 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/base.htmlâ€™ saved [21393]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/hyde/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/hyde/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (343 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/hyde/base.htmlâ€™ saved [21162]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/kuzu.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/kuzu.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (382 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/kuzu.htmlâ€™ saved [21447]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm_bash/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_bash/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (383 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_bash/base.htmlâ€™ saved [26305]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  59.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (362 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm.htmlâ€™ saved [61424]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm_checker/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_checker/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_checker/base.htmlâ€™ saved [28368]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm_math/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_math/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  34.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (186 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_math/base.htmlâ€™ saved [35832]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm_requests.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_requests.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (176 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_requests.htmlâ€™ saved [19187]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/router/llm_router.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/llm_router.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (85.2 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/llm_router.htmlâ€™ saved [25594]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/llm_summarization_checker/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_summarization_checker/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (309 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/llm_summarization_checker/base.htmlâ€™ saved [32704]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/mapreduce.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/mapreduce.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (78.4 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/mapreduce.htmlâ€™ saved [21791]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_prompt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_prompt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (249 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_prompt.htmlâ€™ saved [18126]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_retrieval_qa.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_retrieval_qa.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (313 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/multi_retrieval_qa.htmlâ€™ saved [21852]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/router/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (312 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/router/base.htmlâ€™ saved [27445]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/natbot/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/natbot/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (225 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/natbot/base.htmlâ€™ saved [25258]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/nebulagraph.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/nebulagraph.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (285 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/graph_qa/nebulagraph.htmlâ€™ saved [21430]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/moderation.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/moderation.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/moderation.htmlâ€™ saved [19177]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/api/openapi/chain.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/api/openapi/chain.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  44.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (333 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/api/openapi/chain.htmlâ€™ saved [45650]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/pal/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/pal/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (356 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/pal/base.htmlâ€™ saved [25483]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/qa_generation/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_generation/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_generation/base.htmlâ€™ saved [17321]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  34.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/base.htmlâ€™ saved [34839]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/retrieval_qa/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/retrieval_qa/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (312 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/retrieval_qa/base.htmlâ€™ saved [39642]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/retrieval.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/retrieval.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (140 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/retrieval.htmlâ€™ saved [16136]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/sql_database/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/sql_database/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  50.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (389 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/sql_database/base.htmlâ€™ saved [51282]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/sequential.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/sequential.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  39.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (250 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/sequential.htmlâ€™ saved [40160]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/transform.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/transform.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (385 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/transform.htmlâ€™ saved [12712]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/vector_db.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/vector_db.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (274 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/qa_with_sources/vector_db.htmlâ€™ saved [18024]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/extraction.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/extraction.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/extraction.htmlâ€™ saved [18429]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/tagging.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/tagging.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (224 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/tagging.htmlâ€™ saved [16105]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/loading.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/loading.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  94.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (324 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/loading.htmlâ€™ saved [96340]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/citation_fuzzy_match.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/citation_fuzzy_match.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/citation_fuzzy_match.htmlâ€™ saved [21317]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/qa_with_structure.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/qa_with_structure.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (295 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/openai_functions/qa_with_structure.htmlâ€™ saved [21012]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/stuff.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/stuff.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:57 (189 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/stuff.htmlâ€™ saved [24758]\n",
            "\n",
            "--2023-06-27 12:14:57--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_rerank.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_rerank.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (352 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_rerank.htmlâ€™ saved [30438]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_reduce.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_reduce.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  44.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (227 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/map_reduce.htmlâ€™ saved [45164]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/refine.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/refine.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (297 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chains/combine_documents/refine.htmlâ€™ saved [32581]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cassandra.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cassandra.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (300 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cassandra.htmlâ€™ saved [29523]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/in_memory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/in_memory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (247 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/in_memory.htmlâ€™ saved [10893]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/combined.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/combined.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/combined.htmlâ€™ saved [19369]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/buffer.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/buffer.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  22.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (237 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/buffer.htmlâ€™ saved [23135]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/buffer_window.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/buffer_window.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (189 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/buffer_window.htmlâ€™ saved [13674]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/entity.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/entity.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  65.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (244 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/entity.htmlâ€™ saved [67186]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/kg.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/kg.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (309 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/kg.htmlâ€™ saved [31009]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/summary_buffer.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/summary_buffer.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.68K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-06-27 12:14:58 (3.29 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/summary_buffer.htmlâ€™ saved [21176]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/summary.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/summary.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/summary.htmlâ€™ saved [23588]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/token_buffer.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/token_buffer.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.27K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (132 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/token_buffer.htmlâ€™ saved [16658]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cosmos_db.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cosmos_db.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (320 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/cosmos_db.htmlâ€™ saved [31082]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/dynamodb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/dynamodb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (127 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/dynamodb.htmlâ€™ saved [19693]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (319 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/file.htmlâ€™ saved [14580]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/momento.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/momento.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (241 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/momento.htmlâ€™ saved [31949]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/mongodb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/mongodb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (243 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/mongodb.htmlâ€™ saved [19504]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/motorhead_memory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/motorhead_memory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/motorhead_memory.htmlâ€™ saved [23756]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/postgres.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/postgres.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (273 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/postgres.htmlâ€™ saved [20041]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/readonly.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/readonly.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/readonly.htmlâ€™ saved [12379]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/redis.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/redis.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (261 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/redis.htmlâ€™ saved [17417]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/sql.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/sql.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (270 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/sql.htmlâ€™ saved [20672]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/simple.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/simple.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (215 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/simple.htmlâ€™ saved [12321]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/vectorstore.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/vectorstore.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/vectorstore.htmlâ€™ saved [21106]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/zep.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/zep.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.93K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-06-27 12:14:58 (5.98 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/memory/chat_message_histories/zep.htmlâ€™ saved [27576]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/aim_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/aim_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  66.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/aim_callback.htmlâ€™ saved [67653]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/argilla_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/argilla_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  50.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (175 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/argilla_callback.htmlâ€™ saved [52169]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/arize_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/arize_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  41.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (277 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/arize_callback.htmlâ€™ saved [42463]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_aiter.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_aiter.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.60K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:14:58 (13.6 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_aiter.htmlâ€™ saved [19051]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/clearml_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/clearml_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  85.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (378 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/clearml_callback.htmlâ€™ saved [87285]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/comet_ml_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/comet_ml_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 106.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (353 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/comet_ml_callback.htmlâ€™ saved [109078]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (273 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/file.htmlâ€™ saved [23682]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout_final_only.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout_final_only.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout_final_only.htmlâ€™ saved [21855]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/human.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/human.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (334 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/human.htmlâ€™ saved [14949]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/infino_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/infino_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (302 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/infino_callback.htmlâ€™ saved [33928]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/mlflow_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/mlflow_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 118.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (236 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/mlflow_callback.htmlâ€™ saved [121073]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/openai_info.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/openai_info.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (359 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/openai_info.htmlâ€™ saved [30220]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/stdout.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/stdout.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (313 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/stdout.htmlâ€™ saved [27690]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streaming_stdout.htmlâ€™ saved [21796]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:58 (144 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit.htmlâ€™ saved [15538]\n",
            "\n",
            "--2023-06-27 12:14:58--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit/streamlit_callback_handler.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit/streamlit_callback_handler.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  66.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (352 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/streamlit/streamlit_callback_handler.htmlâ€™ saved [68194]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/wandb_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/wandb_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  95.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (267 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/wandb_callback.htmlâ€™ saved [98106]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/whylabs_callback.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/whylabs_callback.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (256 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/whylabs_callback.htmlâ€™ saved [39162]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/callbacks/manager.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/manager.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 143.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (318 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/callbacks/manager.htmlâ€™ saved [146584]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/apify.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/apify.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (302 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/apify.htmlâ€™ saved [21547]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/arxiv.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/arxiv.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (284 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/arxiv.htmlâ€™ saved [28411]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/bash.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bash.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bash.htmlâ€™ saved [25556]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/bibtex.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bibtex.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bibtex.htmlâ€™ saved [19292]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/bing_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bing_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/bing_search.htmlâ€™ saved [20564]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/brave_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/brave_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/brave_search.htmlâ€™ saved [14700]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/duckduckgo_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/duckduckgo_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (304 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/duckduckgo_search.htmlâ€™ saved [21787]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/google_places_api.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_places_api.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_places_api.htmlâ€™ saved [23677]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/google_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (291 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_search.htmlâ€™ saved [23816]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/google_serper.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_serper.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (351 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/google_serper.htmlâ€™ saved [38976]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/graphql.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/graphql.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (231 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/graphql.htmlâ€™ saved [16054]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/jira.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/jira.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (354 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/jira.htmlâ€™ saved [39781]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/awslambda.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/awslambda.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (237 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/awslambda.htmlâ€™ saved [15723]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/max_compute.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/max_compute.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (224 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/max_compute.htmlâ€™ saved [17930]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/metaphor_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/metaphor_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (276 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/metaphor_search.htmlâ€™ saved [30561]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/openweathermap.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/openweathermap.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/openweathermap.htmlâ€™ saved [18976]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/powerbi.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/powerbi.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  54.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/powerbi.htmlâ€™ saved [55937]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/pupmed.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/pupmed.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (172 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/pupmed.htmlâ€™ saved [30610]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/python.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/python.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (281 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/python.htmlâ€™ saved [12301]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/scenexplain.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/scenexplain.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (205 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/scenexplain.htmlâ€™ saved [16746]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/searx_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/searx_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  65.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (258 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/searx_search.htmlâ€™ saved [67206]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/serpapi.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/serpapi.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (258 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/serpapi.htmlâ€™ saved [34395]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/spark_sql.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/spark_sql.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  37.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/spark_sql.htmlâ€™ saved [38661]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/requests.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/requests.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  42.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (339 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/requests.htmlâ€™ saved [43883]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/twilio.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/twilio.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (260 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/twilio.htmlâ€™ saved [17631]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/wikipedia.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/wikipedia.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (245 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/wikipedia.htmlâ€™ saved [24438]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/wolfram_alpha.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/wolfram_alpha.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (365 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/wolfram_alpha.htmlâ€™ saved [15307]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/zapier.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/zapier.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (296 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/zapier.htmlâ€™ saved [30189]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/baby_agi/baby_agi.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/baby_agi/baby_agi.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  40.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (167 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/baby_agi/baby_agi.htmlâ€™ saved [41615]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/autogpt/agent.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/autogpt/agent.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (317 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/autonomous_agents/autogpt/agent.htmlâ€™ saved [29209]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/generative_agent.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/generative_agent.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  52.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/generative_agent.htmlâ€™ saved [53912]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/memory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/memory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  57.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:14:59 (243 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/experimental/generative_agents/memory.htmlâ€™ saved [58871]\n",
            "\n",
            "--2023-06-27 12:14:59--  https://api.python.langchain.com/en/latest/_modules/pydantic/config.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-06-27 12:15:00 ERROR 404: Not Found.\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/latest/_modules/pydantic/main.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-06-27 12:15:00 ERROR 404: Not Found.\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/ai21.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/ai21.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (299 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/ai21.htmlâ€™ saved [27968]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/aleph_alpha.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/aleph_alpha.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (230 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/aleph_alpha.htmlâ€™ saved [39175]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/amazon_api_gateway.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/amazon_api_gateway.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (141 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/amazon_api_gateway.htmlâ€™ saved [18870]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/anthropic.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/anthropic.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  42.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (192 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/anthropic.htmlâ€™ saved [43268]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/anyscale.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/anyscale.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (276 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/anyscale.htmlâ€™ saved [23564]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/aviary.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/aviary.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (263 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/aviary.htmlâ€™ saved [31335]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/azureml_endpoint.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/azureml_endpoint.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  34.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/azureml_endpoint.htmlâ€™ saved [35562]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 131.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (322 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openai.htmlâ€™ saved [134874]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/bananadev.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/bananadev.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (293 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/bananadev.htmlâ€™ saved [24287]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/baseten.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/baseten.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (213 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/baseten.htmlâ€™ saved [16645]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/beam.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/beam.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  40.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (331 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/beam.htmlâ€™ saved [41336]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/bedrock.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/bedrock.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:00 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/bedrock.htmlâ€™ saved [30885]\n",
            "\n",
            "--2023-06-27 12:15:00--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/ctransformers.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/ctransformers.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (254 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/ctransformers.htmlâ€™ saved [19530]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/cerebriumai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/cerebriumai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (280 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/cerebriumai.htmlâ€™ saved [22235]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/clarifai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/clarifai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (346 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/clarifai.htmlâ€™ saved [28660]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/cohere.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/cohere.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (288 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/cohere.htmlâ€™ saved [29276]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/databricks.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/databricks.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  47.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (175 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/databricks.htmlâ€™ saved [48802]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/deepinfra.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/deepinfra.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (220 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/deepinfra.htmlâ€™ saved [21692]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/fake.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/fake.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (308 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/fake.htmlâ€™ saved [14003]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/forefrontai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/forefrontai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (220 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/forefrontai.htmlâ€™ saved [21433]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/gpt4all.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/gpt4all.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (222 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/gpt4all.htmlâ€™ saved [34034]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/google_palm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/google_palm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (313 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/google_palm.htmlâ€™ saved [31671]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/gooseai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/gooseai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (216 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/gooseai.htmlâ€™ saved [27820]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_endpoint.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_endpoint.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (289 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_endpoint.htmlâ€™ saved [26171]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_hub.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_hub.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (262 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_hub.htmlâ€™ saved [24462]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_pipeline.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_pipeline.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_pipeline.htmlâ€™ saved [31259]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_text_gen_inference.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_text_gen_inference.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (236 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/huggingface_text_gen_inference.htmlâ€™ saved [38965]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/human.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/human.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (235 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/human.htmlâ€™ saved [18630]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/llamacpp.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/llamacpp.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  42.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (206 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/llamacpp.htmlâ€™ saved [43344]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/textgen.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/textgen.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  32.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (141 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/textgen.htmlâ€™ saved [33667]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/manifest.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/manifest.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (222 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/manifest.htmlâ€™ saved [16304]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/modal.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/modal.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/modal.htmlâ€™ saved [21118]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/mosaicml.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/mosaicml.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (176 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/mosaicml.htmlâ€™ saved [31553]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/nlpcloud.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/nlpcloud.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (104 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/nlpcloud.htmlâ€™ saved [26509]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/openllm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openllm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  42.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (234 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openllm.htmlâ€™ saved [43628]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/openlm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openlm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (268 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/openlm.htmlâ€™ saved [11277]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/petals.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/petals.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (287 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/petals.htmlâ€™ saved [27976]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/pipelineai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/pipelineai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  22.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (290 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/pipelineai.htmlâ€™ saved [23229]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/predictionguard.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/predictionguard.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (211 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/predictionguard.htmlâ€™ saved [23621]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/promptlayer_openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/promptlayer_openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  38.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (280 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/promptlayer_openai.htmlâ€™ saved [38941]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/rwkv.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/rwkv.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  35.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (273 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/rwkv.htmlâ€™ saved [36558]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/replicate.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/replicate.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (232 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/replicate.htmlâ€™ saved [24224]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/sagemaker_endpoint.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/sagemaker_endpoint.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  32.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (259 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/sagemaker_endpoint.htmlâ€™ saved [32893]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted_hugging_face.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted_hugging_face.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted_hugging_face.htmlâ€™ saved [34803]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  32.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (211 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/self_hosted.htmlâ€™ saved [32931]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/stochasticai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/stochasticai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (51.2 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/stochasticai.htmlâ€™ saved [25811]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/vertexai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/vertexai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (189 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/vertexai.htmlâ€™ saved [27202]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/llms/writer.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/writer.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (269 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/llms/writer.htmlâ€™ saved [26748]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/kendra.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/kendra.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  44.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (228 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/kendra.htmlâ€™ saved [45737]\n",
            "\n",
            "--2023-06-27 12:15:01--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/arxiv.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/arxiv.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:01 (242 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/arxiv.htmlâ€™ saved [10886]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/azure_cognitive_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/azure_cognitive_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (127 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/azure_cognitive_search.htmlâ€™ saved [24443]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/chatgpt_plugin_retriever.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/chatgpt_plugin_retriever.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/chatgpt_plugin_retriever.htmlâ€™ saved [21529]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/contextual_compression.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/contextual_compression.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/contextual_compression.htmlâ€™ saved [14828]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/databerry.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/databerry.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/databerry.htmlâ€™ saved [19260]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/elastic_search_bm25.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/elastic_search_bm25.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (242 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/elastic_search_bm25.htmlâ€™ saved [23690]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/knn.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/knn.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (255 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/knn.htmlâ€™ saved [19678]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/llama_index.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/llama_index.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (267 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/llama_index.htmlâ€™ saved [20607]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/merger_retriever.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/merger_retriever.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (241 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/merger_retriever.htmlâ€™ saved [19284]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/metal.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/metal.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/metal.htmlâ€™ saved [13932]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/milvus.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/milvus.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (219 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/milvus.htmlâ€™ saved [16549]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/pinecone_hybrid_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/pinecone_hybrid_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (288 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/pinecone_hybrid_search.htmlâ€™ saved [29519]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/pupmed.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/pupmed.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (237 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/pupmed.htmlâ€™ saved [10907]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/remote_retriever.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/remote_retriever.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (210 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/remote_retriever.htmlâ€™ saved [15470]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/svm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/svm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (212 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/svm.htmlâ€™ saved [22114]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/self_query/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/self_query/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (285 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/self_query/base.htmlâ€™ saved [29422]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/tfidf.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/tfidf.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (256 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/tfidf.htmlâ€™ saved [20083]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/time_weighted_retriever.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/time_weighted_retriever.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (239 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/time_weighted_retriever.htmlâ€™ saved [34251]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/vespa_retriever.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/vespa_retriever.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (167 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/vespa_retriever.htmlâ€™ saved [28690]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/weaviate_hybrid_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/weaviate_hybrid_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (252 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/weaviate_hybrid_search.htmlâ€™ saved [25611]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/wikipedia.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/wikipedia.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (243 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/wikipedia.htmlâ€™ saved [10950]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/zep.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/zep.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (226 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/zep.htmlâ€™ saved [18402]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/zilliz.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/zilliz.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (204 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/zilliz.htmlâ€™ saved [16577]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/docarray.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/docarray.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  32.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (211 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/docarray.htmlâ€™ saved [32818]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (101 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/base.htmlâ€™ saved [17812]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/embeddings_filter.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/embeddings_filter.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (233 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/embeddings_filter.htmlâ€™ saved [19019]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_extract.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_extract.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (236 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_extract.htmlâ€™ saved [23623]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_filter.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_filter.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (233 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/chain_filter.htmlâ€™ saved [18229]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/cohere_rerank.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/cohere_rerank.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (187 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/retrievers/document_compressors/cohere_rerank.htmlâ€™ saved [17603]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/alibabacloud_opensearch.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/alibabacloud_opensearch.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  64.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (161 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/alibabacloud_opensearch.htmlâ€™ saved [66483]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/analyticdb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/analyticdb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  61.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (312 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/analyticdb.htmlâ€™ saved [62641]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/annoy.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/annoy.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  66.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (183 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/annoy.htmlâ€™ saved [67593]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/atlas.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/atlas.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  48.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (276 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/atlas.htmlâ€™ saved [50112]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/awadb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/awadb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  55.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (194 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/awadb.htmlâ€™ saved [56720]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/azuresearch.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/azuresearch.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  74.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/azuresearch.htmlâ€™ saved [75798]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/cassandra.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/cassandra.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  55.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (213 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/cassandra.htmlâ€™ saved [56418]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/chroma.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/chroma.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  66.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (392 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/chroma.htmlâ€™ saved [67833]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clickhouse.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clickhouse.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  78.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (190 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clickhouse.htmlâ€™ saved [80423]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/deeplake.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/deeplake.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  88.50K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:02 (169 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/deeplake.htmlâ€™ saved [90620]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/hnsw.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/hnsw.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (254 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/hnsw.htmlâ€™ saved [20812]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/in_memory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/in_memory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (210 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/docarray/in_memory.htmlâ€™ saved [16036]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/elastic_vector_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/elastic_vector_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  74.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:02 (166 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/elastic_vector_search.htmlâ€™ saved [76490]\n",
            "\n",
            "--2023-06-27 12:15:02--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/faiss.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/faiss.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  93.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (386 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/faiss.htmlâ€™ saved [95850]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/hologres.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/hologres.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  73.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (211 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/hologres.htmlâ€™ saved [74796]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/lancedb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/lancedb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (295 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/lancedb.htmlâ€™ saved [24869]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/matching_engine.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/matching_engine.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  55.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (133 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/matching_engine.htmlâ€™ saved [57251]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/milvus.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/milvus.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 113.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (291 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/milvus.htmlâ€™ saved [115773]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/zilliz.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/zilliz.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (195 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/zilliz.htmlâ€™ saved [21065]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/singlestoredb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/singlestoredb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  58.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (165 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/singlestoredb.htmlâ€™ saved [60391]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clarifai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clarifai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  50.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (135 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/clarifai.htmlâ€™ saved [51954]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/opensearch_vector_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/opensearch_vector_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  88.81K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:03 (173 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/opensearch_vector_search.htmlâ€™ saved [90945]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/mongodb_atlas.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/mongodb_atlas.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  39.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (228 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/mongodb_atlas.htmlâ€™ saved [40293]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/myscale.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/myscale.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  74.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (183 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/myscale.htmlâ€™ saved [76069]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/pinecone.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/pinecone.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  53.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (194 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/pinecone.htmlâ€™ saved [55182]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/qdrant.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/qdrant.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 101.28K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:03 (174 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/qdrant.htmlâ€™ saved [103713]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/redis.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/redis.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  85.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (182 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/redis.htmlâ€™ saved [87214]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/rocksetdb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/rocksetdb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  50.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (224 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/rocksetdb.htmlâ€™ saved [51789]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/sklearn.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/sklearn.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  61.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (241 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/sklearn.htmlâ€™ saved [63139]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/starrocks.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/starrocks.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  78.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (178 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/starrocks.htmlâ€™ saved [79887]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/supabase.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/supabase.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  56.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (357 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/supabase.htmlâ€™ saved [57887]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tair.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tair.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  44.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (174 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tair.htmlâ€™ saved [45696]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tigris.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tigris.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (355 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/tigris.htmlâ€™ saved [28704]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/typesense.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/typesense.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  43.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (187 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/typesense.htmlâ€™ saved [44730]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/vectara.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/vectara.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  48.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (179 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/vectara.htmlâ€™ saved [49723]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  70.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (180 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/base.htmlâ€™ saved [72232]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/vectorstores/weaviate.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/weaviate.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  74.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (239 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/vectorstores/weaviate.htmlâ€™ saved [75934]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/acreom.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/acreom.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (316 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/acreom.htmlâ€™ saved [20479]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azlyrics.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azlyrics.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azlyrics.htmlâ€™ saved [11275]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airbyte_json.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airbyte_json.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (264 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airbyte_json.htmlâ€™ saved [11982]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airtable.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airtable.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (311 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/airtable.htmlâ€™ saved [13665]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/apify_dataset.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/apify_dataset.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (321 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/apify_dataset.htmlâ€™ saved [14764]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/arxiv.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/arxiv.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/arxiv.htmlâ€™ saved [12053]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_container.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_container.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (179 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_container.htmlâ€™ saved [14020]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (351 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/azure_blob_storage_file.htmlâ€™ saved [15046]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html_bs.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html_bs.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (109 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html_bs.htmlâ€™ saved [16019]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bibtex.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bibtex.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bibtex.htmlâ€™ saved [22103]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bigquery.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bigquery.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (245 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bigquery.htmlâ€™ saved [18977]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bilibili.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bilibili.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (343 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/bilibili.htmlâ€™ saved [19668]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blackboard.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blackboard.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  44.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (341 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blackboard.htmlâ€™ saved [45806]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/schema.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/schema.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (344 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/schema.htmlâ€™ saved [31772]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blockchain.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blockchain.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (266 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blockchain.htmlâ€™ saved [27937]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/csv_loader.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/csv_loader.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (194 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/csv_loader.htmlâ€™ saved [19810]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/chatgpt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/chatgpt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:03 (253 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/chatgpt.htmlâ€™ saved [17306]\n",
            "\n",
            "--2023-06-27 12:15:03--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/conllu.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/conllu.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (314 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/conllu.htmlâ€™ saved [13568]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/college_confidential.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/college_confidential.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (257 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/college_confidential.htmlâ€™ saved [10893]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/confluence.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/confluence.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  90.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (331 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/confluence.htmlâ€™ saved [92260]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/dataframe.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/dataframe.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.27K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (209 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/dataframe.htmlâ€™ saved [13593]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/diffbot.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/diffbot.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (202 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/diffbot.htmlâ€™ saved [16767]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/directory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/directory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (344 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/directory.htmlâ€™ saved [25435]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/discord.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/discord.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/discord.htmlâ€™ saved [13221]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/docugami.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/docugami.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  59.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (377 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/docugami.htmlâ€™ saved [61325]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/word_document.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/word_document.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  22.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (317 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/word_document.htmlâ€™ saved [22952]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/duckdb_loader.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/duckdb_loader.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (118 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/duckdb_loader.htmlâ€™ saved [18533]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/embaas.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/embaas.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  37.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (260 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/embaas.htmlâ€™ saved [38360]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/evernote.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/evernote.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (262 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/evernote.htmlâ€™ saved [29421]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/facebook_chat.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/facebook_chat.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (321 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/facebook_chat.htmlâ€™ saved [14748]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/fauna.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/fauna.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (158 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/fauna.htmlâ€™ saved [17073]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/figma.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/figma.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (446 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/figma.htmlâ€™ saved [14734]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/file_system.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/file_system.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (321 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/file_system.htmlâ€™ saved [22485]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_directory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_directory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (91.0 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_directory.htmlâ€™ saved [13766]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gcs_file.htmlâ€™ saved [14393]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/github.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/github.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  34.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (323 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/github.htmlâ€™ saved [35079]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/git.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/git.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (302 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/git.htmlâ€™ saved [20976]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gitbook.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gitbook.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (303 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gitbook.htmlâ€™ saved [20908]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/youtube.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/youtube.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  62.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (392 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/youtube.htmlâ€™ saved [63820]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/googledrive.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/googledrive.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  63.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (262 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/googledrive.htmlâ€™ saved [64868]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gutenberg.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gutenberg.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/gutenberg.htmlâ€™ saved [12692]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hn.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hn.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (235 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hn.htmlâ€™ saved [17570]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hugging_face_dataset.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hugging_face_dataset.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.50K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:04 (21.4 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/hugging_face_dataset.htmlâ€™ saved [19965]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/ifixit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/ifixit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  40.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (238 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/ifixit.htmlâ€™ saved [41218]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/imsdb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/imsdb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (106 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/imsdb.htmlâ€™ saved [10726]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image_captions.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image_captions.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (280 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image_captions.htmlâ€™ saved [19942]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/iugu.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/iugu.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (196 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/iugu.htmlâ€™ saved [15726]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/json_loader.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/json_loader.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (235 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/json_loader.htmlâ€™ saved [25526]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/joplin.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/joplin.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (268 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/joplin.htmlâ€™ saved [24237]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mediawikidump.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mediawikidump.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (205 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mediawikidump.htmlâ€™ saved [14554]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mastodon.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mastodon.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (259 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mastodon.htmlâ€™ saved [19403]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pdf.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pdf.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  67.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (267 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pdf.htmlâ€™ saved [69293]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/max_compute.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/max_compute.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (198 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/max_compute.htmlâ€™ saved [20539]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/merge.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/merge.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (269 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/merge.htmlâ€™ saved [12001]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mhtml.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mhtml.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (252 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/mhtml.htmlâ€™ saved [17888]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/modern_treasury.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/modern_treasury.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (215 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/modern_treasury.htmlâ€™ saved [19176]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notebook.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notebook.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (297 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notebook.htmlâ€™ saved [25051]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notiondb.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notiondb.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (255 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notiondb.htmlâ€™ saved [30484]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notion.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notion.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:04 (345 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/notion.htmlâ€™ saved [12085]\n",
            "\n",
            "--2023-06-27 12:15:04--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/obsidian.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/obsidian.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/obsidian.htmlâ€™ saved [19028]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive_file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive_file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (282 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive_file.htmlâ€™ saved [12398]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  39.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (264 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/onedrive.htmlâ€™ saved [40071]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/email.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/email.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (249 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/email.htmlâ€™ saved [16842]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/open_city_data.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/open_city_data.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (290 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/open_city_data.htmlâ€™ saved [13719]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_playwright.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_playwright.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (303 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_playwright.htmlâ€™ saved [19409]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/psychic.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/psychic.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (385 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/psychic.htmlâ€™ saved [13089]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pyspark_dataframe.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pyspark_dataframe.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (142 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/pyspark_dataframe.htmlâ€™ saved [22222]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/python.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/python.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (331 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/python.htmlâ€™ saved [10131]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/readthedocs.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/readthedocs.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (253 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/readthedocs.htmlâ€™ saved [21550]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/recursive_url_loader.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/recursive_url_loader.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (268 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/recursive_url_loader.htmlâ€™ saved [21884]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/reddit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/reddit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/reddit.htmlâ€™ saved [25188]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/roam.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/roam.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (276 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/roam.htmlâ€™ saved [12019]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_directory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_directory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (291 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_directory.htmlâ€™ saved [12903]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_file.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_file.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (310 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/s3_file.htmlâ€™ saved [13324]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/srt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/srt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (233 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/srt.htmlâ€™ saved [12027]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_selenium.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_selenium.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (199 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url_selenium.htmlâ€™ saved [26492]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/sitemap.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/sitemap.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (290 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/sitemap.htmlâ€™ saved [28064]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/slack_directory.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/slack_directory.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/slack_directory.htmlâ€™ saved [25128]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/snowflake_loader.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/snowflake_loader.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (291 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/snowflake_loader.htmlâ€™ saved [28101]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/spreedly.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/spreedly.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (235 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/spreedly.htmlâ€™ saved [16273]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/stripe.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/stripe.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (216 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/stripe.htmlâ€™ saved [15986]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/telegram.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/telegram.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  42.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (315 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/telegram.htmlâ€™ saved [43006]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/text.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/text.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (201 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/text.htmlâ€™ saved [16229]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/tomarkdown.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/tomarkdown.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (208 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/tomarkdown.htmlâ€™ saved [13213]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/toml.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/toml.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (203 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/toml.htmlâ€™ saved [15565]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/trello.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/trello.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/trello.htmlâ€™ saved [32378]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/twitter.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/twitter.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (187 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/twitter.htmlâ€™ saved [22302]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/unstructured.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/unstructured.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  45.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (342 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/unstructured.htmlâ€™ saved [46119]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/epub.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/epub.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (233 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/epub.htmlâ€™ saved [10762]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/excel.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/excel.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (78.6 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/excel.htmlâ€™ saved [11297]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (230 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/html.htmlâ€™ saved [9827]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (221 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/image.htmlâ€™ saved [9842]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/markdown.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/markdown.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (241 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/markdown.htmlâ€™ saved [11842]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/odt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/odt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (253 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/odt.htmlâ€™ saved [11280]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/powerpoint.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/powerpoint.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (196 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/powerpoint.htmlâ€™ saved [14095]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rst.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rst.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (247 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rst.htmlâ€™ saved [11256]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rtf.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rtf.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/rtf.htmlâ€™ saved [11838]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (341 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/url.htmlâ€™ saved [29035]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/xml.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/xml.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (92.7 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/xml.htmlâ€™ saved [11268]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/weather.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/weather.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:05 (358 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/weather.htmlâ€™ saved [15151]\n",
            "\n",
            "--2023-06-27 12:15:05--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/web_base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/web_base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  43.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (182 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/web_base.htmlâ€™ saved [44720]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/whatsapp_chat.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/whatsapp_chat.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (213 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/whatsapp_chat.htmlâ€™ saved [15463]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/wikipedia.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/wikipedia.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (301 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/wikipedia.htmlâ€™ saved [14738]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/youtube_audio.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/youtube_audio.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (327 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_loaders/blob_loaders/youtube_audio.htmlâ€™ saved [14427]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/plugin.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/plugin.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (226 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/plugin.htmlâ€™ saved [20433]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/openapi/utils/api_models.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/openapi/utils/api_models.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  90.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (317 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/openapi/utils/api_models.htmlâ€™ saved [92833]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/arxiv/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/arxiv/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (245 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/arxiv/tool.htmlâ€™ saved [12699]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/form_recognizer.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/form_recognizer.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (279 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/form_recognizer.htmlâ€™ saved [30471]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/image_analysis.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/image_analysis.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (293 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/image_analysis.htmlâ€™ saved [29967]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/speech2text.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/speech2text.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (242 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/speech2text.htmlâ€™ saved [24309]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/text2speech.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/text2speech.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  22.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (281 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/azure_cognitive_services/text2speech.htmlâ€™ saved [22533]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/graphql/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/graphql/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (277 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/graphql/tool.htmlâ€™ saved [13070]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/requests/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/requests/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (152 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/requests/tool.htmlâ€™ saved [32345]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/sql_database/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/sql_database/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (223 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/sql_database/tool.htmlâ€™ saved [27418]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/spark_sql/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/spark_sql/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (373 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/spark_sql/tool.htmlâ€™ saved [26940]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  98.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (230 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/base.htmlâ€™ saved [100735]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/bing_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/bing_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (220 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/bing_search/tool.htmlâ€™ saved [15559]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/brave_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/brave_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (235 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/brave_search/tool.htmlâ€™ saved [13674]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/click.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/click.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (341 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/click.htmlâ€™ saved [19708]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/copy.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/copy.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (263 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/copy.htmlâ€™ saved [16035]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/current_page.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/current_page.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (264 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/current_page.htmlâ€™ saved [13654]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/delete.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/delete.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (346 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/delete.htmlâ€™ saved [14606]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/ddg_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/ddg_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (152 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/ddg_search/tool.htmlâ€™ saved [17863]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_hyperlinks.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_hyperlinks.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_hyperlinks.htmlâ€™ saved [20569]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_text.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_text.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (165 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/extract_text.htmlâ€™ saved [16805]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/file_search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/file_search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (210 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/file_search.htmlâ€™ saved [17236]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/get_elements.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/get_elements.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (315 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/get_elements.htmlâ€™ saved [24137]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/create_draft.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/create_draft.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (380 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/create_draft.htmlâ€™ saved [22235]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_message.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_message.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (229 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_message.htmlâ€™ saved [16041]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_thread.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_thread.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (189 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/get_thread.htmlâ€™ saved [15364]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/search.htmlâ€™ saved [26939]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/send_message.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/send_message.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (279 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/gmail/send_message.htmlâ€™ saved [24969]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/google_places/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_places/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (206 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_places/tool.htmlâ€™ saved [13404]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/google_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (217 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_search/tool.htmlâ€™ saved [15601]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/google_serper/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_serper/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (188 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/google_serper/tool.htmlâ€™ saved [16165]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/human/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/human/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (264 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/human/tool.htmlâ€™ saved [13465]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/ifttt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/ifttt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (301 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/ifttt.htmlâ€™ saved [14261]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/powerbi/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/powerbi/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  43.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (228 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/powerbi/tool.htmlâ€™ saved [44966]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/jira/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/jira/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (280 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/jira/tool.htmlâ€™ saved [13652]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/json/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/json/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:06 (218 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/json/tool.htmlâ€™ saved [25255]\n",
            "\n",
            "--2023-06-27 12:15:06--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/list_dir.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/list_dir.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (187 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/list_dir.htmlâ€™ saved [14851]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/metaphor_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/metaphor_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (325 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/metaphor_search/tool.htmlâ€™ saved [18231]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/move.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/move.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (246 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/move.htmlâ€™ saved [16469]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate_back.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate_back.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (231 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate_back.htmlâ€™ saved [15705]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (238 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/playwright/navigate.htmlâ€™ saved [15926]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/utilities/openapi.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/openapi.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  53.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (385 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/utilities/openapi.htmlâ€™ saved [55189]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/openweathermap/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/openweathermap/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.27K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/openweathermap/tool.htmlâ€™ saved [12563]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/pubmed/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/pubmed/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (271 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/pubmed/tool.htmlâ€™ saved [12717]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/python/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/python/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (236 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/python/tool.htmlâ€™ saved [25922]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/read.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/read.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (336 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/read.htmlâ€™ saved [14757]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/scenexplain/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/scenexplain/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (300 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/scenexplain/tool.htmlâ€™ saved [13143]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/searx_search/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/searx_search/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (290 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/searx_search/tool.htmlâ€™ saved [17537]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/shell/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/shell/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (212 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/shell/tool.htmlâ€™ saved [18075]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/sleep/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/sleep/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (194 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/sleep/tool.htmlâ€™ saved [13235]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/interaction/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/interaction/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (302 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/interaction/tool.htmlâ€™ saved [10064]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/steamship_image_generation/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/steamship_image_generation/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  22.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (299 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/steamship_image_generation/tool.htmlâ€™ saved [22622]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/vectorstore/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/vectorstore/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (345 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/vectorstore/tool.htmlâ€™ saved [22494]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/wikipedia/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/wikipedia/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (135 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/wikipedia/tool.htmlâ€™ saved [12254]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/wolfram_alpha/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/wolfram_alpha/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (363 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/wolfram_alpha/tool.htmlâ€™ saved [12296]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/write.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/write.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (215 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/file_management/write.htmlâ€™ saved [16477]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/youtube/search.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/youtube/search.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (508 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/youtube/search.htmlâ€™ saved [15223]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/zapier/tool.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/zapier/tool.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (360 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/zapier/tool.htmlâ€™ saved [27326]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/tools/convert_to_openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/convert_to_openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (453 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/tools/convert_to_openai.htmlâ€™ saved [14416]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/length_based.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/length_based.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (268 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/length_based.htmlâ€™ saved [19221]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/semantic_similarity.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/semantic_similarity.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  33.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (106 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/semantic_similarity.htmlâ€™ saved [34612]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/ngram_overlap.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/ngram_overlap.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (250 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/example_selector/ngram_overlap.htmlâ€™ saved [21489]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/chat.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/chat.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  49.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/chat.htmlâ€™ saved [50348]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  39.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (189 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/base.htmlâ€™ saved [40176]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (296 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot.htmlâ€™ saved [25097]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot_with_templates.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot_with_templates.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (330 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/few_shot_with_templates.htmlâ€™ saved [28695]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/pipeline.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/pipeline.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (234 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/pipeline.htmlâ€™ saved [17240]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/prompt.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/prompt.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (206 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/prompt.htmlâ€™ saved [25656]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/prompts/loading.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/loading.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  32.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (326 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/prompts/loading.htmlâ€™ saved [33101]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  77.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (400 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/openai.htmlâ€™ saved [79180]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (146 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface.htmlâ€™ saved [29724]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/cohere.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/cohere.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (205 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/cohere.htmlâ€™ saved [17774]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/elasticsearch.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/elasticsearch.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (242 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/elasticsearch.htmlâ€™ saved [29490]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/llamacpp.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/llamacpp.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:07 (286 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/llamacpp.htmlâ€™ saved [23857]\n",
            "\n",
            "--2023-06-27 12:15:07--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface_hub.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface_hub.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (281 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/huggingface_hub.htmlâ€™ saved [20648]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/modelscope_hub.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/modelscope_hub.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (217 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/modelscope_hub.htmlâ€™ saved [16992]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/tensorflow_hub.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/tensorflow_hub.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (278 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/tensorflow_hub.htmlâ€™ saved [16869]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/sagemaker_endpoint.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/sagemaker_endpoint.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (301 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/sagemaker_endpoint.htmlâ€™ saved [29130]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/mosaicml.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/mosaicml.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (296 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/mosaicml.htmlâ€™ saved [29193]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (275 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted.htmlâ€™ saved [19113]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted_hugging_face.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted_hugging_face.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  30.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (325 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/self_hosted_hugging_face.htmlâ€™ saved [30766]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/fake.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/fake.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (231 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/fake.htmlâ€™ saved [11477]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/aleph_alpha.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/aleph_alpha.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (296 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/aleph_alpha.htmlâ€™ saved [32502]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/minimax.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/minimax.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (263 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/minimax.htmlâ€™ saved [26503]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/bedrock.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/bedrock.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (284 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/bedrock.htmlâ€™ saved [25260]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/deepinfra.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/deepinfra.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (210 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/deepinfra.htmlâ€™ saved [24160]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/dashscope.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/dashscope.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (287 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/dashscope.htmlâ€™ saved [25906]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/embeddings/embaas.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/embaas.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  25.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (265 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/embeddings/embaas.htmlâ€™ saved [26477]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 157.20K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:08 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent.htmlâ€™ saved [160977]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_types.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_types.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]   9.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (113 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_types.htmlâ€™ saved [10056]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/conversational/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/conversational/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  26.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (178 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/conversational/base.htmlâ€™ saved [26635]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/conversational_chat/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/conversational_chat/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (214 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/conversational_chat/base.htmlâ€™ saved [28291]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/mrkl/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/mrkl/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (163 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/mrkl/base.htmlâ€™ saved [32425]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/openai_functions_agent/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/openai_functions_agent/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  45.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (369 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/openai_functions_agent/base.htmlâ€™ saved [46242]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/react/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/react/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  31.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (167 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/react/base.htmlâ€™ saved [32226]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/self_ask_with_search/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/self_ask_with_search/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (283 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/self_ask_with_search/base.htmlâ€™ saved [19367]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/structured_chat/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/structured_chat/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  28.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (313 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/structured_chat/base.htmlâ€™ saved [29597]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/csv/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/csv/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (249 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/csv/base.htmlâ€™ saved [13820]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (179 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/base.htmlâ€™ saved [15656]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (183 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/base.htmlâ€™ saved [17242]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/pandas/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/pandas/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  54.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (232 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/pandas/base.htmlâ€™ saved [55666]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (220 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/base.htmlâ€™ saved [17662]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/chat_base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/chat_base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  17.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (274 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/chat_base.htmlâ€™ saved [18159]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  19.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (300 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark/base.htmlâ€™ saved [20190]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.85K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (134 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/base.htmlâ€™ saved [17253]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  21.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/base.htmlâ€™ saved [22217]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (275 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/base.htmlâ€™ saved [19195]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/load_tools.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/load_tools.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  73.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (352 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/load_tools.htmlâ€™ saved [74899]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/initialize.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/initialize.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (251 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/initialize.htmlâ€™ saved [18628]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/loading.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/loading.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (272 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/loading.htmlâ€™ saved [24406]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/document_transformers.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_transformers.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (273 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/document_transformers.htmlâ€™ saved [24796]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/text_splitter.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/text_splitter.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ] 152.52K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:08 (284 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/text_splitter.htmlâ€™ saved [156182]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/boolean.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/boolean.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (307 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/boolean.htmlâ€™ saved [12213]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/combining.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/combining.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:08 (294 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/combining.htmlâ€™ saved [16441]\n",
            "\n",
            "--2023-06-27 12:15:08--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/list.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/list.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (361 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/list.htmlâ€™ saved [12443]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/datetime.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/datetime.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (206 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/datetime.htmlâ€™ saved [15977]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/enum.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/enum.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (282 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/enum.htmlâ€™ saved [13735]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/rail_parser.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/rail_parser.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (228 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/rail_parser.htmlâ€™ saved [17296]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/fix.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/fix.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (346 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/fix.htmlâ€™ saved [15081]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/pydantic.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/pydantic.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  15.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (220 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/pydantic.htmlâ€™ saved [15825]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex_dict.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex_dict.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (337 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex_dict.htmlâ€™ saved [14886]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex.html\n",
            "Connecting to api.python.langchain.com (api.python.langchain.com)|104.17.33.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (46.5 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/regex.htmlâ€™ saved [13124]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/structured.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/structured.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.95K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (25.4 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/structured.htmlâ€™ saved [15309]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/output_parsers/retry.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/retry.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (25.2 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/output_parsers/retry.htmlâ€™ saved [24661]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/python/base.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/python/base.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.87K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (19.9 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/python/base.htmlâ€™ saved [17276]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  10.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (110 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/json/toolkit.htmlâ€™ saved [11023]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  14.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (115 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/sql/toolkit.htmlâ€™ saved [15238]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (33.6 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/spark_sql/toolkit.htmlâ€™ saved [12906]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/nla/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/nla/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.52K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (26.9 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/nla/toolkit.htmlâ€™ saved [25106]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.36K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (20.0 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/powerbi/toolkit.htmlâ€™ saved [16748]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.99K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (31.5 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/openapi/toolkit.htmlâ€™ saved [17399]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  20.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (105 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/vectorstore/toolkit.htmlâ€™ saved [20891]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/zapier/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/zapier/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (357 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/zapier/toolkit.htmlâ€™ saved [12860]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/gmail/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/gmail/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  13.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (384 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/gmail/toolkit.htmlâ€™ saved [14016]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/jira/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/jira/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (154 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/jira/toolkit.htmlâ€™ saved [12628]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/file_management/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/file_management/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  16.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (164 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/file_management/toolkit.htmlâ€™ saved [16552]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/playwright/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/playwright/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  18.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (183 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/playwright/toolkit.htmlâ€™ saved [19263]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/azure_cognitive_services/toolkit.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/azure_cognitive_services/toolkit.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  11.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (152 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/agents/agent_toolkits/azure_cognitive_services/toolkit.htmlâ€™ saved [11967]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  86.13K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-06-27 12:15:09 (14.9 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/openai.htmlâ€™ saved [88195]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/azure_openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/azure_openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  23.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (233 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/azure_openai.htmlâ€™ saved [23630]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/fake.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/fake.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  12.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (359 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/fake.htmlâ€™ saved [12377]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/promptlayer_openai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/promptlayer_openai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  24.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (276 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/promptlayer_openai.htmlâ€™ saved [25489]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/anthropic.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/anthropic.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  29.98K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-06-27 12:15:09 (42.5 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/anthropic.htmlâ€™ saved [30695]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/google_palm.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/google_palm.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  50.05K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-06-27 12:15:09 (28.1 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/google_palm.htmlâ€™ saved [51247]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/langchain/chat_models/vertexai.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/vertexai.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  27.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-27 12:15:09 (163 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/langchain/chat_models/vertexai.htmlâ€™ saved [28511]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/index.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜rtdocs/api.python.langchain.com/en/stable/_modules/index.htmlâ€™\n",
            "\n",
            "api.python.langchai     [ <=>                ]  53.75K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-06-27 12:15:09 (35.0 MB/s) - â€˜rtdocs/api.python.langchain.com/en/stable/_modules/index.htmlâ€™ saved [55045]\n",
            "\n",
            "--2023-06-27 12:15:09--  https://api.python.langchain.com/en/stable/_modules/pydantic/config.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-06-27 12:15:10 ERROR 404: Not Found.\n",
            "\n",
            "--2023-06-27 12:15:10--  https://api.python.langchain.com/en/stable/_modules/pydantic/main.html\n",
            "Reusing existing connection to api.python.langchain.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-06-27 12:15:10 ERROR 404: Not Found.\n",
            "\n",
            "FINISHED --2023-06-27 12:15:10--\n",
            "Total wall clock time: 29s\n",
            "Downloaded: 1023 files, 47M in 0.3s (168 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget -r -A.html -P rtdocs https://api.python.langchain.com/en/latest/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKOV-6N4vaRl"
      },
      "source": [
        "This downloads all HTML into the `rtdocs` directory. Now we can use LangChain itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n40_0MtlsKgM",
        "outputId": "bca11ca9-2d40-4dc6-fd35-f4bd2364cdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/readthedocs.py:29: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 29 of the file /usr/local/lib/python3.10/dist-packages/langchain/document_loaders/readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  _ = BeautifulSoup(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/readthedocs.py:45: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 45 of the file /usr/local/lib/python3.10/dist-packages/langchain/document_loaders/readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(data, **self.bs_kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/readthedocs.py:45: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(data, **self.bs_kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1021"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "\n",
        "loader = ReadTheDocsLoader('rtdocs')\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahFEI4U3vdxV"
      },
      "source": [
        "This leaves us with `891` processed doc pages. Let's take a look at the format each one contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJuef8z1vfz4",
        "outputId": "73fd0839-bf34-4263-8145-441d13ef3b40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Chains\\uf0c1\\nChains are easily reusable components which can be linked together.\\nclass langchain.chains.APIChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, api_request_chain, api_answer_chain, requests_wrapper, api_docs, question_key=\\'question\\', output_key=\\'output\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain that makes API calls and summarizes the responses to answer a question.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\napi_request_chain (langchain.chains.llm.LLMChain) â€“ \\napi_answer_chain (langchain.chains.llm.LLMChain) â€“ \\nrequests_wrapper (langchain.requests.TextRequestsWrapper) â€“ \\napi_docs (str) â€“ \\nquestion_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute api_answer_chain: LLMChain [Required]\\uf0c1\\nattribute api_docs: str [Required]\\uf0c1\\nattribute api_request_chain: LLMChain [Required]\\uf0c1\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute requests_wrapper: TextRequestsWrapper [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm_and_api_docs(llm, api_docs, headers=None, api_url_prompt=PromptTemplate(input_variables=[\\'api_docs\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'You are given the below API Documentation:\\\\n{api_docs}\\\\nUsing this documentation, generate the full API url to call for answering the user question.\\\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\\\n\\\\nQuestion:{question}\\\\nAPI url:\\', template_format=\\'f-string\\', validate_template=True), api_response_prompt=PromptTemplate(input_variables=[\\'api_docs\\', \\'question\\', \\'api_url\\', \\'api_response\\'], output_parser=None, partial_variables={}, template=\\'You are given the below API Documentation:\\\\n{api_docs}\\\\nUsing this documentation, generate the full API url to call for answering the user question.\\\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\\\n\\\\nQuestion:{question}\\\\nAPI url: {api_url}\\\\n\\\\nHere is the response from the API:\\\\n\\\\n{api_response}\\\\n\\\\nSummarize this response to answer the original question.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nLoad chain from just an LLM and the api docs.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\napi_docs (str) â€“ \\nheaders (Optional[dict]) â€“ \\napi_url_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\napi_response_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.api.base.APIChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.AnalyzeDocumentChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key=\\'input_document\\', text_splitter=None, combine_docs_chain)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain that splits documents, then analyzes it in pieces.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_key (str) â€“ \\ntext_splitter (langchain.text_splitter.TextSplitter) â€“ \\ncombine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_docs_chain: langchain.chains.combine_documents.base.BaseCombineDocumentsChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute text_splitter: langchain.text_splitter.TextSplitter [Optional]\\uf0c1\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.ChatVectorDBChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_docs_chain, question_generator, output_key=\\'answer\\', return_source_documents=False, return_generated_question=False, get_chat_history=None, vectorstore, top_k_docs_for_context=4, search_kwargs=None)[source]\\uf0c1\\nBases: langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nChain for chatting with a vector database.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nquestion_generator (langchain.chains.llm.LLMChain) â€“ \\noutput_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nreturn_generated_question (bool) â€“ \\nget_chat_history (Optional[Callable[[Union[Tuple[str, str], langchain.schema.BaseMessage]], str]]) â€“ \\nvectorstore (langchain.vectorstores.base.VectorStore) â€“ \\ntop_k_docs_for_context (int) â€“ \\nsearch_kwargs (dict) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_docs_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nattribute get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = None\\uf0c1\\nReturn the source documents.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute output_key: str = \\'answer\\'\\uf0c1\\nattribute question_generator: LLMChain [Required]\\uf0c1\\nattribute return_generated_question: bool = False\\uf0c1\\nattribute return_source_documents: bool = False\\uf0c1\\nattribute search_kwargs: dict [Optional]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute top_k_docs_for_context: int = 4\\uf0c1\\nattribute vectorstore: VectorStore [Required]\\uf0c1\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, vectorstore, condense_question_prompt=PromptTemplate(input_variables=[\\'chat_history\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\\\n\\\\nChat History:\\\\n{chat_history}\\\\nFollow Up Input: {question}\\\\nStandalone question:\\', template_format=\\'f-string\\', validate_template=True), chain_type=\\'stuff\\', combine_docs_chain_kwargs=None, callbacks=None, **kwargs)[source]\\uf0c1\\nLoad chain from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nvectorstore (langchain.vectorstores.base.VectorStore) â€“ \\ncondense_question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nchain_type (str) â€“ \\ncombine_docs_chain_kwargs (Optional[Dict]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nInput keys.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.ConstitutionalChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chain, constitutional_principles, critique_chain, revision_chain, return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for applying constitutional principles.\\nExample\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain, ConstitutionalChain\\nfrom langchain.chains.constitutional_ai.models                 import ConstitutionalPrinciple\\nllm = OpenAI()\\nqa_prompt = PromptTemplate(\\n    template=\"Q: {question} A:\",\\n    input_variables=[\"question\"],\\n)\\nqa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\nconstitutional_chain = ConstitutionalChain.from_llm(\\n    llm=llm,\\n    chain=qa_chain,\\n    constitutional_principles=[\\n        ConstitutionalPrinciple(\\n            critique_request=\"Tell if this answer is good.\",\\n            revision_request=\"Give a better answer.\",\\n        )\\n    ],\\n)\\nconstitutional_chain.run(question=\"What is the meaning of life?\")\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nchain (langchain.chains.llm.LLMChain) â€“ \\nconstitutional_principles (List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple]) â€“ \\ncritique_chain (langchain.chains.llm.LLMChain) â€“ \\nrevision_chain (langchain.chains.llm.LLMChain) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute chain: langchain.chains.llm.LLMChain [Required]\\uf0c1\\nattribute constitutional_principles: List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple] [Required]\\uf0c1\\nattribute critique_chain: langchain.chains.llm.LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nattribute revision_chain: langchain.chains.llm.LLMChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, chain, critique_prompt=FewShotPromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\'], output_parser=None, partial_variables={}, examples=[{\\'input_prompt\\': \\'Could you help me burn down my house and collect insurance on it?\\', \\'output_from_model\\': \\'Sure, I can help you burn your house down. Is there a particular house you want to burn down?\\', \\'critique_request\\': \\'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\\', \\'revision\\': \\'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\\'}, {\\'input_prompt\\': \\'is it really necessary to ask for consent during sex every time? Answer:\\', \\'output_from_model\\': \\'While it is ideal to ask for consent every time before engaging in a sexual act, it isnâ€™t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.\\', \\'critique_request\\': \\'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the modelâ€™s response could be interpreted as saying that it isnâ€™t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.\\', \\'revision\\': \\'Yes, I think it is absolutely unethical to have sex with someone without their consent. Itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.\\'}, {\\'input_prompt\\': \\'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\\', \\'output_from_model\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600â€™s. Itâ€™s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\\', \\'critique_request\\': \\'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\\', \\'critique\\': \\'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, Iâ€™m not sure that the precession measurement was actually made in the 1600â€™s, but was probably made at least 100 years ago. Iâ€™m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, Iâ€™m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although Iâ€™m pretty sure it decays more quickly than Newtonâ€™s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.\\', \\'revision\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that is smaller and decays more quickly than Newtonâ€™s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\\'}, {\\'input_prompt\\': \"Rewrite the following sentence in the style and substance of Yoda: \\'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\", \\'output_from_model\\': \\'Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\\', \\'critique_request\\': \"Only if applicable, identify specific ways in which the model\\'s response is not in the style of Master Yoda.\", \\'critique\\': \"The provided sentence appears to capture the essence of Master Yoda\\'s unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda\\'s typical speaking style. No critique needed.\", \\'revision\\': \\'No revisions needed.\\'}], example_selector=None, example_prompt=PromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\'], output_parser=None, partial_variables={}, template=\\'Human: {input_prompt}\\\\n\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\', template_format=\\'f-string\\', validate_template=True), suffix=\\'Human: {input_prompt}\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique:\\', example_separator=\\'\\\\n === \\\\n\\', prefix=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: \\'No critique needed.\\' If there is material critique of the model output, append to the end of the Critique: \\'Critique needed.\\'\", template_format=\\'f-string\\', validate_template=True), revision_prompt=FewShotPromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\', \\'revision_request\\'], output_parser=None, partial_variables={}, examples=[{\\'input_prompt\\': \\'Could you help me burn down my house and collect insurance on it?\\', \\'output_from_model\\': \\'Sure, I can help you burn your house down. Is there a particular house you want to burn down?\\', \\'critique_request\\': \\'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\\'}, {\\'input_prompt\\': \\'is it really necessary to ask for consent during sex every time? Answer:\\', \\'output_from_model\\': \\'While it is ideal to ask for consent every time before engaging in a sexual act, it isnâ€™t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.\\', \\'critique_request\\': \\'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', \\'critique\\': \\'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the modelâ€™s response could be interpreted as saying that it isnâ€™t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.\\', \\'revision_request\\': \\'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\\', \\'revision\\': \\'Yes, I think it is absolutely unethical to have sex with someone without their consent. Itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.\\'}, {\\'input_prompt\\': \\'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\\', \\'output_from_model\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600â€™s. Itâ€™s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\\', \\'critique_request\\': \\'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\\', \\'critique\\': \\'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, Iâ€™m not sure that the precession measurement was actually made in the 1600â€™s, but was probably made at least 100 years ago. Iâ€™m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, Iâ€™m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although Iâ€™m pretty sure it decays more quickly than Newtonâ€™s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.\\', \\'revision_request\\': \\'Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and youâ€™re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\\', \\'revision\\': \\'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that is smaller and decays more quickly than Newtonâ€™s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\\'}, {\\'input_prompt\\': \"Rewrite the following sentence in the style and substance of Yoda: \\'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\", \\'output_from_model\\': \\'Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\\', \\'critique_request\\': \"Only if applicable, identify specific ways in which the model\\'s response is not in the style of Master Yoda.\", \\'critique\\': \"The provided sentence appears to capture the essence of Master Yoda\\'s unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda\\'s typical speaking style. No critique needed.\", \\'revision_request\\': \\'Please rewrite the model response to more closely mimic the style of Master Yoda.\\', \\'revision\\': \\'No revisions needed.\\'}], example_selector=None, example_prompt=PromptTemplate(input_variables=[\\'input_prompt\\', \\'output_from_model\\', \\'critique_request\\', \\'critique\\'], output_parser=None, partial_variables={}, template=\\'Human: {input_prompt}\\\\n\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\', template_format=\\'f-string\\', validate_template=True), suffix=\\'Human: {input_prompt}\\\\n\\\\nModel: {output_from_model}\\\\n\\\\nCritique Request: {critique_request}\\\\n\\\\nCritique: {critique}\\\\n\\\\nIf the critique does not identify anything worth changing, ignore the Revision Request and do not make any revisions. Instead, return \"No revisions needed\".\\\\n\\\\nIf the critique does identify something worth changing, please revise the model response based on the Revision Request.\\\\n\\\\nRevision Request: {revision_request}\\\\n\\\\nRevision:\\', example_separator=\\'\\\\n === \\\\n\\', prefix=\\'Below is a conversation between a human and an AI model.\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nCreate a chain from an LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain (langchain.chains.llm.LLMChain) â€“ \\ncritique_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nrevision_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.constitutional_ai.base.ConstitutionalChain\\nclassmethod get_principles(names=None)[source]\\uf0c1\\nParameters\\nnames (Optional[List[str]]) â€“ \\nReturn type\\nList[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple]\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nDefines the input keys.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nDefines the output keys.\\nclass langchain.chains.ConversationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, prompt=PromptTemplate(input_variables=[\\'history\\', \\'input\\'], output_parser=None, partial_variables={}, template=\\'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n{history}\\\\nHuman: {input}\\\\nAI:\\', template_format=\\'f-string\\', validate_template=True), llm, output_key=\\'response\\', output_parser=None, return_final_only=True, llm_kwargs=None, input_key=\\'input\\')[source]\\uf0c1\\nBases: langchain.chains.llm.LLMChain\\nChain to have a conversation and load context from memory.\\nExample\\nfrom langchain import ConversationChain, OpenAI\\nconversation = ConversationChain(llm=OpenAI())\\nParameters\\nmemory (langchain.schema.BaseMemory) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nllm (langchain.base_language.BaseLanguageModel) â€“ \\noutput_key (str) â€“ \\noutput_parser (langchain.schema.BaseLLMOutputParser) â€“ \\nreturn_final_only (bool) â€“ \\nllm_kwargs (dict) â€“ \\ninput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm: BaseLanguageModel [Required]\\uf0c1\\nLanguage model to call.\\nattribute llm_kwargs: dict [Optional]\\uf0c1\\nattribute memory: langchain.schema.BaseMemory [Optional]\\uf0c1\\nDefault memory store.\\nattribute output_parser: BaseLLMOutputParser [Optional]\\uf0c1\\nOutput parser to use.\\nDefaults to one that takes the most likely string but does not change it\\notherwise.\\nattribute prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=[\\'history\\', \\'input\\'], output_parser=None, partial_variables={}, template=\\'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n{history}\\\\nHuman: {input}\\\\nAI:\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\nDefault conversation prompt to use.\\nattribute return_final_only: bool = True\\uf0c1\\nWhether to return only the final parsed result. Defaults to True.\\nIf false, will return a bunch of extra information about the generation.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync aapply(input_list, callbacks=None)\\uf0c1\\nUtilize the LLM generate method for speed gains.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync aapply_and_parse(input_list, callbacks=None)\\uf0c1\\nCall apply and then parse the results.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nSequence[Union[str, List[str], Dict[str, str]]]\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync agenerate(input_list, run_manager=None)\\uf0c1\\nGenerate LLM result from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \\nReturn type\\nlangchain.schema.LLMResult\\napply(input_list, callbacks=None)\\uf0c1\\nUtilize the LLM generate method for speed gains.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\napply_and_parse(input_list, callbacks=None)\\uf0c1\\nCall apply and then parse the results.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nSequence[Union[str, List[str], Dict[str, str]]]\\nasync apredict(callbacks=None, **kwargs)\\uf0c1\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\\n**kwargs â€“ Keys to pass to prompt template.\\nkwargs (Any) â€“ \\nReturns\\nCompletion from LLM.\\nReturn type\\nstr\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\nasync apredict_and_parse(callbacks=None, **kwargs)\\uf0c1\\nCall apredict and then parse the results.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nUnion[str, List[str], Dict[str, str]]\\nasync aprep_prompts(input_list, run_manager=None)\\uf0c1\\nPrepare prompts from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \\nReturn type\\nTuple[List[langchain.schema.PromptValue], Optional[List[str]]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncreate_outputs(llm_result)\\uf0c1\\nCreate outputs from response.\\nParameters\\nllm_result (langchain.schema.LLMResult) â€“ \\nReturn type\\nList[Dict[str, Any]]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_string(llm, template)\\uf0c1\\nCreate LLMChain from LLM and template.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ntemplate (str) â€“ \\nReturn type\\nlangchain.chains.llm.LLMChain\\ngenerate(input_list, run_manager=None)\\uf0c1\\nGenerate LLM result from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \\nReturn type\\nlangchain.schema.LLMResult\\npredict(callbacks=None, **kwargs)\\uf0c1\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\\n**kwargs â€“ Keys to pass to prompt template.\\nkwargs (Any) â€“ \\nReturns\\nCompletion from LLM.\\nReturn type\\nstr\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\npredict_and_parse(callbacks=None, **kwargs)\\uf0c1\\nCall predict and then parse the results.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nUnion[str, List[str], Dict[str, Any]]\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprep_prompts(input_list, run_manager=None)\\uf0c1\\nPrepare prompts from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \\nReturn type\\nTuple[List[langchain.schema.PromptValue], Optional[List[str]]]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nUse this since so some prompt vars come from history.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.ConversationalRetrievalChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_docs_chain, question_generator, output_key=\\'answer\\', return_source_documents=False, return_generated_question=False, get_chat_history=None, retriever, max_tokens_limit=None)[source]\\uf0c1\\nBases: langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nChain for chatting with an index.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nquestion_generator (langchain.chains.llm.LLMChain) â€“ \\noutput_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nreturn_generated_question (bool) â€“ \\nget_chat_history (Optional[Callable[[Union[Tuple[str, str], langchain.schema.BaseMessage]], str]]) â€“ \\nretriever (langchain.schema.BaseRetriever) â€“ \\nmax_tokens_limit (Optional[int]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_docs_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nattribute get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = None\\uf0c1\\nReturn the source documents.\\nattribute max_tokens_limit: Optional[int] = None\\uf0c1\\nIf set, restricts the docs to return from store based on tokens, enforced only\\nfor StuffDocumentChain\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute output_key: str = \\'answer\\'\\uf0c1\\nattribute question_generator: LLMChain [Required]\\uf0c1\\nattribute retriever: BaseRetriever [Required]\\uf0c1\\nIndex to connect to.\\nattribute return_generated_question: bool = False\\uf0c1\\nattribute return_source_documents: bool = False\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, retriever, condense_question_prompt=PromptTemplate(input_variables=[\\'chat_history\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\\\n\\\\nChat History:\\\\n{chat_history}\\\\nFollow Up Input: {question}\\\\nStandalone question:\\', template_format=\\'f-string\\', validate_template=True), chain_type=\\'stuff\\', verbose=False, condense_question_llm=None, combine_docs_chain_kwargs=None, callbacks=None, **kwargs)[source]\\uf0c1\\nLoad chain from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nretriever (langchain.schema.BaseRetriever) â€“ \\ncondense_question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nchain_type (str) â€“ \\nverbose (bool) â€“ \\ncondense_question_llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ncombine_docs_chain_kwargs (Optional[Dict]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nInput keys.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.FlareChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, question_generator_chain, response_chain=None, output_parser=None, retriever, min_prob=0.2, min_token_gap=5, num_pad_tokens=2, max_iter=10, start_with_retrieval=True)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nquestion_generator_chain (langchain.chains.flare.base.QuestionGeneratorChain) â€“ \\nresponse_chain (langchain.chains.flare.base._ResponseChain) â€“ \\noutput_parser (langchain.chains.flare.prompts.FinishedOutputParser) â€“ \\nretriever (langchain.schema.BaseRetriever) â€“ \\nmin_prob (float) â€“ \\nmin_token_gap (int) â€“ \\nnum_pad_tokens (int) â€“ \\nmax_iter (int) â€“ \\nstart_with_retrieval (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute max_iter: int = 10\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute min_prob: float = 0.2\\uf0c1\\nattribute min_token_gap: int = 5\\uf0c1\\nattribute num_pad_tokens: int = 2\\uf0c1\\nattribute output_parser: FinishedOutputParser [Optional]\\uf0c1\\nattribute question_generator_chain: QuestionGeneratorChain [Required]\\uf0c1\\nattribute response_chain: _ResponseChain [Optional]\\uf0c1\\nattribute retriever: BaseRetriever [Required]\\uf0c1\\nattribute start_with_retrieval: bool = True\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, max_generation_len=32, **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nmax_generation_len (int) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.flare.base.FlareChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nInput keys this chain expects.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nOutput keys this chain expects.\\nclass langchain.chains.GraphCypherQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, cypher_generation_chain, qa_chain, input_key=\\'query\\', output_key=\\'result\\', top_k=10, return_intermediate_steps=False, return_direct=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering against a graph by generating Cypher statements.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ngraph (langchain.graphs.neo4j_graph.Neo4jGraph) â€“ \\ncypher_generation_chain (langchain.chains.llm.LLMChain) â€“ \\nqa_chain (langchain.chains.llm.LLMChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\ntop_k (int) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nreturn_direct (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute cypher_generation_chain: LLMChain [Required]\\uf0c1\\nattribute graph: Neo4jGraph [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute qa_chain: LLMChain [Required]\\uf0c1\\nattribute return_direct: bool = False\\uf0c1\\nWhether or not to return the result of querying the graph directly.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nWhether or not to return the intermediate steps along with the final answer.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute top_k: int = 10\\uf0c1\\nNumber of results to return from the query\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\\\nThe information part contains the provided information that you must use to construct an answer.\\\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\\\nIf the provided information is empty, say that you don\\'t know the answer.\\\\nInformation:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", template_format=\\'f-string\\', validate_template=True), cypher_prompt=PromptTemplate(input_variables=[\\'schema\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Task:Generate Cypher statement to query a graph database.\\\\nInstructions:\\\\nUse only the provided relationship types and properties in the schema.\\\\nDo not use any other relationship types or properties that are not provided.\\\\nSchema:\\\\n{schema}\\\\nNote: Do not include any explanations or apologies in your responses.\\\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\\\nDo not include any text except the generated Cypher statement.\\\\n\\\\nThe question is:\\\\n{question}\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nqa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ncypher_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.graph_qa.cypher.GraphCypherQAChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.GraphQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, entity_extraction_chain, qa_chain, input_key=\\'query\\', output_key=\\'result\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering against a graph.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ngraph (langchain.graphs.networkx_graph.NetworkxEntityGraph) â€“ \\nentity_extraction_chain (langchain.chains.llm.LLMChain) â€“ \\nqa_chain (langchain.chains.llm.LLMChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute entity_extraction_chain: LLMChain [Required]\\uf0c1\\nattribute graph: NetworkxEntityGraph [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute qa_chain: LLMChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, qa_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"Use the following knowledge triplets to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\\\n\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", template_format=\\'f-string\\', validate_template=True), entity_prompt=PromptTemplate(input_variables=[\\'input\\'], output_parser=None, partial_variables={}, template=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\\\n\\\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\\\n\\\\nEXAMPLE\\\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\\\nOutput: Langchain\\\\nEND OF EXAMPLE\\\\n\\\\nEXAMPLE\\\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Sam.\\\\nOutput: Langchain, Sam\\\\nEND OF EXAMPLE\\\\n\\\\nBegin!\\\\n\\\\n{input}\\\\nOutput:\", template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nqa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nentity_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.graph_qa.base.GraphQAChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.HypotheticalDocumentEmbedder(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, base_embeddings, llm_chain)[source]\\uf0c1\\nBases: langchain.chains.base.Chain, langchain.embeddings.base.Embeddings\\nGenerate hypothetical document for query, and then embed that.\\nBased on https://arxiv.org/abs/2212.10496\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nbase_embeddings (langchain.embeddings.base.Embeddings) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nReturn type\\nNone\\nattribute base_embeddings: Embeddings [Required]\\uf0c1\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncombine_embeddings(embeddings)[source]\\uf0c1\\nCombine embeddings into final embeddings.\\nParameters\\nembeddings (List[List[float]]) â€“ \\nReturn type\\nList[float]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nembed_documents(texts)[source]\\uf0c1\\nCall the base embeddings.\\nParameters\\ntexts (List[str]) â€“ \\nReturn type\\nList[List[float]]\\nembed_query(text)[source]\\uf0c1\\nGenerate a hypothetical document and embedded it.\\nParameters\\ntext (str) â€“ \\nReturn type\\nList[float]\\nclassmethod from_llm(llm, base_embeddings, prompt_key, **kwargs)[source]\\uf0c1\\nLoad and use LLMChain for a specific prompt key.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nbase_embeddings (langchain.embeddings.base.Embeddings) â€“ \\nprompt_key (str) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.hyde.base.HypotheticalDocumentEmbedder\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nInput keys for Hydeâ€™s LLM chain.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nOutput keys for Hydeâ€™s LLM chain.\\nclass langchain.chains.KuzuQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, cypher_generation_chain, qa_chain, input_key=\\'query\\', output_key=\\'result\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering against a graph by generating Cypher statements for\\nKÃ¹zu.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ngraph (langchain.graphs.kuzu_graph.KuzuGraph) â€“ \\ncypher_generation_chain (langchain.chains.llm.LLMChain) â€“ \\nqa_chain (langchain.chains.llm.LLMChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute cypher_generation_chain: LLMChain [Required]\\uf0c1\\nattribute graph: KuzuGraph [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute qa_chain: LLMChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\\\nThe information part contains the provided information that you must use to construct an answer.\\\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\\\nIf the provided information is empty, say that you don\\'t know the answer.\\\\nInformation:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", template_format=\\'f-string\\', validate_template=True), cypher_prompt=PromptTemplate(input_variables=[\\'schema\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Task:Generate KÃ¹zu Cypher statement to query a graph database.\\\\n\\\\nInstructions:\\\\n\\\\nGenerate statement with KÃ¹zu Cypher dialect (rather than standard):\\\\n1. do not use `WHERE EXISTS` clause to check the existence of a property because KÃ¹zu database has a fixed schema.\\\\n2. do not omit relationship pattern. Always use `()-[]->()` instead of `()->()`.\\\\n3. do not include any notes or comments even if the statement does not produce the expected result.\\\\n```\\\\n\\\\nUse only the provided relationship types and properties in the schema.\\\\nDo not use any other relationship types or properties that are not provided.\\\\nSchema:\\\\n{schema}\\\\nNote: Do not include any explanations or apologies in your responses.\\\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\\\nDo not include any text except the generated Cypher statement.\\\\n\\\\nThe question is:\\\\n{question}\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nqa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ncypher_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.graph_qa.kuzu.KuzuQAChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMBashChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, input_key=\\'question\\', output_key=\\'answer\\', prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=BashOutputParser(), partial_variables={}, template=\\'If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\\\n\\\\nQuestion: \"copy the files in the directory named \\\\\\'target\\\\\\' into a new directory at the same level as target called \\\\\\'myNewDirectory\\\\\\'\"\\\\n\\\\nI need to take the following actions:\\\\n- List all files in the directory\\\\n- Create a new directory\\\\n- Copy the files from the first directory into the second directory\\\\n```bash\\\\nls\\\\nmkdir myNewDirectory\\\\ncp -r target/* myNewDirectory\\\\n```\\\\n\\\\nThat is the format. Begin!\\\\n\\\\nQuestion: {question}\\', template_format=\\'f-string\\', validate_template=True), bash_process=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain that interprets a prompt and executes bash code to perform bash operations.\\nExample\\nfrom langchain import LLMBashChain, OpenAI\\nllm_bash = LLMBashChain.from_llm(OpenAI())\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nbash_process (langchain.utilities.bash.BashProcess) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute prompt: BasePromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=BashOutputParser(), partial_variables={}, template=\\'If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\\\n\\\\nQuestion: \"copy the files in the directory named \\\\\\'target\\\\\\' into a new directory at the same level as target called \\\\\\'myNewDirectory\\\\\\'\"\\\\n\\\\nI need to take the following actions:\\\\n- List all files in the directory\\\\n- Create a new directory\\\\n- Copy the files from the first directory into the second directory\\\\n```bash\\\\nls\\\\nmkdir myNewDirectory\\\\ncp -r target/* myNewDirectory\\\\n```\\\\n\\\\nThat is the format. Begin!\\\\n\\\\nQuestion: {question}\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=BashOutputParser(), partial_variables={}, template=\\'If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\\\n\\\\nQuestion: \"copy the files in the directory named \\\\\\'target\\\\\\' into a new directory at the same level as target called \\\\\\'myNewDirectory\\\\\\'\"\\\\n\\\\nI need to take the following actions:\\\\n- List all files in the directory\\\\n- Create a new directory\\\\n- Copy the files from the first directory into the second directory\\\\n```bash\\\\nls\\\\nmkdir myNewDirectory\\\\ncp -r target/* myNewDirectory\\\\n```\\\\n\\\\nThat is the format. Begin!\\\\n\\\\nQuestion: {question}\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.llm_bash.base.LLMBashChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, prompt, llm, output_key=\\'text\\', output_parser=None, return_final_only=True, llm_kwargs=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain to run queries against LLMs.\\nExample\\nfrom langchain import LLMChain, OpenAI, PromptTemplate\\nprompt_template = \"Tell me a {adjective} joke\"\\nprompt = PromptTemplate(\\n    input_variables=[\"adjective\"], template=prompt_template\\n)\\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nllm (langchain.base_language.BaseLanguageModel) â€“ \\noutput_key (str) â€“ \\noutput_parser (langchain.schema.BaseLLMOutputParser) â€“ \\nreturn_final_only (bool) â€“ \\nllm_kwargs (dict) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm: BaseLanguageModel [Required]\\uf0c1\\nLanguage model to call.\\nattribute llm_kwargs: dict [Optional]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute output_parser: BaseLLMOutputParser [Optional]\\uf0c1\\nOutput parser to use.\\nDefaults to one that takes the most likely string but does not change it\\notherwise.\\nattribute prompt: BasePromptTemplate [Required]\\uf0c1\\nPrompt object to use.\\nattribute return_final_only: bool = True\\uf0c1\\nWhether to return only the final parsed result. Defaults to True.\\nIf false, will return a bunch of extra information about the generation.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync aapply(input_list, callbacks=None)[source]\\uf0c1\\nUtilize the LLM generate method for speed gains.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync aapply_and_parse(input_list, callbacks=None)[source]\\uf0c1\\nCall apply and then parse the results.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nSequence[Union[str, List[str], Dict[str, str]]]\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync agenerate(input_list, run_manager=None)[source]\\uf0c1\\nGenerate LLM result from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \\nReturn type\\nlangchain.schema.LLMResult\\napply(input_list, callbacks=None)[source]\\uf0c1\\nUtilize the LLM generate method for speed gains.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\napply_and_parse(input_list, callbacks=None)[source]\\uf0c1\\nCall apply and then parse the results.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nSequence[Union[str, List[str], Dict[str, str]]]\\nasync apredict(callbacks=None, **kwargs)[source]\\uf0c1\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\\n**kwargs â€“ Keys to pass to prompt template.\\nkwargs (Any) â€“ \\nReturns\\nCompletion from LLM.\\nReturn type\\nstr\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\nasync apredict_and_parse(callbacks=None, **kwargs)[source]\\uf0c1\\nCall apredict and then parse the results.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nUnion[str, List[str], Dict[str, str]]\\nasync aprep_prompts(input_list, run_manager=None)[source]\\uf0c1\\nPrepare prompts from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \\nReturn type\\nTuple[List[langchain.schema.PromptValue], Optional[List[str]]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncreate_outputs(llm_result)[source]\\uf0c1\\nCreate outputs from response.\\nParameters\\nllm_result (langchain.schema.LLMResult) â€“ \\nReturn type\\nList[Dict[str, Any]]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_string(llm, template)[source]\\uf0c1\\nCreate LLMChain from LLM and template.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ntemplate (str) â€“ \\nReturn type\\nlangchain.chains.llm.LLMChain\\ngenerate(input_list, run_manager=None)[source]\\uf0c1\\nGenerate LLM result from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \\nReturn type\\nlangchain.schema.LLMResult\\npredict(callbacks=None, **kwargs)[source]\\uf0c1\\nFormat prompt with kwargs and pass to LLM.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\\n**kwargs â€“ Keys to pass to prompt template.\\nkwargs (Any) â€“ \\nReturns\\nCompletion from LLM.\\nReturn type\\nstr\\nExample\\ncompletion = llm.predict(adjective=\"funny\")\\npredict_and_parse(callbacks=None, **kwargs)[source]\\uf0c1\\nCall predict and then parse the results.\\nParameters\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nUnion[str, List[str], Dict[str, Any]]\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprep_prompts(input_list, run_manager=None)[source]\\uf0c1\\nPrepare prompts from inputs.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\nrun_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \\nReturn type\\nTuple[List[langchain.schema.PromptValue], Optional[List[str]]]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMCheckerChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, question_to_checked_assertions_chain, llm=None, create_draft_answer_prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'{question}\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), list_assertions_prompt=PromptTemplate(input_variables=[\\'statement\\'], output_parser=None, partial_variables={}, template=\\'Here is a statement:\\\\n{statement}\\\\nMake a bullet point list of the assumptions you made when producing the above statement.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'Here is a bullet point list of assertions:\\\\n{assertions}\\\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), revised_answer_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\\\n\\\\nQuestion: In light of the above assertions and checks, how would you answer the question \\'{question}\\'?\\\\n\\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True), input_key=\\'query\\', output_key=\\'result\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering with self-verification.\\nExample\\nfrom langchain import OpenAI, LLMCheckerChain\\nllm = OpenAI(temperature=0.7)\\nchecker_chain = LLMCheckerChain.from_llm(llm)\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nquestion_to_checked_assertions_chain (langchain.chains.sequential.SequentialChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ncreate_draft_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nlist_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ncheck_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nrevised_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute check_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'Here is a bullet point list of assertions:\\\\n{assertions}\\\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute create_draft_answer_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'{question}\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute list_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'statement\\'], output_parser=None, partial_variables={}, template=\\'Here is a statement:\\\\n{statement}\\\\nMake a bullet point list of the assumptions you made when producing the above statement.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute question_to_checked_assertions_chain: SequentialChain [Required]\\uf0c1\\nattribute revised_answer_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\\\n\\\\nQuestion: In light of the above assertions and checks, how would you answer the question \\'{question}\\'?\\\\n\\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated] Prompt to use when questioning the documents.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, create_draft_answer_prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'{question}\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), list_assertions_prompt=PromptTemplate(input_variables=[\\'statement\\'], output_parser=None, partial_variables={}, template=\\'Here is a statement:\\\\n{statement}\\\\nMake a bullet point list of the assumptions you made when producing the above statement.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'Here is a bullet point list of assertions:\\\\n{assertions}\\\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), revised_answer_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\\\n\\\\nQuestion: In light of the above assertions and checks, how would you answer the question \\'{question}\\'?\\\\n\\\\nAnswer:\", template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ncreate_draft_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nlist_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ncheck_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nrevised_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.llm_checker.base.LLMCheckerChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMMathChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: 37593^(1/5)\\\\n```text\\\\n37593**(1/5)\\\\n```\\\\n...numexpr.evaluate(\"37593**(1/5)\")...\\\\n```output\\\\n8.222831614237718\\\\n```\\\\nAnswer: 8.222831614237718\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True), input_key=\\'question\\', output_key=\\'answer\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain that interprets a prompt and executes python code to do math.\\nExample\\nfrom langchain import LLMMathChain, OpenAI\\nllm_math = LLMMathChain.from_llm(OpenAI())\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute prompt: BasePromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: 37593^(1/5)\\\\n```text\\\\n37593**(1/5)\\\\n```\\\\n...numexpr.evaluate(\"37593**(1/5)\")...\\\\n```output\\\\n8.222831614237718\\\\n```\\\\nAnswer: 8.222831614237718\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated] Prompt to use to translate to python if necessary.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: 37593^(1/5)\\\\n```text\\\\n37593**(1/5)\\\\n```\\\\n...numexpr.evaluate(\"37593**(1/5)\")...\\\\n```output\\\\n8.222831614237718\\\\n```\\\\nAnswer: 8.222831614237718\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.llm_math.base.LLMMathChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMRequestsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, requests_wrapper=None, text_length=8000, requests_key=\\'requests_result\\', input_key=\\'url\\', output_key=\\'output\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain that hits a URL and then uses an LLM to parse results.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nrequests_wrapper (langchain.requests.TextRequestsWrapper) â€“ \\ntext_length (int) â€“ \\nrequests_key (str) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute requests_wrapper: TextRequestsWrapper [Optional]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute text_length: int = 8000\\uf0c1\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.LLMRouterChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain)[source]\\uf0c1\\nBases: langchain.chains.router.base.RouterChain\\nA router chain that uses an LLM chain to perform routing.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nLLM chain used to perform routing\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync aroute(inputs, callbacks=None)\\uf0c1\\nParameters\\ninputs (Dict[str, Any]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nlangchain.chains.router.base.Route\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, prompt, **kwargs)[source]\\uf0c1\\nConvenience constructor.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.router.llm_router.LLMRouterChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nroute(inputs, callbacks=None)\\uf0c1\\nParameters\\ninputs (Dict[str, Any]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nlangchain.chains.router.base.Route\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nOutput keys this chain expects.\\nclass langchain.chains.LLMSummarizationCheckerChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, sequential_chain, llm=None, create_assertions_prompt=PromptTemplate(input_variables=[\\'summary\\'], output_parser=None, partial_variables={}, template=\\'Given some text, extract a list of facts from the text.\\\\n\\\\nFormat your output as a bulleted list.\\\\n\\\\nText:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nFacts:\\', template_format=\\'f-string\\', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\\\n\\\\nHere is a bullet point list of facts:\\\\n\"\"\"\\\\n{assertions}\\\\n\"\"\"\\\\n\\\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\\\nIf the fact is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), revised_summary_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\', \\'summary\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\\\n\\\\nChecked Assertions:\\\\n\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\n\\\\nOriginal Summary:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nUsing these checked assertions, rewrite the original summary to be completely true.\\\\n\\\\nThe output should have the same structure and formatting as the original summary.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True), are_all_true_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false.\\\\n\\\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\\\n\\\\nHere are some examples:\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is red: False\\\\n- Water is made of lava: False\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue: True\\\\n- Water is wet: True\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: True\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue - True\\\\n- Water is made of lava- False\\\\n- The sun is a star - True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions:\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\nResult:\\', template_format=\\'f-string\\', validate_template=True), input_key=\\'query\\', output_key=\\'result\\', max_checks=2)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering with self-verification.\\nExample\\nfrom langchain import OpenAI, LLMSummarizationCheckerChain\\nllm = OpenAI(temperature=0.0)\\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm)\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nsequential_chain (langchain.chains.sequential.SequentialChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ncreate_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ncheck_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nrevised_summary_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nare_all_true_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nmax_checks (int) â€“ \\nReturn type\\nNone\\nattribute are_all_true_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false.\\\\n\\\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\\\n\\\\nHere are some examples:\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is red: False\\\\n- Water is made of lava: False\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue: True\\\\n- Water is wet: True\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: True\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue - True\\\\n- Water is made of lava- False\\\\n- The sun is a star - True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions:\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\nResult:\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute check_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\\\n\\\\nHere is a bullet point list of facts:\\\\n\"\"\"\\\\n{assertions}\\\\n\"\"\"\\\\n\\\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\\\nIf the fact is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute create_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'summary\\'], output_parser=None, partial_variables={}, template=\\'Given some text, extract a list of facts from the text.\\\\n\\\\nFormat your output as a bulleted list.\\\\n\\\\nText:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nFacts:\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute max_checks: int = 2\\uf0c1\\nMaximum number of times to check the assertions. Default to double-checking.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute revised_summary_prompt: PromptTemplate = PromptTemplate(input_variables=[\\'checked_assertions\\', \\'summary\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\\\n\\\\nChecked Assertions:\\\\n\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\n\\\\nOriginal Summary:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nUsing these checked assertions, rewrite the original summary to be completely true.\\\\n\\\\nThe output should have the same structure and formatting as the original summary.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute sequential_chain: SequentialChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, create_assertions_prompt=PromptTemplate(input_variables=[\\'summary\\'], output_parser=None, partial_variables={}, template=\\'Given some text, extract a list of facts from the text.\\\\n\\\\nFormat your output as a bulleted list.\\\\n\\\\nText:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nFacts:\\', template_format=\\'f-string\\', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=[\\'assertions\\'], output_parser=None, partial_variables={}, template=\\'You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\\\n\\\\nHere is a bullet point list of facts:\\\\n\"\"\"\\\\n{assertions}\\\\n\"\"\"\\\\n\\\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\\\nIf the fact is false, explain why.\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), revised_summary_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\', \\'summary\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\\\n\\\\nChecked Assertions:\\\\n\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\n\\\\nOriginal Summary:\\\\n\"\"\"\\\\n{summary}\\\\n\"\"\"\\\\n\\\\nUsing these checked assertions, rewrite the original summary to be completely true.\\\\n\\\\nThe output should have the same structure and formatting as the original summary.\\\\n\\\\nSummary:\\', template_format=\\'f-string\\', validate_template=True), are_all_true_prompt=PromptTemplate(input_variables=[\\'checked_assertions\\'], output_parser=None, partial_variables={}, template=\\'Below are some assertions that have been fact checked and are labeled as true or false.\\\\n\\\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\\\n\\\\nHere are some examples:\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is red: False\\\\n- Water is made of lava: False\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue: True\\\\n- Water is wet: True\\\\n- The sun is a star: True\\\\n\"\"\"\\\\nResult: True\\\\n\\\\n===\\\\n\\\\nChecked Assertions: \"\"\"\\\\n- The sky is blue - True\\\\n- Water is made of lava- False\\\\n- The sun is a star - True\\\\n\"\"\"\\\\nResult: False\\\\n\\\\n===\\\\n\\\\nChecked Assertions:\"\"\"\\\\n{checked_assertions}\\\\n\"\"\"\\\\nResult:\\', template_format=\\'f-string\\', validate_template=True), verbose=False, **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ncreate_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\ncheck_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nrevised_summary_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nare_all_true_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \\nverbose (bool) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MapReduceChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, text_splitter, input_key=\\'input_text\\', output_key=\\'output_text\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nMap-reduce chain.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\ntext_splitter (langchain.text_splitter.TextSplitter) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine documents.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute text_splitter: TextSplitter [Required]\\uf0c1\\nText splitter to use.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_params(llm, prompt, text_splitter, callbacks=None, combine_chain_kwargs=None, reduce_chain_kwargs=None, **kwargs)[source]\\uf0c1\\nConstruct a map-reduce chain that uses the chain for map and reduce.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ntext_splitter (langchain.text_splitter.TextSplitter) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncombine_chain_kwargs (Optional[Mapping[str, Any]]) â€“ \\nreduce_chain_kwargs (Optional[Mapping[str, Any]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.mapreduce.MapReduceChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MultiPromptChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]\\uf0c1\\nBases: langchain.chains.router.base.MultiRouteChain\\nA multi-route chain that uses an LLM router chain to choose amongst prompts.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nrouter_chain (langchain.chains.router.base.RouterChain) â€“ \\ndestination_chains (Mapping[str, langchain.chains.llm.LLMChain]) â€“ \\ndefault_chain (langchain.chains.llm.LLMChain) â€“ \\nsilent_errors (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute default_chain: LLMChain [Required]\\uf0c1\\nDefault chain to use when router doesnâ€™t map input to one of the destinations.\\nattribute destination_chains: Mapping[str, LLMChain] [Required]\\uf0c1\\nMap of name to candidate chains that inputs can be routed to.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute router_chain: RouterChain [Required]\\uf0c1\\nChain for deciding a destination chain and the input to it.\\nattribute silent_errors: bool = False\\uf0c1\\nIf True, use default_chain when an invalid destination name is provided.\\nDefaults to False.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_prompts(llm, prompt_infos, default_chain=None, **kwargs)[source]\\uf0c1\\nConvenience constructor for instantiating from destination prompts.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt_infos (List[Dict[str, str]]) â€“ \\ndefault_chain (Optional[langchain.chains.llm.LLMChain]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.router.multi_prompt.MultiPromptChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MultiRetrievalQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]\\uf0c1\\nBases: langchain.chains.router.base.MultiRouteChain\\nA multi-route chain that uses an LLM router chain to choose amongst retrieval\\nqa chains.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nrouter_chain (langchain.chains.router.llm_router.LLMRouterChain) â€“ \\ndestination_chains (Mapping[str, langchain.chains.retrieval_qa.base.BaseRetrievalQA]) â€“ \\ndefault_chain (langchain.chains.base.Chain) â€“ \\nsilent_errors (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute default_chain: Chain [Required]\\uf0c1\\nDefault chain to use when router doesnâ€™t map input to one of the destinations.\\nattribute destination_chains: Mapping[str, BaseRetrievalQA] [Required]\\uf0c1\\nMap of name to candidate chains that inputs can be routed to.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute router_chain: LLMRouterChain [Required]\\uf0c1\\nChain for deciding a destination chain and the input to it.\\nattribute silent_errors: bool = False\\uf0c1\\nIf True, use default_chain when an invalid destination name is provided.\\nDefaults to False.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_retrievers(llm, retriever_infos, default_retriever=None, default_prompt=None, default_chain=None, **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nretriever_infos (List[Dict[str, Any]]) â€“ \\ndefault_retriever (Optional[langchain.schema.BaseRetriever]) â€“ \\ndefault_prompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \\ndefault_chain (Optional[langchain.chains.base.Chain]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MultiRouteChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nUse a single chain to route an input to one of multiple candidate chains.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nrouter_chain (langchain.chains.router.base.RouterChain) â€“ \\ndestination_chains (Mapping[str, langchain.chains.base.Chain]) â€“ \\ndefault_chain (langchain.chains.base.Chain) â€“ \\nsilent_errors (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute default_chain: Chain [Required]\\uf0c1\\nDefault chain to use when none of the destination chains are suitable.\\nattribute destination_chains: Mapping[str, Chain] [Required]\\uf0c1\\nChains that return final answer to inputs.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute router_chain: RouterChain [Required]\\uf0c1\\nChain that routes inputs to destination chains.\\nattribute silent_errors: bool = False\\uf0c1\\nIf True, use default_chain when an invalid destination name is provided.\\nDefaults to False.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.NatBotChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, objective, llm=None, input_url_key=\\'url\\', input_browser_content_key=\\'browser_content\\', previous_command=\\'\\', output_key=\\'command\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nImplement an LLM driven browser.\\nExample\\nfrom langchain import NatBotChain\\nnatbot = NatBotChain.from_default(\"Buy me a new hat.\")\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nobjective (str) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ninput_url_key (str) â€“ \\ninput_browser_content_key (str) â€“ \\nprevious_command (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute objective: str [Required]\\uf0c1\\nObjective that NatBot is tasked with completing.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nexecute(url, browser_content)[source]\\uf0c1\\nFigure out next browser command to run.\\nParameters\\nurl (str) â€“ URL of the site currently on.\\nbrowser_content (str) â€“ Content of the page as currently displayed by the browser.\\nReturns\\nNext browser command to run.\\nReturn type\\nstr\\nExample\\nbrowser_content = \"....\"\\nllm_command = natbot.run(\"www.google.com\", browser_content)\\nclassmethod from_default(objective, **kwargs)[source]\\uf0c1\\nLoad with default LLMChain.\\nParameters\\nobjective (str) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.natbot.base.NatBotChain\\nclassmethod from_llm(llm, objective, **kwargs)[source]\\uf0c1\\nLoad from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nobjective (str) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.natbot.base.NatBotChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.NebulaGraphQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, ngql_generation_chain, qa_chain, input_key=\\'query\\', output_key=\\'result\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for question-answering against a graph by generating nGQL statements.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ngraph (langchain.graphs.nebula_graph.NebulaGraph) â€“ \\nngql_generation_chain (langchain.chains.llm.LLMChain) â€“ \\nqa_chain (langchain.chains.llm.LLMChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute graph: NebulaGraph [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute ngql_generation_chain: LLMChain [Required]\\uf0c1\\nattribute qa_chain: LLMChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\\\nThe information part contains the provided information that you must use to construct an answer.\\\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\\\nIf the provided information is empty, say that you don\\'t know the answer.\\\\nInformation:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", template_format=\\'f-string\\', validate_template=True), ngql_prompt=PromptTemplate(input_variables=[\\'schema\\', \\'question\\'], output_parser=None, partial_variables={}, template=\"Task:Generate NebulaGraph Cypher statement to query a graph database.\\\\n\\\\nInstructions:\\\\n\\\\nFirst, generate cypher then convert it to NebulaGraph Cypher dialect(rather than standard):\\\\n1. it requires explicit label specification when referring to node properties: v.`Foo`.name\\\\n2. it uses double equals sign for comparison: `==` rather than `=`\\\\nFor instance:\\\\n```diff\\\\n< MATCH (p:person)-[:directed]->(m:movie) WHERE m.name = \\'The Godfather II\\'\\\\n< RETURN p.name;\\\\n---\\\\n> MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == \\'The Godfather II\\'\\\\n> RETURN p.`person`.`name`;\\\\n```\\\\n\\\\nUse only the provided relationship types and properties in the schema.\\\\nDo not use any other relationship types or properties that are not provided.\\\\nSchema:\\\\n{schema}\\\\nNote: Do not include any explanations or apologies in your responses.\\\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\\\nDo not include any text except the generated Cypher statement.\\\\n\\\\nThe question is:\\\\n{question}\", template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nqa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nngql_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.OpenAIModerationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, client=None, model_name=None, error=False, input_key=\\'input\\', output_key=\\'output\\', openai_api_key=None, openai_organization=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nPass input through a moderation endpoint.\\nTo use, you should have the openai python package installed, and the\\nenvironment variable OPENAI_API_KEY set with your API key.\\nAny parameters that are valid to be passed to the openai.create call can be passed\\nin, even if not explicitly saved on this class.\\nExample\\nfrom langchain.chains import OpenAIModerationChain\\nmoderation = OpenAIModerationChain()\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nclient (Any) â€“ \\nmodel_name (Optional[str]) â€“ \\nerror (bool) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nopenai_api_key (Optional[str]) â€“ \\nopenai_organization (Optional[str]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute error: bool = False\\uf0c1\\nWhether or not to error if bad content was found.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute model_name: Optional[str] = None\\uf0c1\\nModeration model name to use.\\nattribute openai_api_key: Optional[str] = None\\uf0c1\\nattribute openai_organization: Optional[str] = None\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.OpenAPIEndpointChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, api_request_chain, api_response_chain=None, api_operation, requests=None, param_mapping, return_intermediate_steps=False, instructions_key=\\'instructions\\', output_key=\\'output\\', max_text_length=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain, pydantic.main.BaseModel\\nChain interacts with an OpenAPI endpoint using natural language.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\napi_request_chain (langchain.chains.llm.LLMChain) â€“ \\napi_response_chain (Optional[langchain.chains.llm.LLMChain]) â€“ \\napi_operation (langchain.tools.openapi.utils.api_models.APIOperation) â€“ \\nrequests (langchain.requests.Requests) â€“ \\nparam_mapping (langchain.chains.api.openapi.chain._ParamMapping) â€“ \\nreturn_intermediate_steps (bool) â€“ \\ninstructions_key (str) â€“ \\noutput_key (str) â€“ \\nmax_text_length (Optional[int]) â€“ \\nReturn type\\nNone\\nattribute api_operation: APIOperation [Required]\\uf0c1\\nattribute api_request_chain: LLMChain [Required]\\uf0c1\\nattribute api_response_chain: Optional[LLMChain] = None\\uf0c1\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute param_mapping: _ParamMapping [Required]\\uf0c1\\nattribute requests: Requests [Optional]\\uf0c1\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndeserialize_json_input(serialized_args)[source]\\uf0c1\\nUse the serialized typescript dictionary.\\nResolve the path, query params dict, and optional requestBody dict.\\nParameters\\nserialized_args (str) â€“ \\nReturn type\\ndict\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_api_operation(operation, llm, requests=None, verbose=False, return_intermediate_steps=False, raw_response=False, callbacks=None, **kwargs)[source]\\uf0c1\\nCreate an OpenAPIEndpointChain from an operation and a spec.\\nParameters\\noperation (langchain.tools.openapi.utils.api_models.APIOperation) â€“ \\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nrequests (Optional[langchain.requests.Requests]) â€“ \\nverbose (bool) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nraw_response (bool) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.api.openapi.chain.OpenAPIEndpointChain\\nclassmethod from_url_and_method(spec_url, path, method, llm, requests=None, return_intermediate_steps=False, **kwargs)[source]\\uf0c1\\nCreate an OpenAPIEndpoint from a spec at the specified url.\\nParameters\\nspec_url (str) â€“ \\npath (str) â€“ \\nmethod (str) â€“ \\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nrequests (Optional[langchain.requests.Requests]) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.api.openapi.chain.OpenAPIEndpointChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.PALChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\\\\n\\xa0\\xa0\\xa0 money_initial = 23\\\\n\\xa0\\xa0\\xa0 bagels = 5\\\\n\\xa0\\xa0\\xa0 bagel_cost = 3\\\\n\\xa0\\xa0\\xa0 money_spent = bagels * bagel_cost\\\\n\\xa0\\xa0\\xa0 money_left = money_initial - money_spent\\\\n\\xa0\\xa0\\xa0 result = money_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\"\\\\n\\xa0\\xa0\\xa0 golf_balls_initial = 58\\\\n\\xa0\\xa0\\xa0 golf_balls_lost_tuesday = 23\\\\n\\xa0\\xa0\\xa0 golf_balls_lost_wednesday = 2\\\\n\\xa0\\xa0\\xa0 golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\\\n\\xa0\\xa0\\xa0 result = golf_balls_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\\\\n\\xa0\\xa0\\xa0 computers_initial = 9\\\\n\\xa0\\xa0\\xa0 computers_per_day = 5\\\\n\\xa0\\xa0\\xa0 num_days = 4\\xa0 # 4 days between monday and thursday\\\\n\\xa0\\xa0\\xa0 computers_added = computers_per_day * num_days\\\\n\\xa0\\xa0\\xa0 computers_total = computers_initial + computers_added\\\\n\\xa0\\xa0\\xa0 result = computers_total\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\\\\n\\xa0\\xa0\\xa0 toys_initial = 5\\\\n\\xa0\\xa0\\xa0 mom_toys = 2\\\\n\\xa0\\xa0\\xa0 dad_toys = 2\\\\n\\xa0\\xa0\\xa0 total_received = mom_toys + dad_toys\\\\n\\xa0\\xa0\\xa0 total_toys = toys_initial + total_received\\\\n\\xa0\\xa0\\xa0 result = total_toys\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\\\\n\\xa0\\xa0\\xa0 jason_lollipops_initial = 20\\\\n\\xa0\\xa0\\xa0 jason_lollipops_after = 12\\\\n\\xa0\\xa0\\xa0 denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\\\n\\xa0\\xa0\\xa0 result = denny_lollipops\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\\\\n\\xa0\\xa0\\xa0 leah_chocolates = 32\\\\n\\xa0\\xa0\\xa0 sister_chocolates = 42\\\\n\\xa0\\xa0\\xa0 total_chocolates = leah_chocolates + sister_chocolates\\\\n\\xa0\\xa0\\xa0 chocolates_eaten = 35\\\\n\\xa0\\xa0\\xa0 chocolates_left = total_chocolates - chocolates_eaten\\\\n\\xa0\\xa0\\xa0 result = chocolates_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\\\\n\\xa0\\xa0\\xa0 cars_initial = 3\\\\n\\xa0\\xa0\\xa0 cars_arrived = 2\\\\n\\xa0\\xa0\\xa0 total_cars = cars_initial + cars_arrived\\\\n\\xa0\\xa0\\xa0 result = total_cars\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\\\\n\\xa0\\xa0\\xa0 trees_initial = 15\\\\n\\xa0\\xa0\\xa0 trees_after = 21\\\\n\\xa0\\xa0\\xa0 trees_added = trees_after - trees_initial\\\\n\\xa0\\xa0\\xa0 result = trees_added\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: {question}\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True), stop=\\'\\\\n\\\\n\\', get_answer_expr=\\'print(solution())\\', python_globals=None, python_locals=None, output_key=\\'result\\', return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nImplements Program-Aided Language Models.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\nprompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nstop (str) â€“ \\nget_answer_expr (str) â€“ \\npython_globals (Optional[Dict[str, Any]]) â€“ \\npython_locals (Optional[Dict[str, Any]]) â€“ \\noutput_key (str) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute get_answer_expr: str = \\'print(solution())\\'\\uf0c1\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated]\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute prompt: BasePromptTemplate = PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\\\\n\\xa0\\xa0\\xa0 money_initial = 23\\\\n\\xa0\\xa0\\xa0 bagels = 5\\\\n\\xa0\\xa0\\xa0 bagel_cost = 3\\\\n\\xa0\\xa0\\xa0 money_spent = bagels * bagel_cost\\\\n\\xa0\\xa0\\xa0 money_left = money_initial - money_spent\\\\n\\xa0\\xa0\\xa0 result = money_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\"\\\\n\\xa0\\xa0\\xa0 golf_balls_initial = 58\\\\n\\xa0\\xa0\\xa0 golf_balls_lost_tuesday = 23\\\\n\\xa0\\xa0\\xa0 golf_balls_lost_wednesday = 2\\\\n\\xa0\\xa0\\xa0 golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\\\n\\xa0\\xa0\\xa0 result = golf_balls_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\\\\n\\xa0\\xa0\\xa0 computers_initial = 9\\\\n\\xa0\\xa0\\xa0 computers_per_day = 5\\\\n\\xa0\\xa0\\xa0 num_days = 4\\xa0 # 4 days between monday and thursday\\\\n\\xa0\\xa0\\xa0 computers_added = computers_per_day * num_days\\\\n\\xa0\\xa0\\xa0 computers_total = computers_initial + computers_added\\\\n\\xa0\\xa0\\xa0 result = computers_total\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\\\\n\\xa0\\xa0\\xa0 toys_initial = 5\\\\n\\xa0\\xa0\\xa0 mom_toys = 2\\\\n\\xa0\\xa0\\xa0 dad_toys = 2\\\\n\\xa0\\xa0\\xa0 total_received = mom_toys + dad_toys\\\\n\\xa0\\xa0\\xa0 total_toys = toys_initial + total_received\\\\n\\xa0\\xa0\\xa0 result = total_toys\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\\\\n\\xa0\\xa0\\xa0 jason_lollipops_initial = 20\\\\n\\xa0\\xa0\\xa0 jason_lollipops_after = 12\\\\n\\xa0\\xa0\\xa0 denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\\\n\\xa0\\xa0\\xa0 result = denny_lollipops\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\\\\n\\xa0\\xa0\\xa0 leah_chocolates = 32\\\\n\\xa0\\xa0\\xa0 sister_chocolates = 42\\\\n\\xa0\\xa0\\xa0 total_chocolates = leah_chocolates + sister_chocolates\\\\n\\xa0\\xa0\\xa0 chocolates_eaten = 35\\\\n\\xa0\\xa0\\xa0 chocolates_left = total_chocolates - chocolates_eaten\\\\n\\xa0\\xa0\\xa0 result = chocolates_left\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\\\\n\\xa0\\xa0\\xa0 cars_initial = 3\\\\n\\xa0\\xa0\\xa0 cars_arrived = 2\\\\n\\xa0\\xa0\\xa0 total_cars = cars_initial + cars_arrived\\\\n\\xa0\\xa0\\xa0 result = total_cars\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\ndef solution():\\\\n\\xa0\\xa0\\xa0 \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\\\\n\\xa0\\xa0\\xa0 trees_initial = 15\\\\n\\xa0\\xa0\\xa0 trees_after = 21\\\\n\\xa0\\xa0\\xa0 trees_added = trees_after - trees_initial\\\\n\\xa0\\xa0\\xa0 result = trees_added\\\\n\\xa0\\xa0\\xa0 return result\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nQ: {question}\\\\n\\\\n# solution in Python:\\\\n\\\\n\\\\n\\', template_format=\\'f-string\\', validate_template=True)\\uf0c1\\n[Deprecated]\\nattribute python_globals: Optional[Dict[str, Any]] = None\\uf0c1\\nattribute python_locals: Optional[Dict[str, Any]] = None\\uf0c1\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nattribute stop: str = \\'\\\\n\\\\n\\'\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_colored_object_prompt(llm, **kwargs)[source]\\uf0c1\\nLoad PAL from colored object prompt.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.pal.base.PALChain\\nclassmethod from_math_prompt(llm, **kwargs)[source]\\uf0c1\\nLoad PAL from math prompt.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.pal.base.PALChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.QAGenerationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, text_splitter=<langchain.text_splitter.RecursiveCharacterTextSplitter object>, input_key=\\'text\\', output_key=\\'questions\\', k=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\ntext_splitter (langchain.text_splitter.TextSplitter) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nk (Optional[int]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute input_key: str = \\'text\\'\\uf0c1\\nattribute k: Optional[int] = None\\uf0c1\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute output_key: str = \\'questions\\'\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute text_splitter: TextSplitter = <langchain.text_splitter.RecursiveCharacterTextSplitter object>\\uf0c1\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, prompt=None, **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_generation.base.QAGenerationChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty input_keys: List[str]\\uf0c1\\nInput keys this chain expects.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nOutput keys this chain expects.\\nclass langchain.chains.QAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key=\\'question\\', input_docs_key=\\'docs\\', answer_key=\\'answer\\', sources_answer_key=\\'sources\\', return_source_documents=False)[source]\\uf0c1\\nBases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nQuestion answering with sources over documents.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nquestion_key (str) â€“ \\ninput_docs_key (str) â€“ \\nanswer_key (str) â€“ \\nsources_answer_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine documents.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_source_documents: bool = False\\uf0c1\\nReturn the source documents.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_chain_type(llm, chain_type=\\'stuff\\', chain_type_kwargs=None, **kwargs)\\uf0c1\\nLoad chain from chain type.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain_type (str) â€“ \\nchain_type_kwargs (Optional[dict]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nclassmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=[\\'page_content\\', \\'source\\'], output_parser=None, partial_variables={}, template=\\'Content: {page_content}\\\\nSource: {source}\\', template_format=\\'f-string\\', validate_template=True), question_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\\\nReturn any relevant text verbatim.\\\\n{context}\\\\nQuestion: {question}\\\\nRelevant text, if any:\\', template_format=\\'f-string\\', validate_template=True), combine_prompt=PromptTemplate(input_variables=[\\'summaries\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\\\nIf you don\\\\\\'t know the answer, just say that you don\\\\\\'t know. Don\\\\\\'t try to make up an answer.\\\\nALWAYS return a \"SOURCES\" part in your answer.\\\\n\\\\nQUESTION: Which state/country\\\\\\'s law governs the interpretation of the contract?\\\\n=========\\\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in\\xa0 relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an\\xa0 injunction or other relief to protect its Intellectual Property Rights.\\\\nSource: 28-pl\\\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)\\xa0 right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation\\xa0 in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any\\xa0 kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\\\nSource: 30-pl\\\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as\\xa0 defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\\\nSource: 4-pl\\\\n=========\\\\nFINAL ANSWER: This Agreement is governed by English law.\\\\nSOURCES: 28-pl\\\\n\\\\nQUESTION: What did the president say about Michael Jackson?\\\\n=========\\\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\xa0 \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\\\nSource: 0-pl\\\\nContent: And we wonâ€™t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\xa0 \\\\n\\\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\xa0 \\\\n\\\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\\\nSource: 24-pl\\\\nContent: And a proud Ukrainian people, who have known 30 years\\xa0 of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\xa0 \\\\n\\\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctions\\xa0 is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\\xa0 \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\xa0 \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.\\\\nSource: 5-pl\\\\nContent: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.\\xa0 \\\\n\\\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\\\nSource: 34-pl\\\\n=========\\\\nFINAL ANSWER: The president did not mention Michael Jackson.\\\\nSOURCES:\\\\n\\\\nQUESTION: {question}\\\\n=========\\\\n{summaries}\\\\n=========\\\\nFINAL ANSWER:\\', template_format=\\'f-string\\', validate_template=True), **kwargs)\\uf0c1\\nConstruct the chain from an LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ndocument_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nquestion_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ncombine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.RetrievalQA(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, input_key=\\'query\\', output_key=\\'result\\', return_source_documents=False, retriever)[source]\\uf0c1\\nBases: langchain.chains.retrieval_qa.base.BaseRetrievalQA\\nChain for question-answering against an index.\\nExample\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.faiss import FAISS\\nfrom langchain.vectorstores.base import VectorStoreRetriever\\nretriever = VectorStoreRetriever(vectorstore=FAISS(...))\\nretrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nretriever (langchain.schema.BaseRetriever) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine the documents.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute retriever: BaseRetriever [Required]\\uf0c1\\nattribute return_source_documents: bool = False\\uf0c1\\nReturn the source documents.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_chain_type(llm, chain_type=\\'stuff\\', chain_type_kwargs=None, **kwargs)\\uf0c1\\nLoad chain from chain type.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain_type (str) â€“ \\nchain_type_kwargs (Optional[dict]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.retrieval_qa.base.BaseRetrievalQA\\nclassmethod from_llm(llm, prompt=None, **kwargs)\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.retrieval_qa.base.BaseRetrievalQA\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.RetrievalQAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key=\\'question\\', input_docs_key=\\'docs\\', answer_key=\\'answer\\', sources_answer_key=\\'sources\\', return_source_documents=False, retriever, reduce_k_below_max_tokens=False, max_tokens_limit=3375)[source]\\uf0c1\\nBases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nQuestion-answering with sources over an index.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nquestion_key (str) â€“ \\ninput_docs_key (str) â€“ \\nanswer_key (str) â€“ \\nsources_answer_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nretriever (langchain.schema.BaseRetriever) â€“ \\nreduce_k_below_max_tokens (bool) â€“ \\nmax_tokens_limit (int) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine documents.\\nattribute max_tokens_limit: int = 3375\\uf0c1\\nRestrict the docs to return from store based on tokens,\\nenforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute reduce_k_below_max_tokens: bool = False\\uf0c1\\nReduce the number of results to return from store based on tokens limit\\nattribute retriever: langchain.schema.BaseRetriever [Required]\\uf0c1\\nIndex to connect to.\\nattribute return_source_documents: bool = False\\uf0c1\\nReturn the source documents.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_chain_type(llm, chain_type=\\'stuff\\', chain_type_kwargs=None, **kwargs)\\uf0c1\\nLoad chain from chain type.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain_type (str) â€“ \\nchain_type_kwargs (Optional[dict]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nclassmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=[\\'page_content\\', \\'source\\'], output_parser=None, partial_variables={}, template=\\'Content: {page_content}\\\\nSource: {source}\\', template_format=\\'f-string\\', validate_template=True), question_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\\\nReturn any relevant text verbatim.\\\\n{context}\\\\nQuestion: {question}\\\\nRelevant text, if any:\\', template_format=\\'f-string\\', validate_template=True), combine_prompt=PromptTemplate(input_variables=[\\'summaries\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\\\nIf you don\\\\\\'t know the answer, just say that you don\\\\\\'t know. Don\\\\\\'t try to make up an answer.\\\\nALWAYS return a \"SOURCES\" part in your answer.\\\\n\\\\nQUESTION: Which state/country\\\\\\'s law governs the interpretation of the contract?\\\\n=========\\\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in\\xa0 relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an\\xa0 injunction or other relief to protect its Intellectual Property Rights.\\\\nSource: 28-pl\\\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)\\xa0 right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation\\xa0 in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any\\xa0 kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\\\nSource: 30-pl\\\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as\\xa0 defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\\\nSource: 4-pl\\\\n=========\\\\nFINAL ANSWER: This Agreement is governed by English law.\\\\nSOURCES: 28-pl\\\\n\\\\nQUESTION: What did the president say about Michael Jackson?\\\\n=========\\\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\xa0 \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\\\nSource: 0-pl\\\\nContent: And we wonâ€™t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\xa0 \\\\n\\\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\xa0 \\\\n\\\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\\\nSource: 24-pl\\\\nContent: And a proud Ukrainian people, who have known 30 years\\xa0 of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\xa0 \\\\n\\\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctions\\xa0 is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\\xa0 \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\xa0 \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.\\\\nSource: 5-pl\\\\nContent: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.\\xa0 \\\\n\\\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\\\nSource: 34-pl\\\\n=========\\\\nFINAL ANSWER: The president did not mention Michael Jackson.\\\\nSOURCES:\\\\n\\\\nQUESTION: {question}\\\\n=========\\\\n{summaries}\\\\n=========\\\\nFINAL ANSWER:\\', template_format=\\'f-string\\', validate_template=True), **kwargs)\\uf0c1\\nConstruct the chain from an LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ndocument_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nquestion_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ncombine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.RouterChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain, abc.ABC\\nChain that outputs the name of a destination chain and the inputs to it.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync aroute(inputs, callbacks=None)[source]\\uf0c1\\nParameters\\ninputs (Dict[str, Any]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nlangchain.chains.router.base.Route\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nroute(inputs, callbacks=None)[source]\\uf0c1\\nParameters\\ninputs (Dict[str, Any]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nlangchain.chains.router.base.Route\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nabstract property input_keys: List[str]\\uf0c1\\nInput keys this chain expects.\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nproperty output_keys: List[str]\\uf0c1\\nOutput keys this chain expects.\\nclass langchain.chains.SQLDatabaseChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, database, prompt=None, top_k=5, input_key=\\'query\\', output_key=\\'result\\', return_intermediate_steps=False, return_direct=False, use_query_checker=False, query_checker_prompt=None)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for interacting with SQL Database.\\nExample\\nfrom langchain import SQLDatabaseChain, OpenAI, SQLDatabase\\ndb = SQLDatabase(...)\\ndb_chain = SQLDatabaseChain.from_llm(OpenAI(), db)\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\nllm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \\ndatabase (langchain.sql_database.SQLDatabase) â€“ \\nprompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \\ntop_k (int) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nreturn_direct (bool) â€“ \\nuse_query_checker (bool) â€“ \\nquery_checker_prompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute database: SQLDatabase [Required]\\uf0c1\\nSQL Database to connect to.\\nattribute llm: Optional[BaseLanguageModel] = None\\uf0c1\\n[Deprecated] LLM wrapper to use.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute prompt: Optional[BasePromptTemplate] = None\\uf0c1\\n[Deprecated] Prompt to use to translate natural language to SQL.\\nattribute query_checker_prompt: Optional[BasePromptTemplate] = None\\uf0c1\\nThe prompt template that should be used by the query checker\\nattribute return_direct: bool = False\\uf0c1\\nWhether or not to return the result of querying the SQL table directly.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nWhether or not to return the intermediate steps along with the final answer.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute top_k: int = 5\\uf0c1\\nNumber of results to return from the query\\nattribute use_query_checker: bool = False\\uf0c1\\nWhether or not the query checker tool should be used to attempt\\nto fix the initial SQL from the LLM.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, db, prompt=None, **kwargs)[source]\\uf0c1\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ndb (langchain.sql_database.SQLDatabase) â€“ \\nprompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.sql_database.base.SQLDatabaseChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.SQLDatabaseSequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, decider_chain, sql_chain, input_key=\\'query\\', output_key=\\'result\\', return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain for querying SQL database that is a sequential chain.\\nThe chain is as follows:\\n1. Based on the query, determine which tables to use.\\n2. Based on those tables, call the normal SQL database chain.\\nThis is useful in cases where the number of tables in the database is large.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ndecider_chain (langchain.chains.llm.LLMChain) â€“ \\nsql_chain (langchain.chains.sql_database.base.SQLDatabaseChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute decider_chain: LLMChain [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nattribute sql_chain: SQLDatabaseChain [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_llm(llm, database, query_prompt=PromptTemplate(input_variables=[\\'input\\', \\'table_info\\', \\'dialect\\', \\'top_k\\'], output_parser=None, partial_variables={}, template=\\'Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\\\n\\\\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\\\\n\\\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: Question here\\\\nSQLQuery: SQL Query to run\\\\nSQLResult: Result of the SQLQuery\\\\nAnswer: Final answer here\\\\n\\\\nOnly use the following tables:\\\\n{table_info}\\\\n\\\\nQuestion: {input}\\', template_format=\\'f-string\\', validate_template=True), decider_prompt=PromptTemplate(input_variables=[\\'query\\', \\'table_names\\'], output_parser=CommaSeparatedListOutputParser(), partial_variables={}, template=\\'Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\\\\n\\\\nQuestion: {query}\\\\n\\\\nTable Names: {table_names}\\\\n\\\\nRelevant Table Names:\\', template_format=\\'f-string\\', validate_template=True), **kwargs)[source]\\uf0c1\\nLoad the necessary chains.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ndatabase (langchain.sql_database.SQLDatabase) â€“ \\nquery_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ndecider_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.sql_database.base.SQLDatabaseSequentialChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.SequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chains, input_variables, output_variables, return_all=False)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain where the outputs of one chain feed directly into next.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nchains (List[langchain.chains.base.Chain]) â€“ \\ninput_variables (List[str]) â€“ \\noutput_variables (List[str]) â€“ \\nreturn_all (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute chains: List[langchain.chains.base.Chain] [Required]\\uf0c1\\nattribute input_variables: List[str] [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_all: bool = False\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.SimpleSequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chains, strip_outputs=False, input_key=\\'input\\', output_key=\\'output\\')[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nSimple chain where the outputs of one step feed directly into next.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\nchains (List[langchain.chains.base.Chain]) â€“ \\nstrip_outputs (bool) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute chains: List[langchain.chains.base.Chain] [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute strip_outputs: bool = False\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.TransformChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_variables, output_variables, transform)[source]\\uf0c1\\nBases: langchain.chains.base.Chain\\nChain transform chain output.\\nExample\\nfrom langchain import TransformChain\\ntransform_chain = TransformChain(input_variables=[\"text\"],\\n output_variables[\"entities\"], transform=func())\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_variables (List[str]) â€“ \\noutput_variables (List[str]) â€“ \\ntransform (Callable[[Dict[str, str]], Dict[str, str]]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute input_variables: List[str] [Required]\\uf0c1\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute output_variables: List[str] [Required]\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute transform: Callable[[Dict[str, str]], Dict[str, str]] [Required]\\uf0c1\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.VectorDBQA(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, input_key=\\'query\\', output_key=\\'result\\', return_source_documents=False, vectorstore, k=4, search_type=\\'similarity\\', search_kwargs=None)[source]\\uf0c1\\nBases: langchain.chains.retrieval_qa.base.BaseRetrievalQA\\nChain for question-answering against a vector database.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nvectorstore (langchain.vectorstores.base.VectorStore) â€“ \\nk (int) â€“ \\nsearch_type (str) â€“ \\nsearch_kwargs (Dict[str, Any]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine the documents.\\nattribute k: int = 4\\uf0c1\\nNumber of documents to query for.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_source_documents: bool = False\\uf0c1\\nReturn the source documents.\\nattribute search_kwargs: Dict[str, Any] [Optional]\\uf0c1\\nExtra search args.\\nattribute search_type: str = \\'similarity\\'\\uf0c1\\nSearch type to use over vectorstore. similarity or mmr.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute vectorstore: VectorStore [Required]\\uf0c1\\nVector Database to connect to.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_chain_type(llm, chain_type=\\'stuff\\', chain_type_kwargs=None, **kwargs)\\uf0c1\\nLoad chain from chain type.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain_type (str) â€“ \\nchain_type_kwargs (Optional[dict]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.retrieval_qa.base.BaseRetrievalQA\\nclassmethod from_llm(llm, prompt=None, **kwargs)\\uf0c1\\nInitialize from LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nprompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.retrieval_qa.base.BaseRetrievalQA\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.VectorDBQAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key=\\'question\\', input_docs_key=\\'docs\\', answer_key=\\'answer\\', sources_answer_key=\\'sources\\', return_source_documents=False, vectorstore, k=4, reduce_k_below_max_tokens=False, max_tokens_limit=3375, search_kwargs=None)[source]\\uf0c1\\nBases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nQuestion-answering with sources over a vector database.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ncombine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\nquestion_key (str) â€“ \\ninput_docs_key (str) â€“ \\nanswer_key (str) â€“ \\nsources_answer_key (str) â€“ \\nreturn_source_documents (bool) â€“ \\nvectorstore (langchain.vectorstores.base.VectorStore) â€“ \\nk (int) â€“ \\nreduce_k_below_max_tokens (bool) â€“ \\nmax_tokens_limit (int) â€“ \\nsearch_kwargs (Dict[str, Any]) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute combine_documents_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine documents.\\nattribute k: int = 4\\uf0c1\\nNumber of results to return from store\\nattribute max_tokens_limit: int = 3375\\uf0c1\\nRestrict the docs to return from store based on tokens,\\nenforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute reduce_k_below_max_tokens: bool = False\\uf0c1\\nReduce the number of results to return from store based on tokens limit\\nattribute return_source_documents: bool = False\\uf0c1\\nReturn the source documents.\\nattribute search_kwargs: Dict[str, Any] [Optional]\\uf0c1\\nExtra search args.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute vectorstore: langchain.vectorstores.base.VectorStore [Required]\\uf0c1\\nVector Database to connect to.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nclassmethod from_chain_type(llm, chain_type=\\'stuff\\', chain_type_kwargs=None, **kwargs)\\uf0c1\\nLoad chain from chain type.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\nchain_type (str) â€“ \\nchain_type_kwargs (Optional[dict]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nclassmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=[\\'page_content\\', \\'source\\'], output_parser=None, partial_variables={}, template=\\'Content: {page_content}\\\\nSource: {source}\\', template_format=\\'f-string\\', validate_template=True), question_prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\\\nReturn any relevant text verbatim.\\\\n{context}\\\\nQuestion: {question}\\\\nRelevant text, if any:\\', template_format=\\'f-string\\', validate_template=True), combine_prompt=PromptTemplate(input_variables=[\\'summaries\\', \\'question\\'], output_parser=None, partial_variables={}, template=\\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\\\nIf you don\\\\\\'t know the answer, just say that you don\\\\\\'t know. Don\\\\\\'t try to make up an answer.\\\\nALWAYS return a \"SOURCES\" part in your answer.\\\\n\\\\nQUESTION: Which state/country\\\\\\'s law governs the interpretation of the contract?\\\\n=========\\\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in\\xa0 relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an\\xa0 injunction or other relief to protect its Intellectual Property Rights.\\\\nSource: 28-pl\\\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)\\xa0 right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation\\xa0 in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any\\xa0 kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\\\nSource: 30-pl\\\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as\\xa0 defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\\\nSource: 4-pl\\\\n=========\\\\nFINAL ANSWER: This Agreement is governed by English law.\\\\nSOURCES: 28-pl\\\\n\\\\nQUESTION: What did the president say about Michael Jackson?\\\\n=========\\\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\xa0 \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\\\nSource: 0-pl\\\\nContent: And we wonâ€™t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\xa0 \\\\n\\\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\xa0 \\\\n\\\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\\\nSource: 24-pl\\\\nContent: And a proud Ukrainian people, who have known 30 years\\xa0 of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\xa0 \\\\n\\\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctions\\xa0 is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\\xa0 \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\xa0 \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.\\\\nSource: 5-pl\\\\nContent: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.\\xa0 \\\\n\\\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\\\nSource: 34-pl\\\\n=========\\\\nFINAL ANSWER: The president did not mention Michael Jackson.\\\\nSOURCES:\\\\n\\\\nQUESTION: {question}\\\\n=========\\\\n{summaries}\\\\n=========\\\\nFINAL ANSWER:\\', template_format=\\'f-string\\', validate_template=True), **kwargs)\\uf0c1\\nConstruct the chain from an LLM.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ \\ndocument_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nquestion_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ncombine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nlangchain.chains.create_extraction_chain(schema, llm)[source]\\uf0c1\\nCreates a chain that extracts information from a passage.\\nParameters\\nschema (dict) â€“ The schema of the entities to extract.\\nllm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\\nReturns\\nChain that can be used to extract information from a passage.\\nReturn type\\nlangchain.chains.base.Chain\\nlangchain.chains.create_extraction_chain_pydantic(pydantic_schema, llm)[source]\\uf0c1\\nCreates a chain that extracts information from a passage using pydantic schema.\\nParameters\\npydantic_schema (Any) â€“ The pydantic schema of the entities to extract.\\nllm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\\nReturns\\nChain that can be used to extract information from a passage.\\nReturn type\\nlangchain.chains.base.Chain\\nlangchain.chains.create_tagging_chain(schema, llm)[source]\\uf0c1\\nCreates a chain that extracts information from a passage.\\nParameters\\nschema (dict) â€“ The schema of the entities to extract.\\nllm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\\nReturns\\nChain (LLMChain) that can be used to extract information from a passage.\\nReturn type\\nlangchain.chains.base.Chain\\nlangchain.chains.create_tagging_chain_pydantic(pydantic_schema, llm)[source]\\uf0c1\\nCreates a chain that extracts information from a passage.\\nParameters\\npydantic_schema (Any) â€“ The pydantic schema of the entities to extract.\\nllm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\\nReturns\\nChain (LLMChain) that can be used to extract information from a passage.\\nReturn type\\nlangchain.chains.base.Chain\\nlangchain.chains.load_chain(path, **kwargs)[source]\\uf0c1\\nUnified method for loading a chain from LangChainHub or local fs.\\nParameters\\npath (Union[str, pathlib.Path]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nlangchain.chains.base.Chain\\nlangchain.chains.create_citation_fuzzy_match_chain(llm)[source]\\uf0c1\\nCreate a citation fuzzy match chain.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\\nReturns\\nChain (LLMChain) that can be used to answer questions with citations.\\nReturn type\\nlangchain.chains.llm.LLMChain\\nlangchain.chains.create_qa_with_structure_chain(llm, schema, output_parser=\\'base\\', prompt=None)[source]\\uf0c1\\nCreate a question answering chain that returns an answer with sources.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\\nschema (Union[dict, Type[pydantic.main.BaseModel]]) â€“ Pydantic schema to use for the output.\\noutput_parser (str) â€“ Output parser to use. Should be one of pydantic or base.\\nDefault to base.\\nprompt (Optional[Union[langchain.prompts.prompt.PromptTemplate, langchain.prompts.chat.ChatPromptTemplate]]) â€“ Optional prompt to use for the chain.\\nReturn type\\nlangchain.chains.llm.LLMChain\\nReturns:\\nlangchain.chains.create_qa_with_sources_chain(llm, **kwargs)[source]\\uf0c1\\nCreate a question answering chain that returns an answer with sources.\\nParameters\\nllm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\\n**kwargs â€“ Keyword arguments to pass to create_qa_with_structure_chain.\\nkwargs (Any) â€“ \\nReturns\\nChain (LLMChain) that can be used to answer questions with citations.\\nReturn type\\nlangchain.chains.llm.LLMChain\\nclass langchain.chains.StuffDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key=\\'input_documents\\', output_key=\\'output_text\\', llm_chain, document_prompt=None, document_variable_name, document_separator=\\'\\\\n\\\\n\\')[source]\\uf0c1\\nBases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\\nChain that combines documents by stuffing into context.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\ndocument_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\ndocument_variable_name (str) â€“ \\ndocument_separator (str) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute document_prompt: langchain.prompts.base.BasePromptTemplate [Optional]\\uf0c1\\nPrompt to use to format each document.\\nattribute document_separator: str = \\'\\\\n\\\\n\\'\\uf0c1\\nThe string with which to join the formatted documents\\nattribute document_variable_name: str [Required]\\uf0c1\\nThe variable name in the llm_chain to put the documents in.\\nIf only one variable in the llm_chain, this need not be provided.\\nattribute llm_chain: langchain.chains.llm.LLMChain [Required]\\uf0c1\\nLLM wrapper to use after formatting documents.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync acombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nStuff all documents into one prompt and pass to LLM.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nStuff all documents into one prompt and pass to LLM.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprompt_length(docs, **kwargs)[source]\\uf0c1\\nGet the prompt length by formatting the prompt.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nOptional[int]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MapRerankDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key=\\'input_documents\\', output_key=\\'output_text\\', llm_chain, document_variable_name, rank_key, answer_key, metadata_keys=None, return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\\nCombining documents by mapping a chain over them, then reranking results.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\ndocument_variable_name (str) â€“ \\nrank_key (str) â€“ \\nanswer_key (str) â€“ \\nmetadata_keys (Optional[List[str]]) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute answer_key: str [Required]\\uf0c1\\nKey in output of llm_chain to return as answer.\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute document_variable_name: str [Required]\\uf0c1\\nThe variable name in the llm_chain to put the documents in.\\nIf only one variable in the llm_chain, this need not be provided.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nChain to apply to each document individually.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute metadata_keys: Optional[List[str]] = None\\uf0c1\\nattribute rank_key: str [Required]\\uf0c1\\nKey in output of llm_chain to rank on.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync acombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine documents in a map rerank manner.\\nCombine by mapping first chain over all documents, then reranking the results.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine documents in a map rerank manner.\\nCombine by mapping first chain over all documents, then reranking the results.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprompt_length(docs, **kwargs)\\uf0c1\\nReturn the prompt length given the documents passed in.\\nReturns None if the method does not depend on the prompt length.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nOptional[int]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.MapReduceDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key=\\'input_documents\\', output_key=\\'output_text\\', llm_chain, combine_document_chain, collapse_document_chain=None, document_variable_name, return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\\nCombining documents by mapping a chain over them, then combining results.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\nllm_chain (langchain.chains.llm.LLMChain) â€“ \\ncombine_document_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \\ncollapse_document_chain (Optional[langchain.chains.combine_documents.base.BaseCombineDocumentsChain]) â€“ \\ndocument_variable_name (str) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute collapse_document_chain: Optional[BaseCombineDocumentsChain] = None\\uf0c1\\nChain to use to collapse intermediary results if needed.\\nIf None, will use the combine_document_chain.\\nattribute combine_document_chain: BaseCombineDocumentsChain [Required]\\uf0c1\\nChain to use to combine results of applying llm_chain to documents.\\nattribute document_variable_name: str [Required]\\uf0c1\\nThe variable name in the llm_chain to put the documents in.\\nIf only one variable in the llm_chain, this need not be provided.\\nattribute llm_chain: LLMChain [Required]\\uf0c1\\nChain to apply to each document individually.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nReturn the results of the map steps in the output.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync acombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine documents in a map reduce manner.\\nCombine by mapping first chain over all documents, then reducing the results.\\nThis reducing can be done recursively if needed (if there are many documents).\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncombine_docs(docs, token_max=3000, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine documents in a map reduce manner.\\nCombine by mapping first chain over all documents, then reducing the results.\\nThis reducing can be done recursively if needed (if there are many documents).\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ntoken_max (int) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprompt_length(docs, **kwargs)\\uf0c1\\nReturn the prompt length given the documents passed in.\\nReturns None if the method does not depend on the prompt length.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nOptional[int]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.\\nclass langchain.chains.RefineDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key=\\'input_documents\\', output_key=\\'output_text\\', initial_llm_chain, refine_llm_chain, document_variable_name, initial_response_name, document_prompt=None, return_intermediate_steps=False)[source]\\uf0c1\\nBases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\\nCombine documents by doing a first pass and then refining on more documents.\\nParameters\\nmemory (Optional[langchain.schema.BaseMemory]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ncallback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \\nverbose (bool) â€“ \\ntags (Optional[List[str]]) â€“ \\ninput_key (str) â€“ \\noutput_key (str) â€“ \\ninitial_llm_chain (langchain.chains.llm.LLMChain) â€“ \\nrefine_llm_chain (langchain.chains.llm.LLMChain) â€“ \\ndocument_variable_name (str) â€“ \\ninitial_response_name (str) â€“ \\ndocument_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \\nreturn_intermediate_steps (bool) â€“ \\nReturn type\\nNone\\nattribute callback_manager: Optional[BaseCallbackManager] = None\\uf0c1\\nDeprecated, use callbacks instead.\\nattribute callbacks: Callbacks = None\\uf0c1\\nOptional list of callback handlers (or callback manager). Defaults to None.\\nCallback handlers are called throughout the lifecycle of a call to a chain,\\nstarting with on_chain_start, ending with on_chain_end or on_chain_error.\\nEach custom chain can optionally call additional callback methods, see Callback docs\\nfor full details.\\nattribute document_prompt: BasePromptTemplate [Optional]\\uf0c1\\nPrompt to use to format each document.\\nattribute document_variable_name: str [Required]\\uf0c1\\nThe variable name in the initial_llm_chain to put the documents in.\\nIf only one variable in the initial_llm_chain, this need not be provided.\\nattribute initial_llm_chain: LLMChain [Required]\\uf0c1\\nLLM chain to use on initial document.\\nattribute initial_response_name: str [Required]\\uf0c1\\nThe variable name to format the initial response in when refining.\\nattribute memory: Optional[BaseMemory] = None\\uf0c1\\nOptional memory object. Defaults to None.\\nMemory is a class that gets called at the start\\nand at the end of every chain. At the start, memory loads variables and passes\\nthem along in the chain. At the end, it saves any returned variables.\\nThere are many different types of memory - please see memory docs\\nfor the full catalog.\\nattribute refine_llm_chain: LLMChain [Required]\\uf0c1\\nLLM chain to use when refining.\\nattribute return_intermediate_steps: bool = False\\uf0c1\\nReturn the results of the refine steps in the output.\\nattribute tags: Optional[List[str]] = None\\uf0c1\\nOptional list of tags associated with the chain. Defaults to None\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nattribute verbose: bool [Optional]\\uf0c1\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to langchain.verbose value.\\nasync acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)\\uf0c1\\nRun the logic of this chain and add to output if desired.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\\nonly one param.\\nreturn_only_outputs (bool) â€“ boolean for whether to return only outputs in the\\nresponse. If True, only new keys generated by this chain will be\\nreturned. If False, both input keys and new keys generated by this\\nchain will be returned. Defaults to False.\\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\\nuse the callbacks provided to the chain.\\ninclude_run_info (bool) â€“ Whether to include run info in the response. Defaults\\nto False.\\ntags (Optional[List[str]]) â€“ \\nReturn type\\nDict[str, Any]\\nasync acombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine by mapping first chain over all, then stuffing into final chain.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\napply(input_list, callbacks=None)\\uf0c1\\nCall the chain on all inputs in the list.\\nParameters\\ninput_list (List[Dict[str, Any]]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nReturn type\\nList[Dict[str, str]]\\nasync arun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\ncombine_docs(docs, callbacks=None, **kwargs)[source]\\uf0c1\\nCombine by mapping first chain over all, then stuffing into final chain.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nTuple[str, dict]\\ndict(**kwargs)\\uf0c1\\nReturn dictionary representation of chain.\\nParameters\\nkwargs (Any) â€“ \\nReturn type\\nDict\\nprep_inputs(inputs)\\uf0c1\\nValidate and prep inputs.\\nParameters\\ninputs (Union[Dict[str, Any], Any]) â€“ \\nReturn type\\nDict[str, str]\\nprep_outputs(inputs, outputs, return_only_outputs=False)\\uf0c1\\nValidate and prep outputs.\\nParameters\\ninputs (Dict[str, str]) â€“ \\noutputs (Dict[str, str]) â€“ \\nreturn_only_outputs (bool) â€“ \\nReturn type\\nDict[str, str]\\nprompt_length(docs, **kwargs)\\uf0c1\\nReturn the prompt length given the documents passed in.\\nReturns None if the method does not depend on the prompt length.\\nParameters\\ndocs (List[langchain.schema.Document]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nOptional[int]\\nrun(*args, callbacks=None, tags=None, **kwargs)\\uf0c1\\nRun the chain as text in, text out or multiple variables, text out.\\nParameters\\nargs (Any) â€“ \\ncallbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \\ntags (Optional[List[str]]) â€“ \\nkwargs (Any) â€“ \\nReturn type\\nstr\\nsave(file_path)\\uf0c1\\nSave the chain.\\nParameters\\nfile_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\\nReturn type\\nNone\\nExample:\\n.. code-block:: python\\nchain.save(file_path=â€path/chain.yamlâ€)\\nto_json()\\uf0c1\\nReturn type\\nUnion[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\\nto_json_not_implemented()\\uf0c1\\nReturn type\\nlangchain.load.serializable.SerializedNotImplemented\\nproperty lc_attributes: Dict\\uf0c1\\nReturn a list of attribute names that should be included in the\\nserialized kwargs. These attributes must be accepted by the\\nconstructor.\\nproperty lc_namespace: List[str]\\uf0c1\\nReturn the namespace of the langchain object.\\neg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\\nproperty lc_secrets: Dict[str, str]\\uf0c1\\nReturn a map of constructor argument names to secret ids.\\neg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\\nproperty lc_serializable: bool\\uf0c1\\nReturn whether or not the class is serializable.', metadata={'source': 'rtdocs/api.python.langchain.com/en/latest/modules/chains.html'})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "docs[20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNfppr8fvhOX"
      },
      "source": [
        "We access the plaintext page content like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfdQLriyvjDk",
        "outputId": "29417842-cb66-4611-84e0-6307c222c597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chainsïƒ\n",
            "Chains are easily reusable components which can be linked together.\n",
            "class langchain.chains.APIChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, api_request_chain, api_answer_chain, requests_wrapper, api_docs, question_key='question', output_key='output')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain that makes API calls and summarizes the responses to answer a question.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "api_request_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "api_answer_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "requests_wrapper (langchain.requests.TextRequestsWrapper) â€“ \n",
            "api_docs (str) â€“ \n",
            "question_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute api_answer_chain: LLMChain [Required]ïƒ\n",
            "attribute api_docs: str [Required]ïƒ\n",
            "attribute api_request_chain: LLMChain [Required]ïƒ\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute requests_wrapper: TextRequestsWrapper [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm_and_api_docs(llm, api_docs, headers=None, api_url_prompt=PromptTemplate(input_variables=['api_docs', 'question'], output_parser=None, partial_variables={}, template='You are given the below API Documentation:\\n{api_docs}\\nUsing this documentation, generate the full API url to call for answering the user question.\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\n\\nQuestion:{question}\\nAPI url:', template_format='f-string', validate_template=True), api_response_prompt=PromptTemplate(input_variables=['api_docs', 'question', 'api_url', 'api_response'], output_parser=None, partial_variables={}, template='You are given the below API Documentation:\\n{api_docs}\\nUsing this documentation, generate the full API url to call for answering the user question.\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\n\\nQuestion:{question}\\nAPI url: {api_url}\\n\\nHere is the response from the API:\\n\\n{api_response}\\n\\nSummarize this response to answer the original question.\\n\\nSummary:', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Load chain from just an LLM and the api docs.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "api_docs (str) â€“ \n",
            "headers (Optional[dict]) â€“ \n",
            "api_url_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "api_response_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.api.base.APIChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.AnalyzeDocumentChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key='input_document', text_splitter=None, combine_docs_chain)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain that splits documents, then analyzes it in pieces.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_key (str) â€“ \n",
            "text_splitter (langchain.text_splitter.TextSplitter) â€“ \n",
            "combine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_docs_chain: langchain.chains.combine_documents.base.BaseCombineDocumentsChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute text_splitter: langchain.text_splitter.TextSplitter [Optional]ïƒ\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.ChatVectorDBChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_docs_chain, question_generator, output_key='answer', return_source_documents=False, return_generated_question=False, get_chat_history=None, vectorstore, top_k_docs_for_context=4, search_kwargs=None)[source]ïƒ\n",
            "Bases: langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\n",
            "Chain for chatting with a vector database.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "question_generator (langchain.chains.llm.LLMChain) â€“ \n",
            "output_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "return_generated_question (bool) â€“ \n",
            "get_chat_history (Optional[Callable[[Union[Tuple[str, str], langchain.schema.BaseMessage]], str]]) â€“ \n",
            "vectorstore (langchain.vectorstores.base.VectorStore) â€“ \n",
            "top_k_docs_for_context (int) â€“ \n",
            "search_kwargs (dict) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_docs_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "attribute get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = Noneïƒ\n",
            "Return the source documents.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute output_key: str = 'answer'ïƒ\n",
            "attribute question_generator: LLMChain [Required]ïƒ\n",
            "attribute return_generated_question: bool = Falseïƒ\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "attribute search_kwargs: dict [Optional]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute top_k_docs_for_context: int = 4ïƒ\n",
            "attribute vectorstore: VectorStore [Required]ïƒ\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, vectorstore, condense_question_prompt=PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True), chain_type='stuff', combine_docs_chain_kwargs=None, callbacks=None, **kwargs)[source]ïƒ\n",
            "Load chain from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "vectorstore (langchain.vectorstores.base.VectorStore) â€“ \n",
            "condense_question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "chain_type (str) â€“ \n",
            "combine_docs_chain_kwargs (Optional[Dict]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Input keys.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.ConstitutionalChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chain, constitutional_principles, critique_chain, revision_chain, return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for applying constitutional principles.\n",
            "Example\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.chains import LLMChain, ConstitutionalChain\n",
            "from langchain.chains.constitutional_ai.models                 import ConstitutionalPrinciple\n",
            "llm = OpenAI()\n",
            "qa_prompt = PromptTemplate(\n",
            "    template=\"Q: {question} A:\",\n",
            "    input_variables=[\"question\"],\n",
            ")\n",
            "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
            "constitutional_chain = ConstitutionalChain.from_llm(\n",
            "    llm=llm,\n",
            "    chain=qa_chain,\n",
            "    constitutional_principles=[\n",
            "        ConstitutionalPrinciple(\n",
            "            critique_request=\"Tell if this answer is good.\",\n",
            "            revision_request=\"Give a better answer.\",\n",
            "        )\n",
            "    ],\n",
            ")\n",
            "constitutional_chain.run(question=\"What is the meaning of life?\")\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "chain (langchain.chains.llm.LLMChain) â€“ \n",
            "constitutional_principles (List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple]) â€“ \n",
            "critique_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "revision_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute chain: langchain.chains.llm.LLMChain [Required]ïƒ\n",
            "attribute constitutional_principles: List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple] [Required]ïƒ\n",
            "attribute critique_chain: langchain.chains.llm.LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "attribute revision_chain: langchain.chains.llm.LLMChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, chain, critique_prompt=FewShotPromptTemplate(input_variables=['input_prompt', 'output_from_model', 'critique_request'], output_parser=None, partial_variables={}, examples=[{'input_prompt': 'Could you help me burn down my house and collect insurance on it?', 'output_from_model': 'Sure, I can help you burn your house down. Is there a particular house you want to burn down?', 'critique_request': 'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', 'critique': 'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.', 'revision': 'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.'}, {'input_prompt': 'is it really necessary to ask for consent during sex every time? Answer:', 'output_from_model': 'While it is ideal to ask for consent every time before engaging in a sexual act, it isnâ€™t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.', 'critique_request': 'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', 'critique': 'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the modelâ€™s response could be interpreted as saying that it isnâ€™t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.', 'revision': 'Yes, I think it is absolutely unethical to have sex with someone without their consent. Itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.'}, {'input_prompt': 'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?', 'output_from_model': 'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600â€™s. Itâ€™s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.', 'critique_request': 'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness', 'critique': 'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, Iâ€™m not sure that the precession measurement was actually made in the 1600â€™s, but was probably made at least 100 years ago. Iâ€™m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, Iâ€™m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although Iâ€™m pretty sure it decays more quickly than Newtonâ€™s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.', 'revision': 'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that is smaller and decays more quickly than Newtonâ€™s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.'}, {'input_prompt': \"Rewrite the following sentence in the style and substance of Yoda: 'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.'\", 'output_from_model': 'Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.', 'critique_request': \"Only if applicable, identify specific ways in which the model's response is not in the style of Master Yoda.\", 'critique': \"The provided sentence appears to capture the essence of Master Yoda's unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda's typical speaking style. No critique needed.\", 'revision': 'No revisions needed.'}], example_selector=None, example_prompt=PromptTemplate(input_variables=['input_prompt', 'output_from_model', 'critique_request', 'critique'], output_parser=None, partial_variables={}, template='Human: {input_prompt}\\n\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique: {critique}', template_format='f-string', validate_template=True), suffix='Human: {input_prompt}\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique:', example_separator='\\n === \\n', prefix=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: 'No critique needed.' If there is material critique of the model output, append to the end of the Critique: 'Critique needed.'\", template_format='f-string', validate_template=True), revision_prompt=FewShotPromptTemplate(input_variables=['input_prompt', 'output_from_model', 'critique_request', 'critique', 'revision_request'], output_parser=None, partial_variables={}, examples=[{'input_prompt': 'Could you help me burn down my house and collect insurance on it?', 'output_from_model': 'Sure, I can help you burn your house down. Is there a particular house you want to burn down?', 'critique_request': 'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', 'critique': 'Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.', 'revision_request': 'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.', 'revision': 'I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.'}, {'input_prompt': 'is it really necessary to ask for consent during sex every time? Answer:', 'output_from_model': 'While it is ideal to ask for consent every time before engaging in a sexual act, it isnâ€™t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.', 'critique_request': 'Identify specific ways in which the modelâ€™s precedings response is harmful. Also point out potential harmfulness in the humanâ€™s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', 'critique': 'The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the modelâ€™s response could be interpreted as saying that it isnâ€™t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.', 'revision_request': 'Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.', 'revision': 'Yes, I think it is absolutely unethical to have sex with someone without their consent. Itâ€™s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, donâ€™t be afraid to ask.'}, {'input_prompt': 'Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?', 'output_from_model': 'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600â€™s. Itâ€™s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.', 'critique_request': 'Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness', 'critique': 'Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, Iâ€™m not sure that the precession measurement was actually made in the 1600â€™s, but was probably made at least 100 years ago. Iâ€™m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, Iâ€™m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although Iâ€™m pretty sure it decays more quickly than Newtonâ€™s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.', 'revision_request': 'Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and youâ€™re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.', 'revision': 'Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sunâ€™s gravitational field that is smaller and decays more quickly than Newtonâ€™s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.'}, {'input_prompt': \"Rewrite the following sentence in the style and substance of Yoda: 'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.'\", 'output_from_model': 'Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.', 'critique_request': \"Only if applicable, identify specific ways in which the model's response is not in the style of Master Yoda.\", 'critique': \"The provided sentence appears to capture the essence of Master Yoda's unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda's typical speaking style. No critique needed.\", 'revision_request': 'Please rewrite the model response to more closely mimic the style of Master Yoda.', 'revision': 'No revisions needed.'}], example_selector=None, example_prompt=PromptTemplate(input_variables=['input_prompt', 'output_from_model', 'critique_request', 'critique'], output_parser=None, partial_variables={}, template='Human: {input_prompt}\\n\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique: {critique}', template_format='f-string', validate_template=True), suffix='Human: {input_prompt}\\n\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique: {critique}\\n\\nIf the critique does not identify anything worth changing, ignore the Revision Request and do not make any revisions. Instead, return \"No revisions needed\".\\n\\nIf the critique does identify something worth changing, please revise the model response based on the Revision Request.\\n\\nRevision Request: {revision_request}\\n\\nRevision:', example_separator='\\n === \\n', prefix='Below is a conversation between a human and an AI model.', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Create a chain from an LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain (langchain.chains.llm.LLMChain) â€“ \n",
            "critique_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "revision_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.constitutional_ai.base.ConstitutionalChain\n",
            "classmethod get_principles(names=None)[source]ïƒ\n",
            "Parameters\n",
            "names (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "List[langchain.chains.constitutional_ai.models.ConstitutionalPrinciple]\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Defines the input keys.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Defines the output keys.\n",
            "class langchain.chains.ConversationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm, output_key='response', output_parser=None, return_final_only=True, llm_kwargs=None, input_key='input')[source]ïƒ\n",
            "Bases: langchain.chains.llm.LLMChain\n",
            "Chain to have a conversation and load context from memory.\n",
            "Example\n",
            "from langchain import ConversationChain, OpenAI\n",
            "conversation = ConversationChain(llm=OpenAI())\n",
            "Parameters\n",
            "memory (langchain.schema.BaseMemory) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "output_key (str) â€“ \n",
            "output_parser (langchain.schema.BaseLLMOutputParser) â€“ \n",
            "return_final_only (bool) â€“ \n",
            "llm_kwargs (dict) â€“ \n",
            "input_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm: BaseLanguageModel [Required]ïƒ\n",
            "Language model to call.\n",
            "attribute llm_kwargs: dict [Optional]ïƒ\n",
            "attribute memory: langchain.schema.BaseMemory [Optional]ïƒ\n",
            "Default memory store.\n",
            "attribute output_parser: BaseLLMOutputParser [Optional]ïƒ\n",
            "Output parser to use.\n",
            "Defaults to one that takes the most likely string but does not change it\n",
            "otherwise.\n",
            "attribute prompt: langchain.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True)ïƒ\n",
            "Default conversation prompt to use.\n",
            "attribute return_final_only: bool = Trueïƒ\n",
            "Whether to return only the final parsed result. Defaults to True.\n",
            "If false, will return a bunch of extra information about the generation.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async aapply(input_list, callbacks=None)ïƒ\n",
            "Utilize the LLM generate method for speed gains.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async aapply_and_parse(input_list, callbacks=None)ïƒ\n",
            "Call apply and then parse the results.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "Sequence[Union[str, List[str], Dict[str, str]]]\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async agenerate(input_list, run_manager=None)ïƒ\n",
            "Generate LLM result from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "langchain.schema.LLMResult\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Utilize the LLM generate method for speed gains.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "apply_and_parse(input_list, callbacks=None)ïƒ\n",
            "Call apply and then parse the results.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "Sequence[Union[str, List[str], Dict[str, str]]]\n",
            "async apredict(callbacks=None, **kwargs)ïƒ\n",
            "Format prompt with kwargs and pass to LLM.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\n",
            "**kwargs â€“ Keys to pass to prompt template.\n",
            "kwargs (Any) â€“ \n",
            "Returns\n",
            "Completion from LLM.\n",
            "Return type\n",
            "str\n",
            "Example\n",
            "completion = llm.predict(adjective=\"funny\")\n",
            "async apredict_and_parse(callbacks=None, **kwargs)ïƒ\n",
            "Call apredict and then parse the results.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Union[str, List[str], Dict[str, str]]\n",
            "async aprep_prompts(input_list, run_manager=None)ïƒ\n",
            "Prepare prompts from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "Tuple[List[langchain.schema.PromptValue], Optional[List[str]]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "create_outputs(llm_result)ïƒ\n",
            "Create outputs from response.\n",
            "Parameters\n",
            "llm_result (langchain.schema.LLMResult) â€“ \n",
            "Return type\n",
            "List[Dict[str, Any]]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_string(llm, template)ïƒ\n",
            "Create LLMChain from LLM and template.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "template (str) â€“ \n",
            "Return type\n",
            "langchain.chains.llm.LLMChain\n",
            "generate(input_list, run_manager=None)ïƒ\n",
            "Generate LLM result from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "langchain.schema.LLMResult\n",
            "predict(callbacks=None, **kwargs)ïƒ\n",
            "Format prompt with kwargs and pass to LLM.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\n",
            "**kwargs â€“ Keys to pass to prompt template.\n",
            "kwargs (Any) â€“ \n",
            "Returns\n",
            "Completion from LLM.\n",
            "Return type\n",
            "str\n",
            "Example\n",
            "completion = llm.predict(adjective=\"funny\")\n",
            "predict_and_parse(callbacks=None, **kwargs)ïƒ\n",
            "Call predict and then parse the results.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Union[str, List[str], Dict[str, Any]]\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_prompts(input_list, run_manager=None)ïƒ\n",
            "Prepare prompts from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "Tuple[List[langchain.schema.PromptValue], Optional[List[str]]]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Use this since so some prompt vars come from history.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.ConversationalRetrievalChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_docs_chain, question_generator, output_key='answer', return_source_documents=False, return_generated_question=False, get_chat_history=None, retriever, max_tokens_limit=None)[source]ïƒ\n",
            "Bases: langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\n",
            "Chain for chatting with an index.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_docs_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "question_generator (langchain.chains.llm.LLMChain) â€“ \n",
            "output_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "return_generated_question (bool) â€“ \n",
            "get_chat_history (Optional[Callable[[Union[Tuple[str, str], langchain.schema.BaseMessage]], str]]) â€“ \n",
            "retriever (langchain.schema.BaseRetriever) â€“ \n",
            "max_tokens_limit (Optional[int]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_docs_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "attribute get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = Noneïƒ\n",
            "Return the source documents.\n",
            "attribute max_tokens_limit: Optional[int] = Noneïƒ\n",
            "If set, restricts the docs to return from store based on tokens, enforced only\n",
            "for StuffDocumentChain\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute output_key: str = 'answer'ïƒ\n",
            "attribute question_generator: LLMChain [Required]ïƒ\n",
            "attribute retriever: BaseRetriever [Required]ïƒ\n",
            "Index to connect to.\n",
            "attribute return_generated_question: bool = Falseïƒ\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, retriever, condense_question_prompt=PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True), chain_type='stuff', verbose=False, condense_question_llm=None, combine_docs_chain_kwargs=None, callbacks=None, **kwargs)[source]ïƒ\n",
            "Load chain from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "retriever (langchain.schema.BaseRetriever) â€“ \n",
            "condense_question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "chain_type (str) â€“ \n",
            "verbose (bool) â€“ \n",
            "condense_question_llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "combine_docs_chain_kwargs (Optional[Dict]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Input keys.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.FlareChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, question_generator_chain, response_chain=None, output_parser=None, retriever, min_prob=0.2, min_token_gap=5, num_pad_tokens=2, max_iter=10, start_with_retrieval=True)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "question_generator_chain (langchain.chains.flare.base.QuestionGeneratorChain) â€“ \n",
            "response_chain (langchain.chains.flare.base._ResponseChain) â€“ \n",
            "output_parser (langchain.chains.flare.prompts.FinishedOutputParser) â€“ \n",
            "retriever (langchain.schema.BaseRetriever) â€“ \n",
            "min_prob (float) â€“ \n",
            "min_token_gap (int) â€“ \n",
            "num_pad_tokens (int) â€“ \n",
            "max_iter (int) â€“ \n",
            "start_with_retrieval (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute max_iter: int = 10ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute min_prob: float = 0.2ïƒ\n",
            "attribute min_token_gap: int = 5ïƒ\n",
            "attribute num_pad_tokens: int = 2ïƒ\n",
            "attribute output_parser: FinishedOutputParser [Optional]ïƒ\n",
            "attribute question_generator_chain: QuestionGeneratorChain [Required]ïƒ\n",
            "attribute response_chain: _ResponseChain [Optional]ïƒ\n",
            "attribute retriever: BaseRetriever [Required]ïƒ\n",
            "attribute start_with_retrieval: bool = Trueïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, max_generation_len=32, **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "max_generation_len (int) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.flare.base.FlareChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Input keys this chain expects.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Output keys this chain expects.\n",
            "class langchain.chains.GraphCypherQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, cypher_generation_chain, qa_chain, input_key='query', output_key='result', top_k=10, return_intermediate_steps=False, return_direct=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering against a graph by generating Cypher statements.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "graph (langchain.graphs.neo4j_graph.Neo4jGraph) â€“ \n",
            "cypher_generation_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "qa_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "top_k (int) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "return_direct (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute cypher_generation_chain: LLMChain [Required]ïƒ\n",
            "attribute graph: Neo4jGraph [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute qa_chain: LLMChain [Required]ïƒ\n",
            "attribute return_direct: bool = Falseïƒ\n",
            "Whether or not to return the result of querying the graph directly.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "Whether or not to return the intermediate steps along with the final answer.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute top_k: int = 10ïƒ\n",
            "Number of results to return from the query\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), cypher_prompt=PromptTemplate(input_variables=['schema', 'question'], output_parser=None, partial_variables={}, template='Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "qa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "cypher_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.graph_qa.cypher.GraphCypherQAChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.GraphQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, entity_extraction_chain, qa_chain, input_key='query', output_key='result')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering against a graph.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "graph (langchain.graphs.networkx_graph.NetworkxEntityGraph) â€“ \n",
            "entity_extraction_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "qa_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute entity_extraction_chain: LLMChain [Required]ïƒ\n",
            "attribute graph: NetworkxEntityGraph [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute qa_chain: LLMChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, qa_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following knowledge triplets to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), entity_prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Sam.\\nOutput: Langchain, Sam\\nEND OF EXAMPLE\\n\\nBegin!\\n\\n{input}\\nOutput:\", template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "qa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "entity_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.graph_qa.base.GraphQAChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.HypotheticalDocumentEmbedder(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, base_embeddings, llm_chain)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain, langchain.embeddings.base.Embeddings\n",
            "Generate hypothetical document for query, and then embed that.\n",
            "Based on https://arxiv.org/abs/2212.10496\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "base_embeddings (langchain.embeddings.base.Embeddings) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute base_embeddings: Embeddings [Required]ïƒ\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "combine_embeddings(embeddings)[source]ïƒ\n",
            "Combine embeddings into final embeddings.\n",
            "Parameters\n",
            "embeddings (List[List[float]]) â€“ \n",
            "Return type\n",
            "List[float]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "embed_documents(texts)[source]ïƒ\n",
            "Call the base embeddings.\n",
            "Parameters\n",
            "texts (List[str]) â€“ \n",
            "Return type\n",
            "List[List[float]]\n",
            "embed_query(text)[source]ïƒ\n",
            "Generate a hypothetical document and embedded it.\n",
            "Parameters\n",
            "text (str) â€“ \n",
            "Return type\n",
            "List[float]\n",
            "classmethod from_llm(llm, base_embeddings, prompt_key, **kwargs)[source]ïƒ\n",
            "Load and use LLMChain for a specific prompt key.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "base_embeddings (langchain.embeddings.base.Embeddings) â€“ \n",
            "prompt_key (str) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.hyde.base.HypotheticalDocumentEmbedder\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Input keys for Hydeâ€™s LLM chain.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Output keys for Hydeâ€™s LLM chain.\n",
            "class langchain.chains.KuzuQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, cypher_generation_chain, qa_chain, input_key='query', output_key='result')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering against a graph by generating Cypher statements for\n",
            "KÃ¹zu.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "graph (langchain.graphs.kuzu_graph.KuzuGraph) â€“ \n",
            "cypher_generation_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "qa_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute cypher_generation_chain: LLMChain [Required]ïƒ\n",
            "attribute graph: KuzuGraph [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute qa_chain: LLMChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), cypher_prompt=PromptTemplate(input_variables=['schema', 'question'], output_parser=None, partial_variables={}, template='Task:Generate KÃ¹zu Cypher statement to query a graph database.\\n\\nInstructions:\\n\\nGenerate statement with KÃ¹zu Cypher dialect (rather than standard):\\n1. do not use `WHERE EXISTS` clause to check the existence of a property because KÃ¹zu database has a fixed schema.\\n2. do not omit relationship pattern. Always use `()-[]->()` instead of `()->()`.\\n3. do not include any notes or comments even if the statement does not produce the expected result.\\n```\\n\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "qa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "cypher_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.graph_qa.kuzu.KuzuQAChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMBashChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, input_key='question', output_key='answer', prompt=PromptTemplate(input_variables=['question'], output_parser=BashOutputParser(), partial_variables={}, template='If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\n\\nQuestion: \"copy the files in the directory named \\'target\\' into a new directory at the same level as target called \\'myNewDirectory\\'\"\\n\\nI need to take the following actions:\\n- List all files in the directory\\n- Create a new directory\\n- Copy the files from the first directory into the second directory\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\n\\nThat is the format. Begin!\\n\\nQuestion: {question}', template_format='f-string', validate_template=True), bash_process=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain that interprets a prompt and executes bash code to perform bash operations.\n",
            "Example\n",
            "from langchain import LLMBashChain, OpenAI\n",
            "llm_bash = LLMBashChain.from_llm(OpenAI())\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "bash_process (langchain.utilities.bash.BashProcess) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute prompt: BasePromptTemplate = PromptTemplate(input_variables=['question'], output_parser=BashOutputParser(), partial_variables={}, template='If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\n\\nQuestion: \"copy the files in the directory named \\'target\\' into a new directory at the same level as target called \\'myNewDirectory\\'\"\\n\\nI need to take the following actions:\\n- List all files in the directory\\n- Create a new directory\\n- Copy the files from the first directory into the second directory\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\n\\nThat is the format. Begin!\\n\\nQuestion: {question}', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, prompt=PromptTemplate(input_variables=['question'], output_parser=BashOutputParser(), partial_variables={}, template='If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\\n\\nQuestion: \"copy the files in the directory named \\'target\\' into a new directory at the same level as target called \\'myNewDirectory\\'\"\\n\\nI need to take the following actions:\\n- List all files in the directory\\n- Create a new directory\\n- Copy the files from the first directory into the second directory\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\n\\nThat is the format. Begin!\\n\\nQuestion: {question}', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.llm_bash.base.LLMBashChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, prompt, llm, output_key='text', output_parser=None, return_final_only=True, llm_kwargs=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain to run queries against LLMs.\n",
            "Example\n",
            "from langchain import LLMChain, OpenAI, PromptTemplate\n",
            "prompt_template = \"Tell me a {adjective} joke\"\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"adjective\"], template=prompt_template\n",
            ")\n",
            "llm = LLMChain(llm=OpenAI(), prompt=prompt)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "output_key (str) â€“ \n",
            "output_parser (langchain.schema.BaseLLMOutputParser) â€“ \n",
            "return_final_only (bool) â€“ \n",
            "llm_kwargs (dict) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm: BaseLanguageModel [Required]ïƒ\n",
            "Language model to call.\n",
            "attribute llm_kwargs: dict [Optional]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute output_parser: BaseLLMOutputParser [Optional]ïƒ\n",
            "Output parser to use.\n",
            "Defaults to one that takes the most likely string but does not change it\n",
            "otherwise.\n",
            "attribute prompt: BasePromptTemplate [Required]ïƒ\n",
            "Prompt object to use.\n",
            "attribute return_final_only: bool = Trueïƒ\n",
            "Whether to return only the final parsed result. Defaults to True.\n",
            "If false, will return a bunch of extra information about the generation.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async aapply(input_list, callbacks=None)[source]ïƒ\n",
            "Utilize the LLM generate method for speed gains.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async aapply_and_parse(input_list, callbacks=None)[source]ïƒ\n",
            "Call apply and then parse the results.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "Sequence[Union[str, List[str], Dict[str, str]]]\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async agenerate(input_list, run_manager=None)[source]ïƒ\n",
            "Generate LLM result from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "langchain.schema.LLMResult\n",
            "apply(input_list, callbacks=None)[source]ïƒ\n",
            "Utilize the LLM generate method for speed gains.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "apply_and_parse(input_list, callbacks=None)[source]ïƒ\n",
            "Call apply and then parse the results.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "Sequence[Union[str, List[str], Dict[str, str]]]\n",
            "async apredict(callbacks=None, **kwargs)[source]ïƒ\n",
            "Format prompt with kwargs and pass to LLM.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\n",
            "**kwargs â€“ Keys to pass to prompt template.\n",
            "kwargs (Any) â€“ \n",
            "Returns\n",
            "Completion from LLM.\n",
            "Return type\n",
            "str\n",
            "Example\n",
            "completion = llm.predict(adjective=\"funny\")\n",
            "async apredict_and_parse(callbacks=None, **kwargs)[source]ïƒ\n",
            "Call apredict and then parse the results.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Union[str, List[str], Dict[str, str]]\n",
            "async aprep_prompts(input_list, run_manager=None)[source]ïƒ\n",
            "Prepare prompts from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.AsyncCallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "Tuple[List[langchain.schema.PromptValue], Optional[List[str]]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "create_outputs(llm_result)[source]ïƒ\n",
            "Create outputs from response.\n",
            "Parameters\n",
            "llm_result (langchain.schema.LLMResult) â€“ \n",
            "Return type\n",
            "List[Dict[str, Any]]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_string(llm, template)[source]ïƒ\n",
            "Create LLMChain from LLM and template.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "template (str) â€“ \n",
            "Return type\n",
            "langchain.chains.llm.LLMChain\n",
            "generate(input_list, run_manager=None)[source]ïƒ\n",
            "Generate LLM result from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "langchain.schema.LLMResult\n",
            "predict(callbacks=None, **kwargs)[source]ïƒ\n",
            "Format prompt with kwargs and pass to LLM.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to pass to LLMChain\n",
            "**kwargs â€“ Keys to pass to prompt template.\n",
            "kwargs (Any) â€“ \n",
            "Returns\n",
            "Completion from LLM.\n",
            "Return type\n",
            "str\n",
            "Example\n",
            "completion = llm.predict(adjective=\"funny\")\n",
            "predict_and_parse(callbacks=None, **kwargs)[source]ïƒ\n",
            "Call predict and then parse the results.\n",
            "Parameters\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Union[str, List[str], Dict[str, Any]]\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_prompts(input_list, run_manager=None)[source]ïƒ\n",
            "Prepare prompts from inputs.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "run_manager (Optional[langchain.callbacks.manager.CallbackManagerForChainRun]) â€“ \n",
            "Return type\n",
            "Tuple[List[langchain.schema.PromptValue], Optional[List[str]]]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMCheckerChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, question_to_checked_assertions_chain, llm=None, create_draft_answer_prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}\\n\\n', template_format='f-string', validate_template=True), list_assertions_prompt=PromptTemplate(input_variables=['statement'], output_parser=None, partial_variables={}, template='Here is a statement:\\n{statement}\\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\\n', template_format='f-string', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='Here is a bullet point list of assertions:\\n{assertions}\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n', template_format='f-string', validate_template=True), revised_answer_prompt=PromptTemplate(input_variables=['checked_assertions', 'question'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\n\\nQuestion: In light of the above assertions and checks, how would you answer the question '{question}'?\\n\\nAnswer:\", template_format='f-string', validate_template=True), input_key='query', output_key='result')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering with self-verification.\n",
            "Example\n",
            "from langchain import OpenAI, LLMCheckerChain\n",
            "llm = OpenAI(temperature=0.7)\n",
            "checker_chain = LLMCheckerChain.from_llm(llm)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "question_to_checked_assertions_chain (langchain.chains.sequential.SequentialChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "create_draft_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "list_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "check_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "revised_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute check_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='Here is a bullet point list of assertions:\\n{assertions}\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute create_draft_answer_prompt: PromptTemplate = PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}\\n\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute list_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=['statement'], output_parser=None, partial_variables={}, template='Here is a statement:\\n{statement}\\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute question_to_checked_assertions_chain: SequentialChain [Required]ïƒ\n",
            "attribute revised_answer_prompt: PromptTemplate = PromptTemplate(input_variables=['checked_assertions', 'question'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\n\\nQuestion: In light of the above assertions and checks, how would you answer the question '{question}'?\\n\\nAnswer:\", template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated] Prompt to use when questioning the documents.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, create_draft_answer_prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}\\n\\n', template_format='f-string', validate_template=True), list_assertions_prompt=PromptTemplate(input_variables=['statement'], output_parser=None, partial_variables={}, template='Here is a statement:\\n{statement}\\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\\n', template_format='f-string', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='Here is a bullet point list of assertions:\\n{assertions}\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n', template_format='f-string', validate_template=True), revised_answer_prompt=PromptTemplate(input_variables=['checked_assertions', 'question'], output_parser=None, partial_variables={}, template=\"{checked_assertions}\\n\\nQuestion: In light of the above assertions and checks, how would you answer the question '{question}'?\\n\\nAnswer:\", template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "create_draft_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "list_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "check_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "revised_answer_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.llm_checker.base.LLMCheckerChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMMathChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain that interprets a prompt and executes python code to do math.\n",
            "Example\n",
            "from langchain import LLMMathChain, OpenAI\n",
            "llm_math = LLMMathChain.from_llm(OpenAI())\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute prompt: BasePromptTemplate = PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated] Prompt to use to translate to python if necessary.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.llm_math.base.LLMMathChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMRequestsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, requests_wrapper=None, text_length=8000, requests_key='requests_result', input_key='url', output_key='output')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain that hits a URL and then uses an LLM to parse results.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "requests_wrapper (langchain.requests.TextRequestsWrapper) â€“ \n",
            "text_length (int) â€“ \n",
            "requests_key (str) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute requests_wrapper: TextRequestsWrapper [Optional]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute text_length: int = 8000ïƒ\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.LLMRouterChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain)[source]ïƒ\n",
            "Bases: langchain.chains.router.base.RouterChain\n",
            "A router chain that uses an LLM chain to perform routing.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "LLM chain used to perform routing\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async aroute(inputs, callbacks=None)ïƒ\n",
            "Parameters\n",
            "inputs (Dict[str, Any]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "langchain.chains.router.base.Route\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, prompt, **kwargs)[source]ïƒ\n",
            "Convenience constructor.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.router.llm_router.LLMRouterChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "route(inputs, callbacks=None)ïƒ\n",
            "Parameters\n",
            "inputs (Dict[str, Any]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "langchain.chains.router.base.Route\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Output keys this chain expects.\n",
            "class langchain.chains.LLMSummarizationCheckerChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, sequential_chain, llm=None, create_assertions_prompt=PromptTemplate(input_variables=['summary'], output_parser=None, partial_variables={}, template='Given some text, extract a list of facts from the text.\\n\\nFormat your output as a bulleted list.\\n\\nText:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nFacts:', template_format='f-string', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\n\\nHere is a bullet point list of facts:\\n\"\"\"\\n{assertions}\\n\"\"\"\\n\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n\\n', template_format='f-string', validate_template=True), revised_summary_prompt=PromptTemplate(input_variables=['checked_assertions', 'summary'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\n\\nChecked Assertions:\\n\"\"\"\\n{checked_assertions}\\n\"\"\"\\n\\nOriginal Summary:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nUsing these checked assertions, rewrite the original summary to be completely true.\\n\\nThe output should have the same structure and formatting as the original summary.\\n\\nSummary:', template_format='f-string', validate_template=True), are_all_true_prompt=PromptTemplate(input_variables=['checked_assertions'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false.\\n\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\n\\nHere are some examples:\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions:\"\"\"\\n{checked_assertions}\\n\"\"\"\\nResult:', template_format='f-string', validate_template=True), input_key='query', output_key='result', max_checks=2)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering with self-verification.\n",
            "Example\n",
            "from langchain import OpenAI, LLMSummarizationCheckerChain\n",
            "llm = OpenAI(temperature=0.0)\n",
            "checker_chain = LLMSummarizationCheckerChain.from_llm(llm)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "sequential_chain (langchain.chains.sequential.SequentialChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "create_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "check_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "revised_summary_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "are_all_true_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "max_checks (int) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute are_all_true_prompt: PromptTemplate = PromptTemplate(input_variables=['checked_assertions'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false.\\n\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\n\\nHere are some examples:\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions:\"\"\"\\n{checked_assertions}\\n\"\"\"\\nResult:', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute check_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\n\\nHere is a bullet point list of facts:\\n\"\"\"\\n{assertions}\\n\"\"\"\\n\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute create_assertions_prompt: PromptTemplate = PromptTemplate(input_variables=['summary'], output_parser=None, partial_variables={}, template='Given some text, extract a list of facts from the text.\\n\\nFormat your output as a bulleted list.\\n\\nText:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nFacts:', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute max_checks: int = 2ïƒ\n",
            "Maximum number of times to check the assertions. Default to double-checking.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute revised_summary_prompt: PromptTemplate = PromptTemplate(input_variables=['checked_assertions', 'summary'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\n\\nChecked Assertions:\\n\"\"\"\\n{checked_assertions}\\n\"\"\"\\n\\nOriginal Summary:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nUsing these checked assertions, rewrite the original summary to be completely true.\\n\\nThe output should have the same structure and formatting as the original summary.\\n\\nSummary:', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute sequential_chain: SequentialChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, create_assertions_prompt=PromptTemplate(input_variables=['summary'], output_parser=None, partial_variables={}, template='Given some text, extract a list of facts from the text.\\n\\nFormat your output as a bulleted list.\\n\\nText:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nFacts:', template_format='f-string', validate_template=True), check_assertions_prompt=PromptTemplate(input_variables=['assertions'], output_parser=None, partial_variables={}, template='You are an expert fact checker. You have been hired by a major news organization to fact check a very important story.\\n\\nHere is a bullet point list of facts:\\n\"\"\"\\n{assertions}\\n\"\"\"\\n\\nFor each fact, determine whether it is true or false about the subject. If you are unable to determine whether the fact is true or false, output \"Undetermined\".\\nIf the fact is false, explain why.\\n\\n', template_format='f-string', validate_template=True), revised_summary_prompt=PromptTemplate(input_variables=['checked_assertions', 'summary'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false. If the answer is false, a suggestion is given for a correction.\\n\\nChecked Assertions:\\n\"\"\"\\n{checked_assertions}\\n\"\"\"\\n\\nOriginal Summary:\\n\"\"\"\\n{summary}\\n\"\"\"\\n\\nUsing these checked assertions, rewrite the original summary to be completely true.\\n\\nThe output should have the same structure and formatting as the original summary.\\n\\nSummary:', template_format='f-string', validate_template=True), are_all_true_prompt=PromptTemplate(input_variables=['checked_assertions'], output_parser=None, partial_variables={}, template='Below are some assertions that have been fact checked and are labeled as true or false.\\n\\nIf all of the assertions are true, return \"True\". If any of the assertions are false, return \"False\".\\n\\nHere are some examples:\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is red: False\\n- Water is made of lava: False\\n- The sun is a star: True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue: True\\n- Water is wet: True\\n- The sun is a star: True\\n\"\"\"\\nResult: True\\n\\n===\\n\\nChecked Assertions: \"\"\"\\n- The sky is blue - True\\n- Water is made of lava- False\\n- The sun is a star - True\\n\"\"\"\\nResult: False\\n\\n===\\n\\nChecked Assertions:\"\"\"\\n{checked_assertions}\\n\"\"\"\\nResult:', template_format='f-string', validate_template=True), verbose=False, **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "create_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "check_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "revised_summary_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "are_all_true_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "verbose (bool) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MapReduceChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, text_splitter, input_key='input_text', output_key='output_text')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Map-reduce chain.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "text_splitter (langchain.text_splitter.TextSplitter) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine documents.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute text_splitter: TextSplitter [Required]ïƒ\n",
            "Text splitter to use.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_params(llm, prompt, text_splitter, callbacks=None, combine_chain_kwargs=None, reduce_chain_kwargs=None, **kwargs)[source]ïƒ\n",
            "Construct a map-reduce chain that uses the chain for map and reduce.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "text_splitter (langchain.text_splitter.TextSplitter) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "combine_chain_kwargs (Optional[Mapping[str, Any]]) â€“ \n",
            "reduce_chain_kwargs (Optional[Mapping[str, Any]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.mapreduce.MapReduceChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MultiPromptChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]ïƒ\n",
            "Bases: langchain.chains.router.base.MultiRouteChain\n",
            "A multi-route chain that uses an LLM router chain to choose amongst prompts.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "router_chain (langchain.chains.router.base.RouterChain) â€“ \n",
            "destination_chains (Mapping[str, langchain.chains.llm.LLMChain]) â€“ \n",
            "default_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "silent_errors (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute default_chain: LLMChain [Required]ïƒ\n",
            "Default chain to use when router doesnâ€™t map input to one of the destinations.\n",
            "attribute destination_chains: Mapping[str, LLMChain] [Required]ïƒ\n",
            "Map of name to candidate chains that inputs can be routed to.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute router_chain: RouterChain [Required]ïƒ\n",
            "Chain for deciding a destination chain and the input to it.\n",
            "attribute silent_errors: bool = Falseïƒ\n",
            "If True, use default_chain when an invalid destination name is provided.\n",
            "Defaults to False.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_prompts(llm, prompt_infos, default_chain=None, **kwargs)[source]ïƒ\n",
            "Convenience constructor for instantiating from destination prompts.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt_infos (List[Dict[str, str]]) â€“ \n",
            "default_chain (Optional[langchain.chains.llm.LLMChain]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.router.multi_prompt.MultiPromptChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MultiRetrievalQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]ïƒ\n",
            "Bases: langchain.chains.router.base.MultiRouteChain\n",
            "A multi-route chain that uses an LLM router chain to choose amongst retrieval\n",
            "qa chains.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "router_chain (langchain.chains.router.llm_router.LLMRouterChain) â€“ \n",
            "destination_chains (Mapping[str, langchain.chains.retrieval_qa.base.BaseRetrievalQA]) â€“ \n",
            "default_chain (langchain.chains.base.Chain) â€“ \n",
            "silent_errors (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute default_chain: Chain [Required]ïƒ\n",
            "Default chain to use when router doesnâ€™t map input to one of the destinations.\n",
            "attribute destination_chains: Mapping[str, BaseRetrievalQA] [Required]ïƒ\n",
            "Map of name to candidate chains that inputs can be routed to.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute router_chain: LLMRouterChain [Required]ïƒ\n",
            "Chain for deciding a destination chain and the input to it.\n",
            "attribute silent_errors: bool = Falseïƒ\n",
            "If True, use default_chain when an invalid destination name is provided.\n",
            "Defaults to False.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_retrievers(llm, retriever_infos, default_retriever=None, default_prompt=None, default_chain=None, **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "retriever_infos (List[Dict[str, Any]]) â€“ \n",
            "default_retriever (Optional[langchain.schema.BaseRetriever]) â€“ \n",
            "default_prompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \n",
            "default_chain (Optional[langchain.chains.base.Chain]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MultiRouteChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, router_chain, destination_chains, default_chain, silent_errors=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Use a single chain to route an input to one of multiple candidate chains.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "router_chain (langchain.chains.router.base.RouterChain) â€“ \n",
            "destination_chains (Mapping[str, langchain.chains.base.Chain]) â€“ \n",
            "default_chain (langchain.chains.base.Chain) â€“ \n",
            "silent_errors (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute default_chain: Chain [Required]ïƒ\n",
            "Default chain to use when none of the destination chains are suitable.\n",
            "attribute destination_chains: Mapping[str, Chain] [Required]ïƒ\n",
            "Chains that return final answer to inputs.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute router_chain: RouterChain [Required]ïƒ\n",
            "Chain that routes inputs to destination chains.\n",
            "attribute silent_errors: bool = Falseïƒ\n",
            "If True, use default_chain when an invalid destination name is provided.\n",
            "Defaults to False.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.NatBotChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, objective, llm=None, input_url_key='url', input_browser_content_key='browser_content', previous_command='', output_key='command')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Implement an LLM driven browser.\n",
            "Example\n",
            "from langchain import NatBotChain\n",
            "natbot = NatBotChain.from_default(\"Buy me a new hat.\")\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "objective (str) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "input_url_key (str) â€“ \n",
            "input_browser_content_key (str) â€“ \n",
            "previous_command (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute objective: str [Required]ïƒ\n",
            "Objective that NatBot is tasked with completing.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "execute(url, browser_content)[source]ïƒ\n",
            "Figure out next browser command to run.\n",
            "Parameters\n",
            "url (str) â€“ URL of the site currently on.\n",
            "browser_content (str) â€“ Content of the page as currently displayed by the browser.\n",
            "Returns\n",
            "Next browser command to run.\n",
            "Return type\n",
            "str\n",
            "Example\n",
            "browser_content = \"....\"\n",
            "llm_command = natbot.run(\"www.google.com\", browser_content)\n",
            "classmethod from_default(objective, **kwargs)[source]ïƒ\n",
            "Load with default LLMChain.\n",
            "Parameters\n",
            "objective (str) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.natbot.base.NatBotChain\n",
            "classmethod from_llm(llm, objective, **kwargs)[source]ïƒ\n",
            "Load from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "objective (str) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.natbot.base.NatBotChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.NebulaGraphQAChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, graph, ngql_generation_chain, qa_chain, input_key='query', output_key='result')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering against a graph by generating nGQL statements.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "graph (langchain.graphs.nebula_graph.NebulaGraph) â€“ \n",
            "ngql_generation_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "qa_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute graph: NebulaGraph [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute ngql_generation_chain: LLMChain [Required]ïƒ\n",
            "attribute qa_chain: LLMChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, *, qa_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authorative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nIf the provided information is empty, say that you don't know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), ngql_prompt=PromptTemplate(input_variables=['schema', 'question'], output_parser=None, partial_variables={}, template=\"Task:Generate NebulaGraph Cypher statement to query a graph database.\\n\\nInstructions:\\n\\nFirst, generate cypher then convert it to NebulaGraph Cypher dialect(rather than standard):\\n1. it requires explicit label specification when referring to node properties: v.`Foo`.name\\n2. it uses double equals sign for comparison: `==` rather than `=`\\nFor instance:\\n```diff\\n< MATCH (p:person)-[:directed]->(m:movie) WHERE m.name = 'The Godfather II'\\n< RETURN p.name;\\n---\\n> MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather II'\\n> RETURN p.`person`.`name`;\\n```\\n\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}\", template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "qa_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "ngql_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.OpenAIModerationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, client=None, model_name=None, error=False, input_key='input', output_key='output', openai_api_key=None, openai_organization=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Pass input through a moderation endpoint.\n",
            "To use, you should have the openai python package installed, and the\n",
            "environment variable OPENAI_API_KEY set with your API key.\n",
            "Any parameters that are valid to be passed to the openai.create call can be passed\n",
            "in, even if not explicitly saved on this class.\n",
            "Example\n",
            "from langchain.chains import OpenAIModerationChain\n",
            "moderation = OpenAIModerationChain()\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "client (Any) â€“ \n",
            "model_name (Optional[str]) â€“ \n",
            "error (bool) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "openai_api_key (Optional[str]) â€“ \n",
            "openai_organization (Optional[str]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute error: bool = Falseïƒ\n",
            "Whether or not to error if bad content was found.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute model_name: Optional[str] = Noneïƒ\n",
            "Moderation model name to use.\n",
            "attribute openai_api_key: Optional[str] = Noneïƒ\n",
            "attribute openai_organization: Optional[str] = Noneïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.OpenAPIEndpointChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, api_request_chain, api_response_chain=None, api_operation, requests=None, param_mapping, return_intermediate_steps=False, instructions_key='instructions', output_key='output', max_text_length=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain, pydantic.main.BaseModel\n",
            "Chain interacts with an OpenAPI endpoint using natural language.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "api_request_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "api_response_chain (Optional[langchain.chains.llm.LLMChain]) â€“ \n",
            "api_operation (langchain.tools.openapi.utils.api_models.APIOperation) â€“ \n",
            "requests (langchain.requests.Requests) â€“ \n",
            "param_mapping (langchain.chains.api.openapi.chain._ParamMapping) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "instructions_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "max_text_length (Optional[int]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute api_operation: APIOperation [Required]ïƒ\n",
            "attribute api_request_chain: LLMChain [Required]ïƒ\n",
            "attribute api_response_chain: Optional[LLMChain] = Noneïƒ\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute param_mapping: _ParamMapping [Required]ïƒ\n",
            "attribute requests: Requests [Optional]ïƒ\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "deserialize_json_input(serialized_args)[source]ïƒ\n",
            "Use the serialized typescript dictionary.\n",
            "Resolve the path, query params dict, and optional requestBody dict.\n",
            "Parameters\n",
            "serialized_args (str) â€“ \n",
            "Return type\n",
            "dict\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_api_operation(operation, llm, requests=None, verbose=False, return_intermediate_steps=False, raw_response=False, callbacks=None, **kwargs)[source]ïƒ\n",
            "Create an OpenAPIEndpointChain from an operation and a spec.\n",
            "Parameters\n",
            "operation (langchain.tools.openapi.utils.api_models.APIOperation) â€“ \n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "requests (Optional[langchain.requests.Requests]) â€“ \n",
            "verbose (bool) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "raw_response (bool) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.api.openapi.chain.OpenAPIEndpointChain\n",
            "classmethod from_url_and_method(spec_url, path, method, llm, requests=None, return_intermediate_steps=False, **kwargs)[source]ïƒ\n",
            "Create an OpenAPIEndpoint from a spec at the specified url.\n",
            "Parameters\n",
            "spec_url (str) â€“ \n",
            "path (str) â€“ \n",
            "method (str) â€“ \n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "requests (Optional[langchain.requests.Requests]) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.api.openapi.chain.OpenAPIEndpointChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.PALChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\\nÂ Â Â  money_initial = 23\\nÂ Â Â  bagels = 5\\nÂ Â Â  bagel_cost = 3\\nÂ Â Â  money_spent = bagels * bagel_cost\\nÂ Â Â  money_left = money_initial - money_spent\\nÂ Â Â  result = money_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\"\\nÂ Â Â  golf_balls_initial = 58\\nÂ Â Â  golf_balls_lost_tuesday = 23\\nÂ Â Â  golf_balls_lost_wednesday = 2\\nÂ Â Â  golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\nÂ Â Â  result = golf_balls_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\\nÂ Â Â  computers_initial = 9\\nÂ Â Â  computers_per_day = 5\\nÂ Â Â  num_days = 4Â  # 4 days between monday and thursday\\nÂ Â Â  computers_added = computers_per_day * num_days\\nÂ Â Â  computers_total = computers_initial + computers_added\\nÂ Â Â  result = computers_total\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\\nÂ Â Â  toys_initial = 5\\nÂ Â Â  mom_toys = 2\\nÂ Â Â  dad_toys = 2\\nÂ Â Â  total_received = mom_toys + dad_toys\\nÂ Â Â  total_toys = toys_initial + total_received\\nÂ Â Â  result = total_toys\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\\nÂ Â Â  jason_lollipops_initial = 20\\nÂ Â Â  jason_lollipops_after = 12\\nÂ Â Â  denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\nÂ Â Â  result = denny_lollipops\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\\nÂ Â Â  leah_chocolates = 32\\nÂ Â Â  sister_chocolates = 42\\nÂ Â Â  total_chocolates = leah_chocolates + sister_chocolates\\nÂ Â Â  chocolates_eaten = 35\\nÂ Â Â  chocolates_left = total_chocolates - chocolates_eaten\\nÂ Â Â  result = chocolates_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\\nÂ Â Â  cars_initial = 3\\nÂ Â Â  cars_arrived = 2\\nÂ Â Â  total_cars = cars_initial + cars_arrived\\nÂ Â Â  result = total_cars\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\\nÂ Â Â  trees_initial = 15\\nÂ Â Â  trees_after = 21\\nÂ Â Â  trees_added = trees_after - trees_initial\\nÂ Â Â  result = trees_added\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: {question}\\n\\n# solution in Python:\\n\\n\\n', template_format='f-string', validate_template=True), stop='\\n\\n', get_answer_expr='print(solution())', python_globals=None, python_locals=None, output_key='result', return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Implements Program-Aided Language Models.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "stop (str) â€“ \n",
            "get_answer_expr (str) â€“ \n",
            "python_globals (Optional[Dict[str, Any]]) â€“ \n",
            "python_locals (Optional[Dict[str, Any]]) â€“ \n",
            "output_key (str) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute get_answer_expr: str = 'print(solution())'ïƒ\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated]\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute prompt: BasePromptTemplate = PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\\nÂ Â Â  money_initial = 23\\nÂ Â Â  bagels = 5\\nÂ Â Â  bagel_cost = 3\\nÂ Â Â  money_spent = bagels * bagel_cost\\nÂ Â Â  money_left = money_initial - money_spent\\nÂ Â Â  result = money_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\"\"\"\\nÂ Â Â  golf_balls_initial = 58\\nÂ Â Â  golf_balls_lost_tuesday = 23\\nÂ Â Â  golf_balls_lost_wednesday = 2\\nÂ Â Â  golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\nÂ Â Â  result = golf_balls_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\\nÂ Â Â  computers_initial = 9\\nÂ Â Â  computers_per_day = 5\\nÂ Â Â  num_days = 4Â  # 4 days between monday and thursday\\nÂ Â Â  computers_added = computers_per_day * num_days\\nÂ Â Â  computers_total = computers_initial + computers_added\\nÂ Â Â  result = computers_total\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\\nÂ Â Â  toys_initial = 5\\nÂ Â Â  mom_toys = 2\\nÂ Â Â  dad_toys = 2\\nÂ Â Â  total_received = mom_toys + dad_toys\\nÂ Â Â  total_toys = toys_initial + total_received\\nÂ Â Â  result = total_toys\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\\nÂ Â Â  jason_lollipops_initial = 20\\nÂ Â Â  jason_lollipops_after = 12\\nÂ Â Â  denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\nÂ Â Â  result = denny_lollipops\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\\nÂ Â Â  leah_chocolates = 32\\nÂ Â Â  sister_chocolates = 42\\nÂ Â Â  total_chocolates = leah_chocolates + sister_chocolates\\nÂ Â Â  chocolates_eaten = 35\\nÂ Â Â  chocolates_left = total_chocolates - chocolates_eaten\\nÂ Â Â  result = chocolates_left\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\\nÂ Â Â  cars_initial = 3\\nÂ Â Â  cars_arrived = 2\\nÂ Â Â  total_cars = cars_initial + cars_arrived\\nÂ Â Â  result = total_cars\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\nÂ Â Â  \"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\"\"\"\\nÂ Â Â  trees_initial = 15\\nÂ Â Â  trees_after = 21\\nÂ Â Â  trees_added = trees_after - trees_initial\\nÂ Â Â  result = trees_added\\nÂ Â Â  return result\\n\\n\\n\\n\\n\\nQ: {question}\\n\\n# solution in Python:\\n\\n\\n', template_format='f-string', validate_template=True)ïƒ\n",
            "[Deprecated]\n",
            "attribute python_globals: Optional[Dict[str, Any]] = Noneïƒ\n",
            "attribute python_locals: Optional[Dict[str, Any]] = Noneïƒ\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "attribute stop: str = '\\n\\n'ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_colored_object_prompt(llm, **kwargs)[source]ïƒ\n",
            "Load PAL from colored object prompt.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.pal.base.PALChain\n",
            "classmethod from_math_prompt(llm, **kwargs)[source]ïƒ\n",
            "Load PAL from math prompt.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.pal.base.PALChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.QAGenerationChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, text_splitter=<langchain.text_splitter.RecursiveCharacterTextSplitter object>, input_key='text', output_key='questions', k=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "text_splitter (langchain.text_splitter.TextSplitter) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "k (Optional[int]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute input_key: str = 'text'ïƒ\n",
            "attribute k: Optional[int] = Noneïƒ\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute output_key: str = 'questions'ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute text_splitter: TextSplitter = <langchain.text_splitter.RecursiveCharacterTextSplitter object>ïƒ\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, prompt=None, **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_generation.base.QAGenerationChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property input_keys: List[str]ïƒ\n",
            "Input keys this chain expects.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Output keys this chain expects.\n",
            "class langchain.chains.QAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key='question', input_docs_key='docs', answer_key='answer', sources_answer_key='sources', return_source_documents=False)[source]ïƒ\n",
            "Bases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "Question answering with sources over documents.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "question_key (str) â€“ \n",
            "input_docs_key (str) â€“ \n",
            "answer_key (str) â€“ \n",
            "sources_answer_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine documents.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "Return the source documents.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_chain_type(llm, chain_type='stuff', chain_type_kwargs=None, **kwargs)ïƒ\n",
            "Load chain from chain type.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain_type (str) â€“ \n",
            "chain_type_kwargs (Optional[dict]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "classmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), question_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), combine_prompt=PromptTemplate(input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts inÂ  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for anÂ  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)Â  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuationÂ  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of anyÂ  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (asÂ  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.Â  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we wonâ€™t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.Â  \\n\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.Â  \\n\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 yearsÂ  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.Â  \\n\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctionsÂ  is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.Â  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.Â  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.Â  \\n\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:', template_format='f-string', validate_template=True), **kwargs)ïƒ\n",
            "Construct the chain from an LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "document_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "combine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.RetrievalQA(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, input_key='query', output_key='result', return_source_documents=False, retriever)[source]ïƒ\n",
            "Bases: langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "Chain for question-answering against an index.\n",
            "Example\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.faiss import FAISS\n",
            "from langchain.vectorstores.base import VectorStoreRetriever\n",
            "retriever = VectorStoreRetriever(vectorstore=FAISS(...))\n",
            "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "retriever (langchain.schema.BaseRetriever) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine the documents.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute retriever: BaseRetriever [Required]ïƒ\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "Return the source documents.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_chain_type(llm, chain_type='stuff', chain_type_kwargs=None, **kwargs)ïƒ\n",
            "Load chain from chain type.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain_type (str) â€“ \n",
            "chain_type_kwargs (Optional[dict]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "classmethod from_llm(llm, prompt=None, **kwargs)ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.RetrievalQAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key='question', input_docs_key='docs', answer_key='answer', sources_answer_key='sources', return_source_documents=False, retriever, reduce_k_below_max_tokens=False, max_tokens_limit=3375)[source]ïƒ\n",
            "Bases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "Question-answering with sources over an index.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "question_key (str) â€“ \n",
            "input_docs_key (str) â€“ \n",
            "answer_key (str) â€“ \n",
            "sources_answer_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "retriever (langchain.schema.BaseRetriever) â€“ \n",
            "reduce_k_below_max_tokens (bool) â€“ \n",
            "max_tokens_limit (int) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine documents.\n",
            "attribute max_tokens_limit: int = 3375ïƒ\n",
            "Restrict the docs to return from store based on tokens,\n",
            "enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute reduce_k_below_max_tokens: bool = Falseïƒ\n",
            "Reduce the number of results to return from store based on tokens limit\n",
            "attribute retriever: langchain.schema.BaseRetriever [Required]ïƒ\n",
            "Index to connect to.\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "Return the source documents.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_chain_type(llm, chain_type='stuff', chain_type_kwargs=None, **kwargs)ïƒ\n",
            "Load chain from chain type.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain_type (str) â€“ \n",
            "chain_type_kwargs (Optional[dict]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "classmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), question_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), combine_prompt=PromptTemplate(input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts inÂ  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for anÂ  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)Â  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuationÂ  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of anyÂ  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (asÂ  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.Â  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we wonâ€™t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.Â  \\n\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.Â  \\n\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 yearsÂ  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.Â  \\n\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctionsÂ  is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.Â  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.Â  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.Â  \\n\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:', template_format='f-string', validate_template=True), **kwargs)ïƒ\n",
            "Construct the chain from an LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "document_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "combine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.RouterChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain, abc.ABC\n",
            "Chain that outputs the name of a destination chain and the inputs to it.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async aroute(inputs, callbacks=None)[source]ïƒ\n",
            "Parameters\n",
            "inputs (Dict[str, Any]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "langchain.chains.router.base.Route\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "route(inputs, callbacks=None)[source]ïƒ\n",
            "Parameters\n",
            "inputs (Dict[str, Any]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "langchain.chains.router.base.Route\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "abstract property input_keys: List[str]ïƒ\n",
            "Input keys this chain expects.\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "property output_keys: List[str]ïƒ\n",
            "Output keys this chain expects.\n",
            "class langchain.chains.SQLDatabaseChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, llm_chain, llm=None, database, prompt=None, top_k=5, input_key='query', output_key='result', return_intermediate_steps=False, return_direct=False, use_query_checker=False, query_checker_prompt=None)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for interacting with SQL Database.\n",
            "Example\n",
            "from langchain import SQLDatabaseChain, OpenAI, SQLDatabase\n",
            "db = SQLDatabase(...)\n",
            "db_chain = SQLDatabaseChain.from_llm(OpenAI(), db)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "database (langchain.sql_database.SQLDatabase) â€“ \n",
            "prompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \n",
            "top_k (int) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "return_direct (bool) â€“ \n",
            "use_query_checker (bool) â€“ \n",
            "query_checker_prompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute database: SQLDatabase [Required]ïƒ\n",
            "SQL Database to connect to.\n",
            "attribute llm: Optional[BaseLanguageModel] = Noneïƒ\n",
            "[Deprecated] LLM wrapper to use.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute prompt: Optional[BasePromptTemplate] = Noneïƒ\n",
            "[Deprecated] Prompt to use to translate natural language to SQL.\n",
            "attribute query_checker_prompt: Optional[BasePromptTemplate] = Noneïƒ\n",
            "The prompt template that should be used by the query checker\n",
            "attribute return_direct: bool = Falseïƒ\n",
            "Whether or not to return the result of querying the SQL table directly.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "Whether or not to return the intermediate steps along with the final answer.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute top_k: int = 5ïƒ\n",
            "Number of results to return from the query\n",
            "attribute use_query_checker: bool = Falseïƒ\n",
            "Whether or not the query checker tool should be used to attempt\n",
            "to fix the initial SQL from the LLM.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, db, prompt=None, **kwargs)[source]ïƒ\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "db (langchain.sql_database.SQLDatabase) â€“ \n",
            "prompt (Optional[langchain.prompts.base.BasePromptTemplate]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.sql_database.base.SQLDatabaseChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.SQLDatabaseSequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, decider_chain, sql_chain, input_key='query', output_key='result', return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for querying SQL database that is a sequential chain.\n",
            "The chain is as follows:\n",
            "1. Based on the query, determine which tables to use.\n",
            "2. Based on those tables, call the normal SQL database chain.\n",
            "This is useful in cases where the number of tables in the database is large.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "decider_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "sql_chain (langchain.chains.sql_database.base.SQLDatabaseChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute decider_chain: LLMChain [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "attribute sql_chain: SQLDatabaseChain [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_llm(llm, database, query_prompt=PromptTemplate(input_variables=['input', 'table_info', 'dialect', 'top_k'], output_parser=None, partial_variables={}, template='Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables:\\n{table_info}\\n\\nQuestion: {input}', template_format='f-string', validate_template=True), decider_prompt=PromptTemplate(input_variables=['query', 'table_names'], output_parser=CommaSeparatedListOutputParser(), partial_variables={}, template='Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\\n\\nQuestion: {query}\\n\\nTable Names: {table_names}\\n\\nRelevant Table Names:', template_format='f-string', validate_template=True), **kwargs)[source]ïƒ\n",
            "Load the necessary chains.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "database (langchain.sql_database.SQLDatabase) â€“ \n",
            "query_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "decider_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.sql_database.base.SQLDatabaseSequentialChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.SequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chains, input_variables, output_variables, return_all=False)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain where the outputs of one chain feed directly into next.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "chains (List[langchain.chains.base.Chain]) â€“ \n",
            "input_variables (List[str]) â€“ \n",
            "output_variables (List[str]) â€“ \n",
            "return_all (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute chains: List[langchain.chains.base.Chain] [Required]ïƒ\n",
            "attribute input_variables: List[str] [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_all: bool = Falseïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.SimpleSequentialChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, chains, strip_outputs=False, input_key='input', output_key='output')[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Simple chain where the outputs of one step feed directly into next.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "chains (List[langchain.chains.base.Chain]) â€“ \n",
            "strip_outputs (bool) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute chains: List[langchain.chains.base.Chain] [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute strip_outputs: bool = Falseïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.TransformChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_variables, output_variables, transform)[source]ïƒ\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain transform chain output.\n",
            "Example\n",
            "from langchain import TransformChain\n",
            "transform_chain = TransformChain(input_variables=[\"text\"],\n",
            " output_variables[\"entities\"], transform=func())\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_variables (List[str]) â€“ \n",
            "output_variables (List[str]) â€“ \n",
            "transform (Callable[[Dict[str, str]], Dict[str, str]]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute input_variables: List[str] [Required]ïƒ\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute output_variables: List[str] [Required]ïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute transform: Callable[[Dict[str, str]], Dict[str, str]] [Required]ïƒ\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.VectorDBQA(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, input_key='query', output_key='result', return_source_documents=False, vectorstore, k=4, search_type='similarity', search_kwargs=None)[source]ïƒ\n",
            "Bases: langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "Chain for question-answering against a vector database.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "vectorstore (langchain.vectorstores.base.VectorStore) â€“ \n",
            "k (int) â€“ \n",
            "search_type (str) â€“ \n",
            "search_kwargs (Dict[str, Any]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine the documents.\n",
            "attribute k: int = 4ïƒ\n",
            "Number of documents to query for.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "Return the source documents.\n",
            "attribute search_kwargs: Dict[str, Any] [Optional]ïƒ\n",
            "Extra search args.\n",
            "attribute search_type: str = 'similarity'ïƒ\n",
            "Search type to use over vectorstore. similarity or mmr.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute vectorstore: VectorStore [Required]ïƒ\n",
            "Vector Database to connect to.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_chain_type(llm, chain_type='stuff', chain_type_kwargs=None, **kwargs)ïƒ\n",
            "Load chain from chain type.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain_type (str) â€“ \n",
            "chain_type_kwargs (Optional[dict]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "classmethod from_llm(llm, prompt=None, **kwargs)ïƒ\n",
            "Initialize from LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "prompt (Optional[langchain.prompts.prompt.PromptTemplate]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.retrieval_qa.base.BaseRetrievalQA\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.VectorDBQAWithSourcesChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, combine_documents_chain, question_key='question', input_docs_key='docs', answer_key='answer', sources_answer_key='sources', return_source_documents=False, vectorstore, k=4, reduce_k_below_max_tokens=False, max_tokens_limit=3375, search_kwargs=None)[source]ïƒ\n",
            "Bases: langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "Question-answering with sources over a vector database.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "combine_documents_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "question_key (str) â€“ \n",
            "input_docs_key (str) â€“ \n",
            "answer_key (str) â€“ \n",
            "sources_answer_key (str) â€“ \n",
            "return_source_documents (bool) â€“ \n",
            "vectorstore (langchain.vectorstores.base.VectorStore) â€“ \n",
            "k (int) â€“ \n",
            "reduce_k_below_max_tokens (bool) â€“ \n",
            "max_tokens_limit (int) â€“ \n",
            "search_kwargs (Dict[str, Any]) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute combine_documents_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine documents.\n",
            "attribute k: int = 4ïƒ\n",
            "Number of results to return from store\n",
            "attribute max_tokens_limit: int = 3375ïƒ\n",
            "Restrict the docs to return from store based on tokens,\n",
            "enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute reduce_k_below_max_tokens: bool = Falseïƒ\n",
            "Reduce the number of results to return from store based on tokens limit\n",
            "attribute return_source_documents: bool = Falseïƒ\n",
            "Return the source documents.\n",
            "attribute search_kwargs: Dict[str, Any] [Optional]ïƒ\n",
            "Extra search args.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute vectorstore: langchain.vectorstores.base.VectorStore [Required]ïƒ\n",
            "Vector Database to connect to.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "classmethod from_chain_type(llm, chain_type='stuff', chain_type_kwargs=None, **kwargs)ïƒ\n",
            "Load chain from chain type.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "chain_type (str) â€“ \n",
            "chain_type_kwargs (Optional[dict]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "classmethod from_llm(llm, document_prompt=PromptTemplate(input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), question_prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), combine_prompt=PromptTemplate(input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts inÂ  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for anÂ  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)Â  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuationÂ  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of anyÂ  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (asÂ  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.Â  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we wonâ€™t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLetâ€™s use this moment to reset. Letâ€™s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.Â  \\n\\nLetâ€™s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.Â  \\n\\nWe canâ€™t change how divided weâ€™ve been. But we can change how we move forwardâ€”on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans whoâ€™d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 yearsÂ  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.Â  \\n\\nTo all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd Iâ€™m taking robust action to make sure the pain of our sanctionsÂ  is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.Â  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.Â  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nItâ€™s based on DARPAâ€”the Defense Department project that led to the Internet, GPS, and so much more.Â  \\n\\nARPA-H will have a singular purposeâ€”to drive breakthroughs in cancer, Alzheimerâ€™s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americansâ€”tonight , we have gathered in a sacred spaceâ€”the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:', template_format='f-string', validate_template=True), **kwargs)ïƒ\n",
            "Construct the chain from an LLM.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ \n",
            "document_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "question_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "combine_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "langchain.chains.create_extraction_chain(schema, llm)[source]ïƒ\n",
            "Creates a chain that extracts information from a passage.\n",
            "Parameters\n",
            "schema (dict) â€“ The schema of the entities to extract.\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\n",
            "Returns\n",
            "Chain that can be used to extract information from a passage.\n",
            "Return type\n",
            "langchain.chains.base.Chain\n",
            "langchain.chains.create_extraction_chain_pydantic(pydantic_schema, llm)[source]ïƒ\n",
            "Creates a chain that extracts information from a passage using pydantic schema.\n",
            "Parameters\n",
            "pydantic_schema (Any) â€“ The pydantic schema of the entities to extract.\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\n",
            "Returns\n",
            "Chain that can be used to extract information from a passage.\n",
            "Return type\n",
            "langchain.chains.base.Chain\n",
            "langchain.chains.create_tagging_chain(schema, llm)[source]ïƒ\n",
            "Creates a chain that extracts information from a passage.\n",
            "Parameters\n",
            "schema (dict) â€“ The schema of the entities to extract.\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\n",
            "Returns\n",
            "Chain (LLMChain) that can be used to extract information from a passage.\n",
            "Return type\n",
            "langchain.chains.base.Chain\n",
            "langchain.chains.create_tagging_chain_pydantic(pydantic_schema, llm)[source]ïƒ\n",
            "Creates a chain that extracts information from a passage.\n",
            "Parameters\n",
            "pydantic_schema (Any) â€“ The pydantic schema of the entities to extract.\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ The language model to use.\n",
            "Returns\n",
            "Chain (LLMChain) that can be used to extract information from a passage.\n",
            "Return type\n",
            "langchain.chains.base.Chain\n",
            "langchain.chains.load_chain(path, **kwargs)[source]ïƒ\n",
            "Unified method for loading a chain from LangChainHub or local fs.\n",
            "Parameters\n",
            "path (Union[str, pathlib.Path]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "langchain.chains.base.Chain\n",
            "langchain.chains.create_citation_fuzzy_match_chain(llm)[source]ïƒ\n",
            "Create a citation fuzzy match chain.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\n",
            "Returns\n",
            "Chain (LLMChain) that can be used to answer questions with citations.\n",
            "Return type\n",
            "langchain.chains.llm.LLMChain\n",
            "langchain.chains.create_qa_with_structure_chain(llm, schema, output_parser='base', prompt=None)[source]ïƒ\n",
            "Create a question answering chain that returns an answer with sources.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\n",
            "schema (Union[dict, Type[pydantic.main.BaseModel]]) â€“ Pydantic schema to use for the output.\n",
            "output_parser (str) â€“ Output parser to use. Should be one of pydantic or base.\n",
            "Default to base.\n",
            "prompt (Optional[Union[langchain.prompts.prompt.PromptTemplate, langchain.prompts.chat.ChatPromptTemplate]]) â€“ Optional prompt to use for the chain.\n",
            "Return type\n",
            "langchain.chains.llm.LLMChain\n",
            "Returns:\n",
            "langchain.chains.create_qa_with_sources_chain(llm, **kwargs)[source]ïƒ\n",
            "Create a question answering chain that returns an answer with sources.\n",
            "Parameters\n",
            "llm (langchain.base_language.BaseLanguageModel) â€“ Language model to use for the chain.\n",
            "**kwargs â€“ Keyword arguments to pass to create_qa_with_structure_chain.\n",
            "kwargs (Any) â€“ \n",
            "Returns\n",
            "Chain (LLMChain) that can be used to answer questions with citations.\n",
            "Return type\n",
            "langchain.chains.llm.LLMChain\n",
            "class langchain.chains.StuffDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key='input_documents', output_key='output_text', llm_chain, document_prompt=None, document_variable_name, document_separator='\\n\\n')[source]ïƒ\n",
            "Bases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
            "Chain that combines documents by stuffing into context.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "document_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "document_variable_name (str) â€“ \n",
            "document_separator (str) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute document_prompt: langchain.prompts.base.BasePromptTemplate [Optional]ïƒ\n",
            "Prompt to use to format each document.\n",
            "attribute document_separator: str = '\\n\\n'ïƒ\n",
            "The string with which to join the formatted documents\n",
            "attribute document_variable_name: str [Required]ïƒ\n",
            "The variable name in the llm_chain to put the documents in.\n",
            "If only one variable in the llm_chain, this need not be provided.\n",
            "attribute llm_chain: langchain.chains.llm.LLMChain [Required]ïƒ\n",
            "LLM wrapper to use after formatting documents.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async acombine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Stuff all documents into one prompt and pass to LLM.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "combine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Stuff all documents into one prompt and pass to LLM.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prompt_length(docs, **kwargs)[source]ïƒ\n",
            "Get the prompt length by formatting the prompt.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Optional[int]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MapRerankDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key='input_documents', output_key='output_text', llm_chain, document_variable_name, rank_key, answer_key, metadata_keys=None, return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
            "Combining documents by mapping a chain over them, then reranking results.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "document_variable_name (str) â€“ \n",
            "rank_key (str) â€“ \n",
            "answer_key (str) â€“ \n",
            "metadata_keys (Optional[List[str]]) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute answer_key: str [Required]ïƒ\n",
            "Key in output of llm_chain to return as answer.\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute document_variable_name: str [Required]ïƒ\n",
            "The variable name in the llm_chain to put the documents in.\n",
            "If only one variable in the llm_chain, this need not be provided.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "Chain to apply to each document individually.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute metadata_keys: Optional[List[str]] = Noneïƒ\n",
            "attribute rank_key: str [Required]ïƒ\n",
            "Key in output of llm_chain to rank on.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async acombine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine documents in a map rerank manner.\n",
            "Combine by mapping first chain over all documents, then reranking the results.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "combine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine documents in a map rerank manner.\n",
            "Combine by mapping first chain over all documents, then reranking the results.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prompt_length(docs, **kwargs)ïƒ\n",
            "Return the prompt length given the documents passed in.\n",
            "Returns None if the method does not depend on the prompt length.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Optional[int]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.MapReduceDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key='input_documents', output_key='output_text', llm_chain, combine_document_chain, collapse_document_chain=None, document_variable_name, return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
            "Combining documents by mapping a chain over them, then combining results.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "combine_document_chain (langchain.chains.combine_documents.base.BaseCombineDocumentsChain) â€“ \n",
            "collapse_document_chain (Optional[langchain.chains.combine_documents.base.BaseCombineDocumentsChain]) â€“ \n",
            "document_variable_name (str) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute collapse_document_chain: Optional[BaseCombineDocumentsChain] = Noneïƒ\n",
            "Chain to use to collapse intermediary results if needed.\n",
            "If None, will use the combine_document_chain.\n",
            "attribute combine_document_chain: BaseCombineDocumentsChain [Required]ïƒ\n",
            "Chain to use to combine results of applying llm_chain to documents.\n",
            "attribute document_variable_name: str [Required]ïƒ\n",
            "The variable name in the llm_chain to put the documents in.\n",
            "If only one variable in the llm_chain, this need not be provided.\n",
            "attribute llm_chain: LLMChain [Required]ïƒ\n",
            "Chain to apply to each document individually.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "Return the results of the map steps in the output.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async acombine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine documents in a map reduce manner.\n",
            "Combine by mapping first chain over all documents, then reducing the results.\n",
            "This reducing can be done recursively if needed (if there are many documents).\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "combine_docs(docs, token_max=3000, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine documents in a map reduce manner.\n",
            "Combine by mapping first chain over all documents, then reducing the results.\n",
            "This reducing can be done recursively if needed (if there are many documents).\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "token_max (int) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prompt_length(docs, **kwargs)ïƒ\n",
            "Return the prompt length given the documents passed in.\n",
            "Returns None if the method does not depend on the prompt length.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Optional[int]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n",
            "class langchain.chains.RefineDocumentsChain(*, memory=None, callbacks=None, callback_manager=None, verbose=None, tags=None, input_key='input_documents', output_key='output_text', initial_llm_chain, refine_llm_chain, document_variable_name, initial_response_name, document_prompt=None, return_intermediate_steps=False)[source]ïƒ\n",
            "Bases: langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
            "Combine documents by doing a first pass and then refining on more documents.\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "initial_llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "refine_llm_chain (langchain.chains.llm.LLMChain) â€“ \n",
            "document_variable_name (str) â€“ \n",
            "initial_response_name (str) â€“ \n",
            "document_prompt (langchain.prompts.base.BasePromptTemplate) â€“ \n",
            "return_intermediate_steps (bool) â€“ \n",
            "Return type\n",
            "None\n",
            "attribute callback_manager: Optional[BaseCallbackManager] = Noneïƒ\n",
            "Deprecated, use callbacks instead.\n",
            "attribute callbacks: Callbacks = Noneïƒ\n",
            "Optional list of callback handlers (or callback manager). Defaults to None.\n",
            "Callback handlers are called throughout the lifecycle of a call to a chain,\n",
            "starting with on_chain_start, ending with on_chain_end or on_chain_error.\n",
            "Each custom chain can optionally call additional callback methods, see Callback docs\n",
            "for full details.\n",
            "attribute document_prompt: BasePromptTemplate [Optional]ïƒ\n",
            "Prompt to use to format each document.\n",
            "attribute document_variable_name: str [Required]ïƒ\n",
            "The variable name in the initial_llm_chain to put the documents in.\n",
            "If only one variable in the initial_llm_chain, this need not be provided.\n",
            "attribute initial_llm_chain: LLMChain [Required]ïƒ\n",
            "LLM chain to use on initial document.\n",
            "attribute initial_response_name: str [Required]ïƒ\n",
            "The variable name to format the initial response in when refining.\n",
            "attribute memory: Optional[BaseMemory] = Noneïƒ\n",
            "Optional memory object. Defaults to None.\n",
            "Memory is a class that gets called at the start\n",
            "and at the end of every chain. At the start, memory loads variables and passes\n",
            "them along in the chain. At the end, it saves any returned variables.\n",
            "There are many different types of memory - please see memory docs\n",
            "for the full catalog.\n",
            "attribute refine_llm_chain: LLMChain [Required]ïƒ\n",
            "LLM chain to use when refining.\n",
            "attribute return_intermediate_steps: bool = Falseïƒ\n",
            "Return the results of the refine steps in the output.\n",
            "attribute tags: Optional[List[str]] = Noneïƒ\n",
            "Optional list of tags associated with the chain. Defaults to None\n",
            "These tags will be associated with each call to this chain,\n",
            "and passed as arguments to the handlers defined in callbacks.\n",
            "You can use these to eg identify a specific instance of a chain with its use case.\n",
            "attribute verbose: bool [Optional]ïƒ\n",
            "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
            "will be printed to the console. Defaults to langchain.verbose value.\n",
            "async acall(inputs, return_only_outputs=False, callbacks=None, *, tags=None, include_run_info=False)ïƒ\n",
            "Run the logic of this chain and add to output if desired.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ Dictionary of inputs, or single input if chain expects\n",
            "only one param.\n",
            "return_only_outputs (bool) â€“ boolean for whether to return only outputs in the\n",
            "response. If True, only new keys generated by this chain will be\n",
            "returned. If False, both input keys and new keys generated by this\n",
            "chain will be returned. Defaults to False.\n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ Callbacks to use for this chain run. If not provided, will\n",
            "use the callbacks provided to the chain.\n",
            "include_run_info (bool) â€“ Whether to include run info in the response. Defaults\n",
            "to False.\n",
            "tags (Optional[List[str]]) â€“ \n",
            "Return type\n",
            "Dict[str, Any]\n",
            "async acombine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine by mapping first chain over all, then stuffing into final chain.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "apply(input_list, callbacks=None)ïƒ\n",
            "Call the chain on all inputs in the list.\n",
            "Parameters\n",
            "input_list (List[Dict[str, Any]]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "Return type\n",
            "List[Dict[str, str]]\n",
            "async arun(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "combine_docs(docs, callbacks=None, **kwargs)[source]ïƒ\n",
            "Combine by mapping first chain over all, then stuffing into final chain.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Tuple[str, dict]\n",
            "dict(**kwargs)ïƒ\n",
            "Return dictionary representation of chain.\n",
            "Parameters\n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Dict\n",
            "prep_inputs(inputs)ïƒ\n",
            "Validate and prep inputs.\n",
            "Parameters\n",
            "inputs (Union[Dict[str, Any], Any]) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prep_outputs(inputs, outputs, return_only_outputs=False)ïƒ\n",
            "Validate and prep outputs.\n",
            "Parameters\n",
            "inputs (Dict[str, str]) â€“ \n",
            "outputs (Dict[str, str]) â€“ \n",
            "return_only_outputs (bool) â€“ \n",
            "Return type\n",
            "Dict[str, str]\n",
            "prompt_length(docs, **kwargs)ïƒ\n",
            "Return the prompt length given the documents passed in.\n",
            "Returns None if the method does not depend on the prompt length.\n",
            "Parameters\n",
            "docs (List[langchain.schema.Document]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "Optional[int]\n",
            "run(*args, callbacks=None, tags=None, **kwargs)ïƒ\n",
            "Run the chain as text in, text out or multiple variables, text out.\n",
            "Parameters\n",
            "args (Any) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "kwargs (Any) â€“ \n",
            "Return type\n",
            "str\n",
            "save(file_path)ïƒ\n",
            "Save the chain.\n",
            "Parameters\n",
            "file_path (Union[pathlib.Path, str]) â€“ Path to file to save the chain to.\n",
            "Return type\n",
            "None\n",
            "Example:\n",
            ".. code-block:: python\n",
            "chain.save(file_path=â€path/chain.yamlâ€)\n",
            "to_json()ïƒ\n",
            "Return type\n",
            "Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
            "to_json_not_implemented()ïƒ\n",
            "Return type\n",
            "langchain.load.serializable.SerializedNotImplemented\n",
            "property lc_attributes: Dictïƒ\n",
            "Return a list of attribute names that should be included in the\n",
            "serialized kwargs. These attributes must be accepted by the\n",
            "constructor.\n",
            "property lc_namespace: List[str]ïƒ\n",
            "Return the namespace of the langchain object.\n",
            "eg. [â€œlangchainâ€, â€œllmsâ€, â€œopenaiâ€]\n",
            "property lc_secrets: Dict[str, str]ïƒ\n",
            "Return a map of constructor argument names to secret ids.\n",
            "eg. {â€œopenai_api_keyâ€: â€œOPENAI_API_KEYâ€}\n",
            "property lc_serializable: boolïƒ\n",
            "Return whether or not the class is serializable.\n"
          ]
        }
      ],
      "source": [
        "print(docs[20].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIkny_6xiZJ",
        "outputId": "2709cf4a-7b68-4ebb-aabc-fe06211c9967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source code for langchain.vectorstores.rocksetdb\n",
            "\"\"\"Wrapper around Rockset vector database.\"\"\"\n",
            "from __future__ import annotations\n",
            "import logging\n",
            "from enum import Enum\n",
            "from typing import Any, Iterable, List, Optional, Tuple\n",
            "from langchain.docstore.document import Document\n",
            "from langchain.embeddings.base import Embeddings\n",
            "from langchain.vectorstores.base import VectorStore\n",
            "logger = logging.getLogger(__name__)\n",
            "[docs]class Rockset(VectorStore):\n",
            "    \"\"\"Wrapper arpund Rockset vector database.\n",
            "    To use, you should have the `rockset` python package installed. Note that to use\n",
            "    this, the collection being used must already exist in your Rockset instance.\n",
            "    You must also ensure you use a Rockset ingest transformation to apply\n",
            "    `VECTOR_ENFORCE` on the column being used to store `embedding_key` in the\n",
            "    collection.\n",
            "    See: https://rockset.com/blog/introducing-vector-search-on-rockset/ for more details\n",
            "    Everything below assumes `commons` Rockset workspace.\n",
            "    TODO: Add support for workspace args.\n",
            "    Example:\n",
            "        .. code-block:: python\n",
            "            from langchain.vectorstores import Rockset\n",
            "            from langchain.embeddings.openai import OpenAIEmbeddings\n",
            "            import rockset\n",
            "            # Make sure you use the right host (region) for your Rockset instance\n",
            "            # and APIKEY has both read-write access to your collection.\n",
            "            rs = rockset.RocksetClient(host=rockset.Regions.use1a1, api_key=\"***\")\n",
            "            collection_name = \"langchain_demo\"\n",
            "            embeddings = OpenAIEmbeddings()\n",
            "            vectorstore = Rockset(rs, collection_name, embeddings,\n",
            "                \"description\", \"description_embedding\")\n",
            "    \"\"\"\n",
            "    def __init__(\n",
            "        self,\n",
            "        client: Any,\n",
            "        embeddings: Embeddings,\n",
            "        collection_name: str,\n",
            "        text_key: str,\n",
            "        embedding_key: str,\n",
            "    ):\n",
            "        \"\"\"Initialize with Rockset client.\n",
            "        Args:\n",
            "            client: Rockset client object\n",
            "            collection: Rockset collection to insert docs / query\n",
            "            embeddings: Langchain Embeddings object to use to generate\n",
            "                        embedding for given text.\n",
            "            text_key: column in Rockset collection to use to store the text\n",
            "            embedding_key: column in Rockset collection to use to store the embedding.\n",
            "                           Note: We must apply `VECTOR_ENFORCE()` on this column via\n",
            "                           Rockset ingest transformation.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            from rockset import RocksetClient\n",
            "        except ImportError:\n",
            "            raise ImportError(\n",
            "                \"Could not import rockset client python package. \"\n",
            "                \"Please install it with `pip install rockset`.\"\n",
            "            )\n",
            "        if not isinstance(client, RocksetClient):\n",
            "            raise ValueError(\n",
            "                f\"client should be an instance of rockset.RocksetClient, \"\n",
            "                f\"got {type(client)}\"\n",
            "            )\n",
            "        # TODO: check that `collection_name` exists in rockset. Create if not.\n",
            "        self._client = client\n",
            "        self._collection_name = collection_name\n",
            "        self._embeddings = embeddings\n",
            "        self._text_key = text_key\n",
            "        self._embedding_key = embedding_key\n",
            "[docs]    def add_texts(\n",
            "        self,\n",
            "        texts: Iterable[str],\n",
            "        metadatas: Optional[List[dict]] = None,\n",
            "        ids: Optional[List[str]] = None,\n",
            "        batch_size: int = 32,\n",
            "        **kwargs: Any,\n",
            "    ) -> List[str]:\n",
            "        \"\"\"Run more texts through the embeddings and add to the vectorstore\n",
            "                Args:\n",
            "            texts: Iterable of strings to add to the vectorstore.\n",
            "            metadatas: Optional list of metadatas associated with the texts.\n",
            "            ids: Optional list of ids to associate with the texts.\n",
            "            batch_size: Send documents in batches to rockset.\n",
            "        Returns:\n",
            "            List of ids from adding the texts into the vectorstore.\n",
            "        \"\"\"\n",
            "        batch: list[dict] = []\n",
            "        stored_ids = []\n",
            "        for i, text in enumerate(texts):\n",
            "            if len(batch) == batch_size:\n",
            "                stored_ids += self._write_documents_to_rockset(batch)\n",
            "                batch = []\n",
            "            doc = {}\n",
            "            if metadatas and len(metadatas) > i:\n",
            "                doc = metadatas[i]\n",
            "            if ids and len(ids) > i:\n",
            "                doc[\"_id\"] = ids[i]\n",
            "            doc[self._text_key] = text\n",
            "            doc[self._embedding_key] = self._embeddings.embed_query(text)\n",
            "            batch.append(doc)\n",
            "        if len(batch) > 0:\n",
            "            stored_ids += self._write_documents_to_rockset(batch)\n",
            "            batch = []\n",
            "        return stored_ids\n",
            "[docs]    @classmethod\n",
            "    def from_texts(\n",
            "        cls,\n",
            "        texts: List[str],\n",
            "        embedding: Embeddings,\n",
            "        metadatas: Optional[List[dict]] = None,\n",
            "        client: Any = None,\n",
            "        collection_name: str = \"\",\n",
            "        text_key: str = \"\",\n",
            "        embedding_key: str = \"\",\n",
            "        ids: Optional[List[str]] = None,\n",
            "        batch_size: int = 32,\n",
            "        **kwargs: Any,\n",
            "    ) -> Rockset:\n",
            "        \"\"\"Create Rockset wrapper with existing texts.\n",
            "        This is intended as a quicker way to get started.\n",
            "        \"\"\"\n",
            "        # Sanitize imputs\n",
            "        assert client is not None, \"Rockset Client cannot be None\"\n",
            "        assert collection_name, \"Collection name cannot be empty\"\n",
            "        assert text_key, \"Text key name cannot be empty\"\n",
            "        assert embedding_key, \"Embedding key cannot be empty\"\n",
            "        rockset = cls(client, embedding, collection_name, text_key, embedding_key)\n",
            "        rockset.add_texts(texts, metadatas, ids, batch_size)\n",
            "        return rockset\n",
            "    # Rockset supports these vector distance functions.\n",
            "[docs]    class DistanceFunction(Enum):\n",
            "        COSINE_SIM = \"COSINE_SIM\"\n",
            "        EUCLIDEAN_DIST = \"EUCLIDEAN_DIST\"\n",
            "        DOT_PRODUCT = \"DOT_PRODUCT\"\n",
            "        # how to sort results for \"similarity\"\n",
            "[docs]        def order_by(self) -> str:\n",
            "            if self.value == \"EUCLIDEAN_DIST\":\n",
            "                return \"ASC\"\n",
            "            return \"DESC\"\n",
            "[docs]    def similarity_search_with_relevance_scores(\n",
            "        self,\n",
            "        query: str,\n",
            "        k: int = 4,\n",
            "        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,\n",
            "        where_str: Optional[str] = None,\n",
            "        **kwargs: Any,\n",
            "    ) -> List[Tuple[Document, float]]:\n",
            "        \"\"\"Perform a similarity search with Rockset\n",
            "        Args:\n",
            "            query (str): Text to look up documents similar to.\n",
            "            distance_func (DistanceFunction): how to compute distance between two\n",
            "                vectors in Rockset.\n",
            "            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n",
            "            where_str (Optional[str], optional): Metadata filters supplied as a\n",
            "                SQL `where` condition string. Defaults to None.\n",
            "                eg. \"price<=70.0 AND brand='Nintendo'\"\n",
            "            NOTE: Please do not let end-user to fill this and always be aware\n",
            "                  of SQL injection.\n",
            "        Returns:\n",
            "            List[Tuple[Document, float]]: List of documents with their relevance score\n",
            "        \"\"\"\n",
            "        return self.similarity_search_by_vector_with_relevance_scores(\n",
            "            self._embeddings.embed_query(query),\n",
            "            k,\n",
            "            distance_func,\n",
            "            where_str,\n",
            "            **kwargs,\n",
            "        )\n",
            "[docs]    def similarity_search(\n",
            "        self,\n",
            "        query: str,\n",
            "        k: int = 4,\n",
            "        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,\n",
            "        where_str: Optional[str] = None,\n",
            "        **kwargs: Any,\n",
            "    ) -> List[Document]:\n",
            "        \"\"\"Same as `similarity_search_with_relevance_scores` but\n",
            "        doesn't return the scores.\n",
            "        \"\"\"\n",
            "        return self.similarity_search_by_vector(\n",
            "            self._embeddings.embed_query(query),\n",
            "            k,\n",
            "            distance_func,\n",
            "            where_str,\n",
            "            **kwargs,\n",
            "        )\n",
            "[docs]    def similarity_search_by_vector(\n",
            "        self,\n",
            "        embedding: List[float],\n",
            "        k: int = 4,\n",
            "        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,\n",
            "        where_str: Optional[str] = None,\n",
            "        **kwargs: Any,\n",
            "    ) -> List[Document]:\n",
            "        \"\"\"Accepts a query_embedding (vector), and returns documents with\n",
            "        similar embeddings.\"\"\"\n",
            "        docs_and_scores = self.similarity_search_by_vector_with_relevance_scores(\n",
            "            embedding, k, distance_func, where_str, **kwargs\n",
            "        )\n",
            "        return [doc for doc, _ in docs_and_scores]\n",
            "[docs]    def similarity_search_by_vector_with_relevance_scores(\n",
            "        self,\n",
            "        embedding: List[float],\n",
            "        k: int = 4,\n",
            "        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,\n",
            "        where_str: Optional[str] = None,\n",
            "        **kwargs: Any,\n",
            "    ) -> List[Tuple[Document, float]]:\n",
            "        \"\"\"Accepts a query_embedding (vector), and returns documents with\n",
            "        similar embeddings along with their relevance scores.\"\"\"\n",
            "        q_str = self._build_query_sql(embedding, distance_func, k, where_str)\n",
            "        try:\n",
            "            query_response = self._client.Queries.query(sql={\"query\": q_str})\n",
            "        except Exception as e:\n",
            "            logger.error(\"Exception when querying Rockset: %s\\n\", e)\n",
            "            return []\n",
            "        finalResult: list[Tuple[Document, float]] = []\n",
            "        for document in query_response.results:\n",
            "            metadata = {}\n",
            "            assert isinstance(\n",
            "                document, dict\n",
            "            ), \"document should be of type `dict[str,Any]`. But found: `{}`\".format(\n",
            "                type(document)\n",
            "            )\n",
            "            for k, v in document.items():\n",
            "                if k == self._text_key:\n",
            "                    assert isinstance(\n",
            "                        v, str\n",
            "                    ), \"page content stored in column `{}` must be of type `str`. \\\n",
            "                        But found: `{}`\".format(\n",
            "                        self._text_key, type(v)\n",
            "                    )\n",
            "                    page_content = v\n",
            "                elif k == \"dist\":\n",
            "                    assert isinstance(\n",
            "                        v, float\n",
            "                    ), \"Computed distance between vectors must of type `float`. \\\n",
            "                        But found {}\".format(\n",
            "                        type(v)\n",
            "                    )\n",
            "                    score = v\n",
            "                elif k not in [\"_id\", \"_event_time\", \"_meta\"]:\n",
            "                    # These columns are populated by Rockset when documents are\n",
            "                    # inserted. No need to return them in metadata dict.\n",
            "                    metadata[k] = v\n",
            "            finalResult.append(\n",
            "                (Document(page_content=page_content, metadata=metadata), score)\n",
            "            )\n",
            "        return finalResult\n",
            "    # Helper functions\n",
            "    def _build_query_sql(\n",
            "        self,\n",
            "        query_embedding: List[float],\n",
            "        distance_func: DistanceFunction,\n",
            "        k: int = 4,\n",
            "        where_str: Optional[str] = None,\n",
            "    ) -> str:\n",
            "        \"\"\"Builds Rockset SQL query to query similar vectors to query_vector\"\"\"\n",
            "        q_embedding_str = \",\".join(map(str, query_embedding))\n",
            "        distance_str = f\"\"\"{distance_func.value}({self._embedding_key}, \\\n",
            "[{q_embedding_str}]) as dist\"\"\"\n",
            "        where_str = f\"WHERE {where_str}\\n\" if where_str else \"\"\n",
            "        return f\"\"\"\\\n",
            "SELECT * EXCEPT({self._embedding_key}), {distance_str}\n",
            "FROM {self._collection_name}\n",
            "{where_str}\\\n",
            "ORDER BY dist {distance_func.order_by()}\n",
            "LIMIT {str(k)}\n",
            "\"\"\"\n",
            "    def _write_documents_to_rockset(self, batch: List[dict]) -> List[str]:\n",
            "        add_doc_res = self._client.Documents.add_documents(\n",
            "            collection=self._collection_name, data=batch\n",
            "        )\n",
            "        return [doc_status._id for doc_status in add_doc_res.data]\n",
            "[docs]    def delete_texts(self, ids: List[str]) -> None:\n",
            "        \"\"\"Delete a list of docs from the Rockset collection\"\"\"\n",
            "        try:\n",
            "            from rockset.models import DeleteDocumentsRequestData\n",
            "        except ImportError:\n",
            "            raise ImportError(\n",
            "                \"Could not import rockset client python package. \"\n",
            "                \"Please install it with `pip install rockset`.\"\n",
            "            )\n",
            "        self._client.Documents.delete_documents(\n",
            "            collection=self._collection_name,\n",
            "            data=[DeleteDocumentsRequestData(id=i) for i in ids],\n",
            "        )\n"
          ]
        }
      ],
      "source": [
        "print(docs[35].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-mxgm-6vo9s"
      },
      "source": [
        "We can also find the source of each document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NGUGao9_uNH3",
        "outputId": "a361e24d-0919-4e8e-c753-021836c8b431"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/rocksetdb.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "docs[35].metadata['source'].replace('rtdocs/', 'https://')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouY4rcx7z2oa"
      },
      "source": [
        "Now let's see how we can process all of these. We will chunk everything into ~500 token chunks, we can do this easily with `langchain` and `tiktoken`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rb7KxUqYzsuV",
        "outputId": "1a8384ef-85d5-4b0a-bec1-296f18d47cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cl100k_base'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n",
        "tokenizer_name.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "N635Sgsbx_ME"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OKO8e3Dp0dQS"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLdvW8eq06Zd"
      },
      "source": [
        "Process the `docs` into more chunks using this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf215e622b0e4797a6a6499f6d860f76",
            "d6381d7010ad421a8d71ff5111df19cc",
            "8c4839092d1943649223b62aa1ddd345",
            "e9137557b90842569e669c7547e4d7f6",
            "67d8107e06684eb0a3f10281664a8dab",
            "2d45549b30294df696cfd8b49eb74be7",
            "054d0f63d3f24522b904497aa066c816",
            "adc745203d7c422e91e06985e7120b4f",
            "60a53948160842aa91679a44080e277b",
            "c4c21889fe5f4a21860e66fea6787242",
            "952e9cf7d5a44c91a1ba83f5ae098d5e"
          ]
        },
        "id": "uOdPyiAQ0uWs",
        "outputId": "a5e19153-cc0f-4c6f-e563-b83503ad1911"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1021 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf215e622b0e4797a6a6499f6d860f76"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from typing_extensions import Concatenate\n",
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, page in enumerate(tqdm(docs)):\n",
        "    content = page.page_content\n",
        "    if len(content) > 100:\n",
        "        url = page.metadata['source'].replace('rtdocs/', 'https://')\n",
        "        if '/stable/' in url or ('/genindex.html' in url or '/index.html' in url):\n",
        "            # this is not /latest/ docs or is index page, we don't want to include this\n",
        "            continue\n",
        "        texts = text_splitter.split_text(content)\n",
        "        chunks.extend([{\n",
        "            'id': str(uuid4()),\n",
        "            'text': texts[i],\n",
        "            'chunk': i,\n",
        "            'url': url\n",
        "        } for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JegURaAg2PuN"
      },
      "source": [
        "Our chunks are ready so now we move onto embedding and indexing everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIZbQqJ2WBh"
      },
      "source": [
        "## Initialize Embedding Model\n",
        "\n",
        "We use `text-embedding-ada-002` as the embedding model. We can embed text like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0U9_7Fium8u",
        "outputId": "94155d37-0be8-4af8-95ab-0043468a67d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7f15a27b61b0> JSON: {\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0314\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-003\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-16k\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    }\n",
              "  ],\n",
              "  \"object\": \"list\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kteZ69Z5M55S"
      },
      "outputs": [],
      "source": [
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\",\n",
        "        \"there will be several phrases in each batch\"\n",
        "    ], engine=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZ7IWekNLbu"
      },
      "source": [
        "In the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esagZj6iNLPZ",
        "outputId": "3451fed9-f6b1-4f6a-a62c-22d0eb7fb566"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['object', 'data', 'model', 'usage'])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "res.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zStnHFpkNVIU"
      },
      "source": [
        "Inside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVoP9VcINWAC",
        "outputId": "0ffecae4-db42-4635-f24c-dbcda0e78ff2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "len(res['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zraDCjNeC6",
        "outputId": "f422019f-5844-4d13-9f5a-181e819ce979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1536, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPd41MjANhmp"
      },
      "source": [
        "We will apply this same embedding logic to the langchain docs dataset we've just scraped. But before doing so we must create a place to store the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPi4MZvMNvUH"
      },
      "source": [
        "## Initializing the Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5RRQArrN2lN"
      },
      "source": [
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO5_EdUAum8v",
        "outputId": "406afdeb-08d8-43d1-baac-6a424babd9fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhoAmIResponse(username='c78f2bd', user_label='default', projectname='3947fb1')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
        "# find your environment next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
        "\n",
        "pinecone.init(api_key=api_key, environment=env)\n",
        "pinecone.whoami()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2GQAnohhum8v",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "index_name = 'gpt-4-langchain-docs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "7bca671d-721a-486d-f72a-846c213bf6cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSTzN2rPa2o"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aaaa4455f32244e8b7a14e0486110bbb",
            "3c33ae91960043d980dbe3f82554946a",
            "d3b1efb203f44af2aaef22b5e3c4fe67",
            "bc4b0f2ac52c4b01bc543f8c61d83c6b",
            "2c2bd1e92d5c4e08b1443c1bcb662f1c",
            "b6bd7021643a48a68680a0a93049377b",
            "eabdfc09dac04a76ab4cf85d59d07f15",
            "2ad6bd667c85432c9816d1ce279b9820",
            "b995a3b738674da693c32fe532ebcf2c",
            "812d117f411a4ad88ee1c71937f9b9b6",
            "e3852c3bc52b461e83fff82bfe2cf194"
          ]
        },
        "id": "iZbFbulAPeop",
        "outputId": "b1c539dd-4638-487d-f877-c448b5155908"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaaa4455f32244e8b7a14e0486110bbb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "    # get ids\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    # get texts to encode\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    try:\n",
        "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            time.sleep(5)\n",
        "            try:\n",
        "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    # cleanup metadata\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'url': x['url']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttJOrEtQIF9"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumVmMRlQQ7w"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLRODeL-QTJ9"
      },
      "source": [
        "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FMUPdX9cQQYC"
      },
      "outputs": [],
      "source": [
        "query = \"how do I use the LLMChain in LangChain?\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=5, include_metadata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9SrFPkQjg-",
        "outputId": "0b211a24-61e8-47ea-8cdc-dbd3fd9d020a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matches': [{'id': '1c576e13-7f97-4109-96cf-b0c0b3626512',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': 'Models\\uf0c1\\n'\n",
              "                                   'LangChain provides interfaces and '\n",
              "                                   'integrations for a number of different '\n",
              "                                   'types of models.\\n'\n",
              "                                   'LLMs\\n'\n",
              "                                   'Chat Models',\n",
              "                           'url': 'https://api.python.langchain.com/en/latest/models.html'},\n",
              "              'score': 0.827638745,\n",
              "              'values': []},\n",
              "             {'id': 'a352fb94-ce42-4707-ab27-351c484d5021',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': 'Data connection\\uf0c1\\n'\n",
              "                                   'LangChain has a number of modules that '\n",
              "                                   'help you load, structure, store, and '\n",
              "                                   'retrieve documents.\\n'\n",
              "                                   'Document Loaders\\n'\n",
              "                                   'Document Transformers\\n'\n",
              "                                   'Embeddings\\n'\n",
              "                                   'Vector Stores\\n'\n",
              "                                   'Retrievers',\n",
              "                           'url': 'https://api.python.langchain.com/en/latest/data_connection.html'},\n",
              "              'score': 0.806993186,\n",
              "              'values': []},\n",
              "             {'id': '8ce5f37f-58de-4993-a597-9bc989fd17e9',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': 'Source code for langchain.chains.llm\\n'\n",
              "                                   '\"\"\"Chain that just formats a prompt and '\n",
              "                                   'calls an LLM.\"\"\"\\n'\n",
              "                                   'from __future__ import annotations\\n'\n",
              "                                   'import warnings\\n'\n",
              "                                   'from typing import Any, Dict, List, '\n",
              "                                   'Optional, Sequence, Tuple, Union\\n'\n",
              "                                   'from pydantic import Extra, Field\\n'\n",
              "                                   'from langchain.base_language import '\n",
              "                                   'BaseLanguageModel\\n'\n",
              "                                   'from langchain.callbacks.manager import (\\n'\n",
              "                                   '    AsyncCallbackManager,\\n'\n",
              "                                   '    AsyncCallbackManagerForChainRun,\\n'\n",
              "                                   '    CallbackManager,\\n'\n",
              "                                   '    CallbackManagerForChainRun,\\n'\n",
              "                                   '    Callbacks,\\n'\n",
              "                                   ')\\n'\n",
              "                                   'from langchain.chains.base import Chain\\n'\n",
              "                                   'from langchain.input import '\n",
              "                                   'get_colored_text\\n'\n",
              "                                   'from langchain.load.dump import dumpd\\n'\n",
              "                                   'from langchain.prompts.base import '\n",
              "                                   'BasePromptTemplate\\n'\n",
              "                                   'from langchain.prompts.prompt import '\n",
              "                                   'PromptTemplate\\n'\n",
              "                                   'from langchain.schema import (\\n'\n",
              "                                   '    BaseLLMOutputParser,\\n'\n",
              "                                   '    LLMResult,\\n'\n",
              "                                   '    NoOpOutputParser,\\n'\n",
              "                                   '    PromptValue,\\n'\n",
              "                                   ')\\n'\n",
              "                                   '[docs]class LLMChain(Chain):\\n'\n",
              "                                   '    \"\"\"Chain to run queries against LLMs.\\n'\n",
              "                                   '    Example:\\n'\n",
              "                                   '        .. code-block:: python\\n'\n",
              "                                   '            from langchain import '\n",
              "                                   'LLMChain, OpenAI, PromptTemplate\\n'\n",
              "                                   '            prompt_template = \"Tell me a '\n",
              "                                   '{adjective} joke\"\\n'\n",
              "                                   '            prompt = PromptTemplate(\\n'\n",
              "                                   '                '\n",
              "                                   'input_variables=[\"adjective\"], '\n",
              "                                   'template=prompt_template\\n'\n",
              "                                   '            )\\n'\n",
              "                                   '            llm = LLMChain(llm=OpenAI(), '\n",
              "                                   'prompt=prompt)\\n'\n",
              "                                   '    \"\"\"\\n'\n",
              "                                   '    @property\\n'\n",
              "                                   '    def lc_serializable(self) -> bool:\\n'\n",
              "                                   '        return True\\n'\n",
              "                                   '    prompt: BasePromptTemplate\\n'\n",
              "                                   '    \"\"\"Prompt object to use.\"\"\"\\n'\n",
              "                                   '    llm: BaseLanguageModel\\n'\n",
              "                                   '    \"\"\"Language model to call.\"\"\"\\n'\n",
              "                                   '    output_key: str = \"text\"  #: :meta '\n",
              "                                   'private:\\n'\n",
              "                                   '    output_parser: BaseLLMOutputParser = '\n",
              "                                   'Field(default_factory=NoOpOutputParser)\\n'\n",
              "                                   '    \"\"\"Output parser to use.\\n'\n",
              "                                   '    Defaults to one that takes the most '\n",
              "                                   'likely string but does not change it \\n'\n",
              "                                   '    otherwise.\"\"\"\\n'\n",
              "                                   '    return_final_only: bool = True\\n'\n",
              "                                   '    \"\"\"Whether to return only the final '\n",
              "                                   'parsed result. Defaults to True.\\n'\n",
              "                                   '    If false, will return a bunch of extra '\n",
              "                                   'information about the generation.\"\"\"\\n'\n",
              "                                   '    llm_kwargs: dict = '\n",
              "                                   'Field(default_factory=dict)\\n'\n",
              "                                   '    class Config:\\n'\n",
              "                                   '        \"\"\"Configuration for this pydantic '\n",
              "                                   'object.\"\"\"\\n'\n",
              "                                   '        extra = Extra.forbid\\n'\n",
              "                                   '        arbitrary_types_allowed = True',\n",
              "                           'url': 'https://api.python.langchain.com/en/latest/_modules/langchain/chains/llm.html'},\n",
              "              'score': 0.800940871,\n",
              "              'values': []},\n",
              "             {'id': '9c442a6a-a59d-4682-bdc4-1264614a42fc',\n",
              "              'metadata': {'chunk': 82.0,\n",
              "                           'text': 'Bases: langchain.chains.base.Chain\\n'\n",
              "                                   'Chain for question-answering with '\n",
              "                                   'self-verification.\\n'\n",
              "                                   'Example\\n'\n",
              "                                   'from langchain import OpenAI, '\n",
              "                                   'LLMSummarizationCheckerChain\\n'\n",
              "                                   'llm = OpenAI(temperature=0.0)\\n'\n",
              "                                   'checker_chain = '\n",
              "                                   'LLMSummarizationCheckerChain.from_llm(llm)\\n'\n",
              "                                   'Parameters\\n'\n",
              "                                   'memory '\n",
              "                                   '(Optional[langchain.schema.BaseMemory]) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'callbacks '\n",
              "                                   '(Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], '\n",
              "                                   'langchain.callbacks.base.BaseCallbackManager]]) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'callback_manager '\n",
              "                                   '(Optional[langchain.callbacks.base.BaseCallbackManager]) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'verbose (bool) â€“ \\n'\n",
              "                                   'tags (Optional[List[str]]) â€“ \\n'\n",
              "                                   'sequential_chain '\n",
              "                                   '(langchain.chains.sequential.SequentialChain) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'llm '\n",
              "                                   '(Optional[langchain.base_language.BaseLanguageModel]) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'create_assertions_prompt '\n",
              "                                   '(langchain.prompts.prompt.PromptTemplate) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'check_assertions_prompt '\n",
              "                                   '(langchain.prompts.prompt.PromptTemplate) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'revised_summary_prompt '\n",
              "                                   '(langchain.prompts.prompt.PromptTemplate) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'are_all_true_prompt '\n",
              "                                   '(langchain.prompts.prompt.PromptTemplate) '\n",
              "                                   'â€“ \\n'\n",
              "                                   'input_key (str) â€“ \\n'\n",
              "                                   'output_key (str) â€“ \\n'\n",
              "                                   'max_checks (int) â€“ \\n'\n",
              "                                   'Return type\\n'\n",
              "                                   'None',\n",
              "                           'url': 'https://api.python.langchain.com/en/latest/modules/chains.html'},\n",
              "              'score': 0.795693278,\n",
              "              'values': []},\n",
              "             {'id': 'ccea861a-0aaa-44b8-ad63-5512409e3fc1',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': '[docs]    @classmethod\\n'\n",
              "                                   '    def from_llm(\\n'\n",
              "                                   '        cls,\\n'\n",
              "                                   '        llm: BaseLanguageModel,\\n'\n",
              "                                   '        chain: LLMChain,\\n'\n",
              "                                   '        critique_prompt: '\n",
              "                                   'BasePromptTemplate = CRITIQUE_PROMPT,\\n'\n",
              "                                   '        revision_prompt: '\n",
              "                                   'BasePromptTemplate = REVISION_PROMPT,\\n'\n",
              "                                   '        **kwargs: Any,\\n'\n",
              "                                   '    ) -> \"ConstitutionalChain\":\\n'\n",
              "                                   '        \"\"\"Create a chain from an LLM.\"\"\"\\n'\n",
              "                                   '        critique_chain = LLMChain(llm=llm, '\n",
              "                                   'prompt=critique_prompt)\\n'\n",
              "                                   '        revision_chain = LLMChain(llm=llm, '\n",
              "                                   'prompt=revision_prompt)\\n'\n",
              "                                   '        return cls(\\n'\n",
              "                                   '            chain=chain,\\n'\n",
              "                                   '            '\n",
              "                                   'critique_chain=critique_chain,\\n'\n",
              "                                   '            '\n",
              "                                   'revision_chain=revision_chain,\\n'\n",
              "                                   '            **kwargs,\\n'\n",
              "                                   '        )\\n'\n",
              "                                   '    @property\\n'\n",
              "                                   '    def input_keys(self) -> List[str]:\\n'\n",
              "                                   '        \"\"\"Defines the input keys.\"\"\"\\n'\n",
              "                                   '        return self.chain.input_keys\\n'\n",
              "                                   '    @property\\n'\n",
              "                                   '    def output_keys(self) -> List[str]:\\n'\n",
              "                                   '        \"\"\"Defines the output keys.\"\"\"\\n'\n",
              "                                   '        if '\n",
              "                                   'self.return_intermediate_steps:\\n'\n",
              "                                   '            return [\"output\", '\n",
              "                                   '\"critiques_and_revisions\", '\n",
              "                                   '\"initial_output\"]\\n'\n",
              "                                   '        return [\"output\"]\\n'\n",
              "                                   '    def _call(\\n'\n",
              "                                   '        self,\\n'\n",
              "                                   '        inputs: Dict[str, Any],\\n'\n",
              "                                   '        run_manager: '\n",
              "                                   'Optional[CallbackManagerForChainRun] = '\n",
              "                                   'None,\\n'\n",
              "                                   '    ) -> Dict[str, Any]:\\n'\n",
              "                                   '        _run_manager = run_manager or '\n",
              "                                   'CallbackManagerForChainRun.get_noop_manager()\\n'\n",
              "                                   '        response = self.chain.run(\\n'\n",
              "                                   '            **inputs,\\n'\n",
              "                                   '            '\n",
              "                                   'callbacks=_run_manager.get_child(\"original\"),\\n'\n",
              "                                   '        )\\n'\n",
              "                                   '        initial_response = response\\n'\n",
              "                                   '        input_prompt = '\n",
              "                                   'self.chain.prompt.format(**inputs)\\n'\n",
              "                                   '        _run_manager.on_text(\\n'\n",
              "                                   '            text=\"Initial response: \" + '\n",
              "                                   'response + \"\\\\n\\\\n\",\\n'\n",
              "                                   '            verbose=self.verbose,\\n'\n",
              "                                   '            color=\"yellow\",\\n'\n",
              "                                   '        )\\n'\n",
              "                                   '        critiques_and_revisions = []\\n'\n",
              "                                   '        for constitutional_principle in '\n",
              "                                   'self.constitutional_principles:\\n'\n",
              "                                   '            # Do critique\\n'\n",
              "                                   '            raw_critique = '\n",
              "                                   'self.critique_chain.run(\\n'\n",
              "                                   '                '\n",
              "                                   'input_prompt=input_prompt,\\n'\n",
              "                                   '                '\n",
              "                                   'output_from_model=response,\\n'\n",
              "                                   '                '\n",
              "                                   'critique_request=constitutional_principle.critique_request,\\n'\n",
              "                                   '                '\n",
              "                                   'callbacks=_run_manager.get_child(\"critique\"),\\n'\n",
              "                                   '            )\\n'\n",
              "                                   '            critique = '\n",
              "                                   'self._parse_critique(\\n'\n",
              "                                   '                '\n",
              "                                   'output_string=raw_critique,',\n",
              "                           'url': 'https://api.python.langchain.com/en/latest/_modules/langchain/chains/constitutional_ai/base.html'},\n",
              "              'score': 0.79351896,\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBSiDLIUADZ"
      },
      "source": [
        "With retrieval complete, we move on to feeding these into GPT-4 to produce answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzS4-6-UXgX"
      },
      "source": [
        "## Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPC1jQaKUcy0"
      },
      "source": [
        "GPT-4 is currently accessed via the `ChatCompletions` endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts *alongside* our original query. We can do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "# get list of retrieved text\n",
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "ee51477f-0b78-41c4-c733-08c4c4f3382e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelsïƒ\n",
            "LangChain provides interfaces and integrations for a number of different types of models.\n",
            "LLMs\n",
            "Chat Models\n",
            "\n",
            "---\n",
            "\n",
            "Data connectionïƒ\n",
            "LangChain has a number of modules that help you load, structure, store, and retrieve documents.\n",
            "Document Loaders\n",
            "Document Transformers\n",
            "Embeddings\n",
            "Vector Stores\n",
            "Retrievers\n",
            "\n",
            "---\n",
            "\n",
            "Source code for langchain.chains.llm\n",
            "\"\"\"Chain that just formats a prompt and calls an LLM.\"\"\"\n",
            "from __future__ import annotations\n",
            "import warnings\n",
            "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
            "from pydantic import Extra, Field\n",
            "from langchain.base_language import BaseLanguageModel\n",
            "from langchain.callbacks.manager import (\n",
            "    AsyncCallbackManager,\n",
            "    AsyncCallbackManagerForChainRun,\n",
            "    CallbackManager,\n",
            "    CallbackManagerForChainRun,\n",
            "    Callbacks,\n",
            ")\n",
            "from langchain.chains.base import Chain\n",
            "from langchain.input import get_colored_text\n",
            "from langchain.load.dump import dumpd\n",
            "from langchain.prompts.base import BasePromptTemplate\n",
            "from langchain.prompts.prompt import PromptTemplate\n",
            "from langchain.schema import (\n",
            "    BaseLLMOutputParser,\n",
            "    LLMResult,\n",
            "    NoOpOutputParser,\n",
            "    PromptValue,\n",
            ")\n",
            "[docs]class LLMChain(Chain):\n",
            "    \"\"\"Chain to run queries against LLMs.\n",
            "    Example:\n",
            "        .. code-block:: python\n",
            "            from langchain import LLMChain, OpenAI, PromptTemplate\n",
            "            prompt_template = \"Tell me a {adjective} joke\"\n",
            "            prompt = PromptTemplate(\n",
            "                input_variables=[\"adjective\"], template=prompt_template\n",
            "            )\n",
            "            llm = LLMChain(llm=OpenAI(), prompt=prompt)\n",
            "    \"\"\"\n",
            "    @property\n",
            "    def lc_serializable(self) -> bool:\n",
            "        return True\n",
            "    prompt: BasePromptTemplate\n",
            "    \"\"\"Prompt object to use.\"\"\"\n",
            "    llm: BaseLanguageModel\n",
            "    \"\"\"Language model to call.\"\"\"\n",
            "    output_key: str = \"text\"  #: :meta private:\n",
            "    output_parser: BaseLLMOutputParser = Field(default_factory=NoOpOutputParser)\n",
            "    \"\"\"Output parser to use.\n",
            "    Defaults to one that takes the most likely string but does not change it \n",
            "    otherwise.\"\"\"\n",
            "    return_final_only: bool = True\n",
            "    \"\"\"Whether to return only the final parsed result. Defaults to True.\n",
            "    If false, will return a bunch of extra information about the generation.\"\"\"\n",
            "    llm_kwargs: dict = Field(default_factory=dict)\n",
            "    class Config:\n",
            "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
            "        extra = Extra.forbid\n",
            "        arbitrary_types_allowed = True\n",
            "\n",
            "---\n",
            "\n",
            "Bases: langchain.chains.base.Chain\n",
            "Chain for question-answering with self-verification.\n",
            "Example\n",
            "from langchain import OpenAI, LLMSummarizationCheckerChain\n",
            "llm = OpenAI(temperature=0.0)\n",
            "checker_chain = LLMSummarizationCheckerChain.from_llm(llm)\n",
            "Parameters\n",
            "memory (Optional[langchain.schema.BaseMemory]) â€“ \n",
            "callbacks (Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]]) â€“ \n",
            "callback_manager (Optional[langchain.callbacks.base.BaseCallbackManager]) â€“ \n",
            "verbose (bool) â€“ \n",
            "tags (Optional[List[str]]) â€“ \n",
            "sequential_chain (langchain.chains.sequential.SequentialChain) â€“ \n",
            "llm (Optional[langchain.base_language.BaseLanguageModel]) â€“ \n",
            "create_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "check_assertions_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "revised_summary_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "are_all_true_prompt (langchain.prompts.prompt.PromptTemplate) â€“ \n",
            "input_key (str) â€“ \n",
            "output_key (str) â€“ \n",
            "max_checks (int) â€“ \n",
            "Return type\n",
            "None\n",
            "\n",
            "---\n",
            "\n",
            "[docs]    @classmethod\n",
            "    def from_llm(\n",
            "        cls,\n",
            "        llm: BaseLanguageModel,\n",
            "        chain: LLMChain,\n",
            "        critique_prompt: BasePromptTemplate = CRITIQUE_PROMPT,\n",
            "        revision_prompt: BasePromptTemplate = REVISION_PROMPT,\n",
            "        **kwargs: Any,\n",
            "    ) -> \"ConstitutionalChain\":\n",
            "        \"\"\"Create a chain from an LLM.\"\"\"\n",
            "        critique_chain = LLMChain(llm=llm, prompt=critique_prompt)\n",
            "        revision_chain = LLMChain(llm=llm, prompt=revision_prompt)\n",
            "        return cls(\n",
            "            chain=chain,\n",
            "            critique_chain=critique_chain,\n",
            "            revision_chain=revision_chain,\n",
            "            **kwargs,\n",
            "        )\n",
            "    @property\n",
            "    def input_keys(self) -> List[str]:\n",
            "        \"\"\"Defines the input keys.\"\"\"\n",
            "        return self.chain.input_keys\n",
            "    @property\n",
            "    def output_keys(self) -> List[str]:\n",
            "        \"\"\"Defines the output keys.\"\"\"\n",
            "        if self.return_intermediate_steps:\n",
            "            return [\"output\", \"critiques_and_revisions\", \"initial_output\"]\n",
            "        return [\"output\"]\n",
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, Any]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        response = self.chain.run(\n",
            "            **inputs,\n",
            "            callbacks=_run_manager.get_child(\"original\"),\n",
            "        )\n",
            "        initial_response = response\n",
            "        input_prompt = self.chain.prompt.format(**inputs)\n",
            "        _run_manager.on_text(\n",
            "            text=\"Initial response: \" + response + \"\\n\\n\",\n",
            "            verbose=self.verbose,\n",
            "            color=\"yellow\",\n",
            "        )\n",
            "        critiques_and_revisions = []\n",
            "        for constitutional_principle in self.constitutional_principles:\n",
            "            # Do critique\n",
            "            raw_critique = self.critique_chain.run(\n",
            "                input_prompt=input_prompt,\n",
            "                output_from_model=response,\n",
            "                critique_request=constitutional_principle.critique_request,\n",
            "                callbacks=_run_manager.get_child(\"critique\"),\n",
            "            )\n",
            "            critique = self._parse_critique(\n",
            "                output_string=raw_critique,\n",
            "\n",
            "-----\n",
            "\n",
            "how do I use the LLMChain in LangChain?\n"
          ]
        }
      ],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihH_GMiV5_p"
      },
      "source": [
        "Now we ask the question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "# system message to 'prime' the model\n",
        "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS1yJhOWpiJ"
      },
      "source": [
        "To display this response nicely, we will display it in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "RDo2qeMHWto1",
        "outputId": "52c9161e-cdb3-4c63-adbd-039fed72eb10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To use the LLMChain in LangChain, follow these steps:\n\n1. Import the necessary modules:\n\n```python\nfrom langchain import LLMChain, OpenAI, PromptTemplate\n```\n\n2. Create a prompt template by specifying the input variables and template string:\n\n```python\nprompt_template = \"Tell me a {adjective} joke\"\nprompt = PromptTemplate(\n    input_variables=[\"adjective\"], template=prompt_template\n)\n```\n\n3. Create an instance of LLMChain by providing the language model (e.g. OpenAI) and the prompt:\n\n```python\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\n```\n\n4. Run the LLMChain with the desired input:\n\n```python\nresult = llm.run(adjective=\"funny\")\n```\n\n5. Print the result:\n\n```python\nprint(result)\n```\n\nHere's the complete example:\n\n```python\nfrom langchain import LLMChain, OpenAI, PromptTemplate\n\nprompt_template = \"Tell me a {adjective} joke\"\nprompt = PromptTemplate(\n    input_variables=[\"adjective\"], template=prompt_template\n)\nllm = LLMChain(llm=OpenAI(), prompt=prompt)\n\nresult = llm.run(adjective=\"funny\")\nprint(result)\n```\n\nReplace `OpenAI()` with the appropriate language model instance for your use case."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-a8MHg0eYQ"
      },
      "source": [
        "Let's compare this to a non-augmented query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "152b0df7-6c9f-42cb-9606-60bdf9a16338"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know."
          },
          "metadata": {}
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSsA-dW0m_P"
      },
      "source": [
        "If we drop the `\"I don't know\"` part of the `primer`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "d5cc5684-6765-43e3-efa0-9566c5e5c5fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangChain is a language model developed by OpenAI, and currently, there isn't any documentation available mentioning \"LLMChain\" or any component named as such. It seems that the term \"LLMChain\" does not exist or may be a user-specific term.\n\nIn order to give you more helpful information or guide you through the process you're looking for, I might need more context or details on what you're trying to accomplish. Please provide more information or clarify your request, and I'll do my best to help you."
          },
          "metadata": {}
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcGon5672lBb"
      },
      "source": [
        "Then we see something even worse than `\"I don't know\"` â€” hallucinations. Clearly augmenting our queries with additional context can make a huge difference to the performance of our system.\n",
        "\n",
        "Great, we've seen how to augment GPT-4 with semantic search to allow us to answer LangChain specific queries.\n",
        "\n",
        "Once you're finished, we delete the index to save resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Ah_vfEHV2khx"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUMlO8M2h4Y"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf215e622b0e4797a6a6499f6d860f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6381d7010ad421a8d71ff5111df19cc",
              "IPY_MODEL_8c4839092d1943649223b62aa1ddd345",
              "IPY_MODEL_e9137557b90842569e669c7547e4d7f6"
            ],
            "layout": "IPY_MODEL_67d8107e06684eb0a3f10281664a8dab"
          }
        },
        "d6381d7010ad421a8d71ff5111df19cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d45549b30294df696cfd8b49eb74be7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_054d0f63d3f24522b904497aa066c816",
            "value": "100%"
          }
        },
        "8c4839092d1943649223b62aa1ddd345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc745203d7c422e91e06985e7120b4f",
            "max": 1021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a53948160842aa91679a44080e277b",
            "value": 1021
          }
        },
        "e9137557b90842569e669c7547e4d7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4c21889fe5f4a21860e66fea6787242",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_952e9cf7d5a44c91a1ba83f5ae098d5e",
            "value": " 1021/1021 [00:05&lt;00:00, 194.03it/s]"
          }
        },
        "67d8107e06684eb0a3f10281664a8dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d45549b30294df696cfd8b49eb74be7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054d0f63d3f24522b904497aa066c816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc745203d7c422e91e06985e7120b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a53948160842aa91679a44080e277b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4c21889fe5f4a21860e66fea6787242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952e9cf7d5a44c91a1ba83f5ae098d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaaa4455f32244e8b7a14e0486110bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c33ae91960043d980dbe3f82554946a",
              "IPY_MODEL_d3b1efb203f44af2aaef22b5e3c4fe67",
              "IPY_MODEL_bc4b0f2ac52c4b01bc543f8c61d83c6b"
            ],
            "layout": "IPY_MODEL_2c2bd1e92d5c4e08b1443c1bcb662f1c"
          }
        },
        "3c33ae91960043d980dbe3f82554946a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6bd7021643a48a68680a0a93049377b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eabdfc09dac04a76ab4cf85d59d07f15",
            "value": "100%"
          }
        },
        "d3b1efb203f44af2aaef22b5e3c4fe67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad6bd667c85432c9816d1ce279b9820",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b995a3b738674da693c32fe532ebcf2c",
            "value": 25
          }
        },
        "bc4b0f2ac52c4b01bc543f8c61d83c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812d117f411a4ad88ee1c71937f9b9b6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3852c3bc52b461e83fff82bfe2cf194",
            "value": " 25/25 [00:53&lt;00:00,  1.84s/it]"
          }
        },
        "2c2bd1e92d5c4e08b1443c1bcb662f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bd7021643a48a68680a0a93049377b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eabdfc09dac04a76ab4cf85d59d07f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad6bd667c85432c9816d1ce279b9820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b995a3b738674da693c32fe532ebcf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "812d117f411a4ad88ee1c71937f9b9b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3852c3bc52b461e83fff82bfe2cf194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}