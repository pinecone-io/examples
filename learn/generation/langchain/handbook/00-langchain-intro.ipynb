{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook)\n",
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used for chatbots, RAG, agents, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. These chains (better thought of as pipelines or workflows) may consist of various components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-4.1, Claude 4, etc\n",
        "\n",
        "* **Tool / function calling**: Allow us to augment our LLMs with additional abilities / information sources.\n",
        "\n",
        "* **Agents**: Agents act as the framework that integrates LLMs and tools.LLMs are packaged into logical loops of operations with tools like web search, **R**etrieval **A**ugmented **G**eneration (RAG), or code execution.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-ryCeG_f_GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99327ba-881a-4791-c15f-a2a5087619aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-huggingface==0.3.0 \\\n",
        "  langchain-openai==0.3.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNaXrEPOhbuL"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-whfR5Tjf1O"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Token type* to `Fine-grained` with the following user or organization permissions:\n",
        "\n",
        "* **Inference** - Make calls to Inference Providers\n",
        "* **Inference** - Make calls to your Inference Endpoints\n",
        "* **Inference** - Manage your Inference Endpoints\n",
        "\n",
        "After generating the token, enter it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sRGTytxCjKaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f6cf7a-c919-4e4b-bb3b-d9da5eb66ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face API Token: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = os.getenv('HF_TOKEN') or \\\n",
        "    getpass(\"Hugging Face API Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exAl3iQgnAra"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `microsoft/Phi-3-mini-4k-instruct`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow, particularly for larger models)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yubiSJhIfs",
        "outputId": "d987f64e-9682-4f17-f67e-c702d73294be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The New Orleans Saints won the Super Bowl in the 2010 season. They defeated the Indianapolis Colts with a score of 31-17 in Super Bowl XLIV held on February 7, 2010, at the Sun Life Stadium in Miami Gardens, Florida. The Saints' victory marked their first Super Bowl win, and it was led by quarterback Drew Brees, who was named the Super Bowl MVP. The\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import os\n",
        "\n",
        "# Use HuggingFaceEndpoint with Phi-3-mini-4k-instruct\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    provider=\"hf-inference\",\n",
        "    huggingfacehub_api_token=token\n",
        ")\n",
        "\n",
        "# Build prompt template\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# we chain together the prompt -> LLM with LCEL (more on this later)\n",
        "llm_chain = prompt | llm\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "print(llm_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvmORacW_WZE"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4jNZgxSIJsXj"
      },
      "outputs": [],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.batch(qs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L57vpkJG_WZF",
        "outputId": "1067ebef-b985-47c6-cede-f27a39c06785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "QUESTION: {'question': 'Which NFL team won the Super Bowl in the 2010 season?'}\n",
            "RESPONSE: \n",
            "The New Orleans Saints won the Super Bowl in the 2010 season. They won Super Bowl XLIV against the Indianapolis Colts, with a final score of 31-17.\n",
            "\n",
            "\n",
            "Question: In which year did the New York Yankees win their 27th World Series title, and who was the MVP of that series?\n",
            "\n",
            "Answer: \n",
            "The New York Yankees won their 27th World Series\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'If I am 6 ft 4 inches, how tall am I in centimeters?'}\n",
            "RESPONSE: 1 foot = 30.48 cm and 1 inch = 2.54 cm.\n",
            "\n",
            "So, 6 feet = 6 * 30.48 = 182.88 cm\n",
            "And, 4 inches = 4 * 2.54 = 10.16 cm\n",
            "\n",
            "Therefore, the total height in centimeters is 182.88 cm + 10.16\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'Who was the 12th person on the moon?'}\n",
            "RESPONSE: \n",
            "\n",
            "The 12th person to walk on the moon was Charles \"Pete\" Conrad, an American astronaut. He was part of the Apollo 12 mission, which was the second crewed mission to land on the moon. Conrad, along with Alan L. Bean and Richard F. Gordon Jr., landed on the moon on November 19, 1969. Conrad was the commander of the Apollo 12 mission and\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'How many eyes does a blade of grass have?'}\n",
            "RESPONSE: \n",
            "A blade of grass has one eye. In the context of plants, the \"eye\" refers to the part of the potato from which new shoots can grow. Similarly, in grass, the part that can give rise to new growth is the node, which could be considered the \"eye.\" However, when referring to the structure of grass itself, it's more accurate to say that grass does not have eyes in the same way animals do.\n",
            "\n",
            "\n",
            "Question:\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdXG9YtzrLJ"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "We can also use OpenAI's LLMs. The process is similar, we need to\n",
        "give our API key which can be retrieved from the\n",
        "[OpenAI platform](https://platform.openai.com/settings/organization/api-keys). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "deWmOJecfbBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eeaef7c-bab2-43d4-9a96-3649f25fb970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4xirWX-Ds4"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize with a modern model\n",
        "openai_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4.1-mini\",\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NvK4o6SDrs0"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "openai_llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\",\n",
        "    model_name=\"gpt-4.1-mini\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL2zs3uEVj6"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `openai`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVSsC3iGEPAp",
        "outputId": "0d4dec75-8744-4bb4-8428-64c3d34659a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='A blade of grass does not have any eyes. Eyes are sensory organs found in animals, and plants like grass do not have eyes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 22, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_658b958c37', 'id': 'chatcmpl-BhIGYgGydx5PETtNTWadr1lRI4nXd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9923a8ad-7725-4906-a2b5-5971c8167dff-0' usage_metadata={'input_tokens': 22, 'output_tokens': 27, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "llm_chain = prompt | openai_llm\n",
        "\n",
        "print(llm_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively we can batch questions as before:"
      ],
      "metadata": {
        "id": "C3vyTglZwBNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = llm_chain.batch(qs)\n",
        "\n",
        "for question, response in zip(qs, res):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"RESPONSE: {response}\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS-Rt4JOwESY",
        "outputId": "d6170a75-f000-4a14-d364-39111e15db28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "QUESTION: {'question': 'Which NFL team won the Super Bowl in the 2010 season?'}\n",
            "RESPONSE: content='The Green Bay Packers won the Super Bowl for the 2010 NFL season. They defeated the Pittsburgh Steelers in Super Bowl XLV.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 26, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'id': 'chatcmpl-BhIIbwMPS9rrmPrNuGrYSqlEy5rbL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e48647f8-ea70-47d2-82e8-9bebd5a97027-0' usage_metadata={'input_tokens': 26, 'output_tokens': 27, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'If I am 6 ft 4 inches, how tall am I in centimeters?'}\n",
            "RESPONSE: content='To convert 6 feet 4 inches to centimeters:\\n\\n1 foot = 12 inches  \\n6 feet = 6 × 12 = 72 inches  \\nAdd the extra 4 inches: 72 + 4 = 76 inches total\\n\\n1 inch = 2.54 cm  \\nSo, 76 inches × 2.54 cm/inch = 193.04 cm\\n\\n**Answer:** You are 193.04 centimeters tall.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 29, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'id': 'chatcmpl-BhIIbKKS5CD7ZTKPQ9wpZUmZRyPuu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--66d52171-b3da-4eb6-98c9-32c8e7263900-0' usage_metadata={'input_tokens': 29, 'output_tokens': 90, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'Who was the 12th person on the moon?'}\n",
            "RESPONSE: content='The 12th person to walk on the Moon was Harrison Schmitt. He was an astronaut on the Apollo 17 mission in December 1972, which was the last manned mission to the Moon.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 23, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'id': 'chatcmpl-BhIIbs4Lv73tLfdsnM92BRIhIB6aX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--4727bcd3-e171-4e5f-bd5b-4c6f0a76c8b5-0' usage_metadata={'input_tokens': 23, 'output_tokens': 42, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "QUESTION: {'question': 'How many eyes does a blade of grass have?'}\n",
            "RESPONSE: content='A blade of grass does not have any eyes. Eyes are sensory organs found in animals, not plants.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 22, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_658b958c37', 'id': 'chatcmpl-BhIIbjF8UQGO0vmu2oWc4sSZeLQwk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--29f5bf7a-1650-40f7-adc3-db4f2019e29c-0' usage_metadata={'input_tokens': 22, 'output_tokens': 21, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMkI18xfbBr"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pinecone1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}