{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZR3iGJJtdDE",
        "outputId": "9ebb9f66-add2-4567-e37e-05323073e26b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain openai langchain-community numexpr"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a4ba72d",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
        "\n",
        "# Getting Started with Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain â€” a `PromptTemplate` â€” to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain â€” a LLM.\n",
        "\n",
        "We'll start by importing all the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import re\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "from langchain.callbacks import get_openai_callback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "v86cmyppxdfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v86cmyppxdfc",
        "outputId": "a897108c-be81-49b8-8802-d71741243e43"
      },
      "outputs": [],
      "source": [
        "# OPENAI_API_KEY = getpass() # SRA_DEBUGGING: Uncomment once testing is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "baaa74b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# initialize the models\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-4\" or other available models\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What are chains anyway?\n",
        "\n",
        "Chains in LangChain are now built using the LangChain Expression Language (LCEL), which takes a declarative approach to combining components. Instead of using predefined chain classes, LCEL lets you compose chains using the `|` operator and other composition primitives.\n",
        "\n",
        "### Types of Chain Composition\n",
        "\n",
        "1. **Sequential Chains** (`|` operator)\n",
        "   - Chain components one after another\n",
        "   - Example: `prompt | llm | output_parser`\n",
        "\n",
        "2. **Parallel Chains** (`RunnableParallel`)\n",
        "   - Run multiple operations concurrently\n",
        "   - Example: Running multiple prompts or retrievers in parallel\n",
        "\n",
        "3. **Complex Workflows**\n",
        "   - For more complex scenarios involving branching, cycles, or multiple agents\n",
        "   - Recommended to use LangGraph instead of LCEL directly\n",
        "\n",
        "Let's start with a simple example: creating a sequential math chain that can handle calculations..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b4161561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4161561",
        "outputId": "16486831-80a5-41df-906e-376575b7f514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The result is: 2.4116004626599237\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import numexpr\n",
        "\n",
        "# Create a function to handle calculations\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Calculate using numexpr, with support for basic math operations.\"\"\"\n",
        "    try:\n",
        "        result = float(numexpr.evaluate(expression))\n",
        "        return f\"The result is: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in calculation: {str(e)}\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful math assistant. When given a math problem, respond ONLY with the mathematical expression that would solve it. For example, if asked 'What is 2 raised to the 3rd power?', respond only with '2**3'.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Create the chain using LCEL\n",
        "math_chain = (\n",
        "    prompt \n",
        "    | ChatOpenAI(temperature=0) \n",
        "    | StrOutputParser()  # Convert to string\n",
        "    | calculate  # Our calculation function\n",
        ")\n",
        "\n",
        "# Use the chain with our example\n",
        "response = math_chain.invoke({\n",
        "    \"question\": \"What is 13 raised to the .3432 power?\"\n",
        "})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fecbca",
      "metadata": {},
      "source": [
        "Let's see what is going on here. The chain processes our input through several sequential steps:\n",
        "\n",
        "1. The prompt template formats our question\n",
        "2. The LLM converts it to a mathematical expression\n",
        "3. The StrOutputParser ensures we get a clean string\n",
        "4. Finally, our calculate function computes the result\n",
        "\n",
        "But how did the LLM know to return just the mathematical expression? ðŸ¤”\n",
        "\n",
        "**Enter prompts**\n",
        "\n",
        "The question we send isn't the only input the LLM receives ðŸ˜‰. Look at our prompt template:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "42224b54",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful math assistant. When given a math problem, respond ONLY with the mathematical expression that would solve it. For example, if asked 'What is 2 raised to the 3rd power?', respond only with '2**3'.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abdb8561",
      "metadata": {},
      "source": [
        "The system message explicitly instructs the LLM to return only the mathematical expression. Without this context, the LLM would try to calculate the result itself. Let's test this by trying without the system message:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8e477bc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13 raised to the 0.3432 power is approximately 3.144.\n"
          ]
        }
      ],
      "source": [
        "# Simple prompt without guidance\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "basic_chain = (\n",
        "    prompt \n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = basic_chain.invoke({\n",
        "    \"question\": \"What is 13 raised to the .3432 power?\"\n",
        "})\n",
        "print(response)  # The LLM tries to calculate it directly and gets it wrong!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56da4f48",
      "metadata": {},
      "source": [
        "This demonstrates the power of prompting in LCEL: by carefully designing our prompts, we can guide the LLM's behavior precisely.\n",
        "\n",
        "The beauty of LCEL's sequential composition is how clearly we can see each step in the chain:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0fb5f537",
      "metadata": {},
      "outputs": [],
      "source": [
        "math_chain = (\n",
        "    prompt                         # Step 1: Format the input with our system message\n",
        "    | ChatOpenAI(temperature=0)    # Step 2: Get mathematical expression from LLM\n",
        "    | StrOutputParser()            # Step 3: Convert to clean string\n",
        "    | calculate                    # Step 4: Evaluate the expression\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4ea2a2",
      "metadata": {},
      "source": [
        "Each step flows naturally into the next using the `|` operator, making it easy to understand and modify the chain's behavior. This is much more flexible than the old approach of using predefined chain classes - we can easily add, remove, or modify steps as needed!\n",
        "\n",
        "*_Note: The `calculate` function uses `numexpr` to safely evaluate mathematical expressions without needing a full Python REPL (Read-Eval-Print Loop)._"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f66a25a2",
      "metadata": {
        "id": "f66a25a2"
      },
      "source": [
        "### Building Complex Chains with LCEL\n",
        "\n",
        "Let's build a more complex example that shows how to combine different components using LCEL. We'll create a chain that cleans up messy text and then paraphrases it in a specific style.\n",
        "\n",
        "First, let's create a function to clean up text by removing extra spaces and newlines. In LCEL, we can use regular functions directly in our chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "41d95063",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    # replace multiple new lines and multiple spaces with a single one\n",
        "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ca3a11",
      "metadata": {},
      "source": [
        "Now, let's create our prompt template for the paraphrasing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "71a5596e",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a creative writing assistant.\"),\n",
        "    (\"user\", \"\"\"Please paraphrase this text in the style of {style}:\n",
        "\n",
        "{text}\"\"\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5ff6c9",
      "metadata": {},
      "source": [
        "Now we can combine everything into a sequential chain using LCEL's `|` operator. The beauty of LCEL is how naturally we can compose these components:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "268da3a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yo, check it â€“ chains in the mix,\n",
            "Bringin' together pieces quick,\n",
            "Like a puzzle, makin' it slick,\n",
            "One app, one vibe, one click.\n",
            "\n",
            "User input start the flow,\n",
            "PromptTemplate style it, let it show,\n",
            "LLM takes it, makes it grow,\n",
            "Chains connect, makin' moves, yo.\n",
            "\n",
            "Complex chains, we ain't playin',\n",
            "Combining links, they stay swayin',\n",
            "Mix it up, keep it innovatin',\n",
            "Chains and components, we stay creatin'.\n"
          ]
        }
      ],
      "source": [
        "# Create the chain using LCEL\n",
        "style_chain = (\n",
        "    {\n",
        "        \"text\": lambda x: clean_text(x[\"text\"]),  # Extract and clean the text from input dict\n",
        "        \"style\": lambda x: x[\"style\"]  # Extract style from input dict\n",
        "    }\n",
        "    | prompt  # Format with our template\n",
        "    | ChatOpenAI(temperature=0.7)  # Generate creative paraphrase\n",
        "    | StrOutputParser()  # Convert to string\n",
        ")\n",
        "\n",
        "# Our input text with messy spacing\n",
        "input_text = \"\"\"\n",
        "Chains allow us to combine multiple \n",
        "\n",
        "\n",
        "components together to create a single, coherent application. \n",
        "\n",
        "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
        "\n",
        "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
        "\n",
        "\n",
        "combining chains with other components.\n",
        "\"\"\"\n",
        "\n",
        "# Run the chain\n",
        "response = style_chain.invoke({\n",
        "    \"text\": input_text,\n",
        "    \"style\": \"a 90s rapper\"\n",
        "})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b1f98f8",
      "metadata": {},
      "source": [
        "Let's look at how this chain works:\n",
        "\n",
        "1. The dictionary `{\"text\": clean_text, \"style\": lambda x: x}` processes our inputs in parallel using `RunnableParallel`\n",
        "2. The `|` operator connects each component, showing the clear flow of data\n",
        "3. Each step in the chain serves a specific purpose and is easily modifiable\n",
        "4. The components work together seamlessly to process and transform the text\n",
        "\n",
        "This demonstrates how LCEL lets us compose simple components into powerful chains while keeping the code readable and maintainable. Whether you're processing text, generating content, or building complex workflows, LCEL's composition primitives make it easy to build exactly what you need! ðŸ”¥"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2cc688ca",
      "metadata": {},
      "source": [
        "That's it for this example on chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pinecone1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
