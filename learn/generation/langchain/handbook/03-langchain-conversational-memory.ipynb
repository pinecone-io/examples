{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc93d05f",
      "metadata": {
        "id": "cc93d05f"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hcqKO0aI6_PI",
      "metadata": {
        "id": "hcqKO0aI6_PI"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Conversational Memory\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows a _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* â€” meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore this form of memory in the context of the LangChain library.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZR3iGJJtdDE",
        "outputId": "98873b1a-5688-4f64-c400-e17be707c56b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
        "                                                  ConversationSummaryMemory, \n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c02c4fa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02c4fa2",
        "outputId": "ed941db8-a50d-4e7d-d302-7b6b8c371c25"
      },
      "outputs": [],
      "source": [
        "# OPENAI_API_KEY = getpass() # SRA_DEBUGGING: Uncomment once finished testing.\n",
        "# TODO: SRA_DEBUGGING: Should we switch to env var?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "baaa74b8",
      "metadata": {
        "id": "baaa74b8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize with a modern model\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-4\" or other available models\n",
        "    temperature=0.7,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnNF6i9r8RY_",
      "metadata": {
        "id": "CnNF6i9r8RY_"
      },
      "source": [
        "Now let's dive into **Conversational Memory**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What is Memory?\n",
        "\n",
        "Memory is an agent's capacity of remembering previous interactions with the user (think chatbots). By default, chains are stateless, meaning they treat each incoming query independently. For applications like chatbots, it's crucial to remember previous interactions.\n",
        "\n",
        "Let's build a conversational chain using LCEL that demonstrates how memory works. We'll break it down step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6eb822",
      "metadata": {},
      "source": [
        "First, initialize a message history to store our conversations. This is a simple in-memory store that keeps track of all messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "96ff1ce3",
      "metadata": {
        "id": "96ff1ce3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.memory import ChatMessageHistory\n",
        "history = ChatMessageHistory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f9546e",
      "metadata": {},
      "source": [
        "Create a prompt template that structures our conversation\n",
        "This template has three parts:\n",
        "1. A system message that sets the AI's behavior\n",
        "2. A placeholder for our conversation history\n",
        "3. The user's current input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4eb766cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    # 1. System message sets the behavior\n",
        "    (\"system\", \"You are a friendly and helpful AI assistant. Be truthful and concise.\"),\n",
        "    # 2. MessagesPlaceholder will contain all previous messages\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    # 3. The current user input\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3348e13b",
      "metadata": {},
      "source": [
        "Let's examine what our prompt template contains:\n",
        "\n",
        "First, we see the input variables that our prompt expects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ceb125d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt input variables: ['chat_history', 'input']\n"
          ]
        }
      ],
      "source": [
        "print(f\"prompt input variables: {prompt.input_variables}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69db7502",
      "metadata": {},
      "source": [
        "This shows us that our prompt needs two pieces of information to work:\n",
        "- `chat_history`: Where our previous conversation messages will go\n",
        "- `input`: Where the user's current message will go"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49af8ec4",
      "metadata": {},
      "source": [
        "Next, let's look at the actual structure of our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "9d5cc635",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. System Message:\n",
            "Type: SystemMessagePromptTemplate\n",
            "Template: You are a friendly and helpful AI assistant. Be truthful and concise.\n",
            "\n",
            "2. Messages Placeholder:\n",
            "Type: MessagesPlaceholder\n",
            "Variable Name: chat_history\n",
            "\n",
            "3. Human Message Template:\n",
            "Type: HumanMessagePromptTemplate\n",
            "Template: {input}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Show the first component (System Message)\n",
        "print(\"\\n1. System Message:\")\n",
        "print(f\"Type: {type(prompt.messages[0]).__name__}\")\n",
        "print(f\"Template: {prompt.messages[0].prompt.template}\")\n",
        "\n",
        "# Show the second component (Messages Placeholder)\n",
        "print(\"\\n2. Messages Placeholder:\")\n",
        "print(f\"Type: {type(prompt.messages[1]).__name__}\")\n",
        "print(f\"Variable Name: {prompt.messages[1].variable_name}\")\n",
        "\n",
        "# Show the third component (Human Message Template)\n",
        "print(\"\\n3. Human Message Template:\")\n",
        "print(f\"Type: {type(prompt.messages[2]).__name__}\")\n",
        "print(f\"Template: {prompt.messages[2].prompt.template}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c1477c",
      "metadata": {},
      "source": [
        "This output reveals the three-part structure of our conversation template:\n",
        "\n",
        "1. A **System Message** that gives the AI its instructions and personality. This is fixed and doesn't change between conversations.\n",
        "\n",
        "2. A **Messages Placeholder** named `chat_history` - this is where all previous messages in the conversation will be inserted. This helps the AI remember what was discussed before.\n",
        "\n",
        "3. A **Human Message Template** that takes the current `{input}` - this is where each new user message will go.\n",
        "\n",
        "This structure ensures that every conversation includes:\n",
        "- Consistent AI behavior (via the system message)\n",
        "- Full conversation context (via chat_history)\n",
        "- The current user's input (via the input template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac58524f",
      "metadata": {},
      "source": [
        "Now let's build our chain using LCEL's composition syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "2dc2c4bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\n",
        "        # Extract the user's input from the incoming dictionary\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        # Get the current conversation history\n",
        "        \"chat_history\": lambda x: history.messages\n",
        "    }\n",
        "    | prompt  # Format everything into our prompt template\n",
        "    | llm     # Send to the LLM for response\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e09d51",
      "metadata": {},
      "source": [
        "Using LCEL's `|` operator, we clearly show the flow of data:\n",
        "   - First, prepare the input data and history\n",
        "   - Then, format it into our prompt\n",
        "   - Finally, send it to the LLM for response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44bc741",
      "metadata": {},
      "source": [
        "Let's use our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c27d65f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Input: Hi! How are you? Also remember the number 42, it's important, or so I've been told.\n",
            "Response: Hello! I'm here and ready to help. Thank you for sharing the number 42 with me. If you have any questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Send a message and get a response\n",
        "user_input = \"Hi! How are you? Also remember the number 42, it's important, or so I've been told.\"\n",
        "print(f\"User Input: {user_input}\")\n",
        "response = chain.invoke({\"input\": f\"{user_input}\"})\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4fa8c5",
      "metadata": {},
      "source": [
        "We explicitly update our history after each interaction, giving us full control over what gets remembered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5b076df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update our conversation history with both messages\n",
        "history.add_user_message(\"Hi! How are you?\")\n",
        "history.add_ai_message(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541c0aa9",
      "metadata": {},
      "source": [
        "Now `history` contains both messages and will be included in the next interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "fef1a8ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of messages in history: 4\n",
            "\n",
            "First message (User):\n",
            "Type: HumanMessage\n",
            "Content: Hi! How are you?\n",
            "\n",
            "Second message (AI):\n",
            "Type: AIMessage\n",
            "Content: Hello! I'm here and ready to help. Thank you for sharing the number 42 with me. If you have any questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Show the total number of messages stored\n",
        "print(f\"Number of messages in history: {len(history.messages)}\")\n",
        "\n",
        "# Show the first message (user's message)\n",
        "print(\"\\nFirst message (User):\")\n",
        "print(f\"Type: {type(history.messages[0]).__name__}\")\n",
        "print(f\"Content: {history.messages[0].content}\")\n",
        "\n",
        "# Show the second message (AI's response)\n",
        "print(\"\\nSecond message (AI):\")\n",
        "print(f\"Type: {type(history.messages[1]).__name__}\")\n",
        "print(f\"Content: {history.messages[1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534d0652",
      "metadata": {},
      "source": [
        "You can continue the conversation by invoking the chain again - it will automatically include the previous interaction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "34213e25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Input: What was the number in my first message to you?\n",
            "Response: The number you shared in your first message was 42. If you have any more questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation\n",
        "user_input =  \"What was the number in my first message to you?\"\n",
        "print(f\"User Input: {user_input}\")\n",
        "response2 = chain.invoke({\"input\": f\"{user_input}\"})\n",
        "print(f\"Response: {response2.content}\")\n",
        "history.add_user_message(\"What was my first message to you?\")\n",
        "history.add_ai_message(response2.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8adaa7b3",
      "metadata": {},
      "source": [
        "We can now check the updated `history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ec1576f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of messages in history: 4\n",
            "\n",
            "First message (User):\n",
            "Type: HumanMessage\n",
            "Content: Hi! How are you?\n",
            "\n",
            "Second message (AI):\n",
            "Type: AIMessage\n",
            "Content: Hello! I'm here and ready to help. Thank you for sharing the number 42 with me. If you have any questions or need assistance, feel free to ask!\n",
            "\n",
            "Third message (User):\n",
            "Type: HumanMessage\n",
            "Content: What was my first message to you?\n",
            "\n",
            "Fourth message (AI):\n",
            "Type: AIMessage\n",
            "Content: The number you shared in your first message was 42. If you have any more questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Show the total number of messages stored\n",
        "print(f\"Number of messages in history: {len(history.messages)}\")\n",
        "\n",
        "# Show the first message (user's message)\n",
        "print(\"\\nFirst message (User):\")\n",
        "print(f\"Type: {type(history.messages[0]).__name__}\")\n",
        "print(f\"Content: {history.messages[0].content}\")\n",
        "\n",
        "# Show the second message (AI's response)\n",
        "print(\"\\nSecond message (AI):\")\n",
        "print(f\"Type: {type(history.messages[1]).__name__}\")\n",
        "print(f\"Content: {history.messages[1].content}\")\n",
        "\n",
        "# Show the first message (user's message)\n",
        "print(\"\\nThird message (User):\")\n",
        "print(f\"Type: {type(history.messages[2]).__name__}\")\n",
        "print(f\"Content: {history.messages[2].content}\")\n",
        "\n",
        "# Show the second message (AI's response)\n",
        "print(\"\\nFourth message (AI):\")\n",
        "print(f\"Type: {type(history.messages[3]).__name__}\")\n",
        "print(f\"Content: {history.messages[3].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1a33f6",
      "metadata": {
        "id": "0f1a33f6"
      },
      "source": [
        "## Memory types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d732b7a",
      "metadata": {
        "id": "4d732b7a"
      },
      "source": [
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Memory in LCEL: Using RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d3cb2b",
      "metadata": {
        "id": "53d3cb2b"
      },
      "source": [
        "The modern approach to conversation memory in LangChain uses `RunnableWithMessageHistory` to maintain a history of the conversation. This can be used in place of the the - now deprecated - `ConversationBufferMemory` class. Both maintain history automatically, without the need for manual user updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80a974a",
      "metadata": {
        "id": "d80a974a"
      },
      "source": [
        "**Key Features of RunnableWithMessageHistory:**\n",
        "1. **Complete Message Preservation**\n",
        "   - Every message is stored exactly as it was sent or received\n",
        "   - No summarization, modification, or alteration of the content\n",
        "   - Maintains the full context of sessions\n",
        "\n",
        "2. **Automatic History Management**\n",
        "   - New messages are automatically added to the conversation history\n",
        "   - Previous messages are automatically included in the context for each new interaction\n",
        "   - Each session maintains its own separate history\n",
        "\n",
        "Think of it like a perfect secretary who:\n",
        "- Records every word exactly as spoken\n",
        "- Automatically keeps track of who said what and when\n",
        "- Makes sure each sessions's history is available for the next interaction\n",
        "- Keeps different sessions separate and organized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb14f56",
      "metadata": {},
      "source": [
        "Let's set up our chain with memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "292ce553",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "# Create a store to maintain histories\n",
        "message_histories = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in message_histories:\n",
        "        message_histories[session_id] = ChatMessageHistory()\n",
        "    return message_histories[session_id]\n",
        "\n",
        "# Create our components\n",
        "llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly and helpful AI assistant. Be truthful and concise.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create our chain with history\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    prompt | llm,\n",
        "    get_session_history,  # Use our store-based getter\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\"\n",
        ")\n",
        "\n",
        "# Start a conversation session\n",
        "session_id = \"demo_session\"\n",
        "\n",
        "def count_tokens(chain, message):\n",
        "    \"\"\"Count tokens used in a conversation turn\"\"\"\n",
        "    response = chain.invoke(\n",
        "        {\"input\": message},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    return response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ddcb7e",
      "metadata": {},
      "source": [
        "Let's try some interactions and examine the token usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "58da2dea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First User Input: Good morning AI! Remember, Remember, the 5th of November! Gumdrops, treacle and pop!\n",
            "First Response: Good morning! It seems like you're referencing the UK traditional rhyme \"Remember, Remember the 5th of November\" related to Guy Fawkes Night. Have a great day!\n",
            "\n",
            "Conversation History after first message:\n",
            "human: Good morning AI! Remember, Remember, the 5th of November! Gumdrops, treacle and pop!\n",
            "ai: Good morning! It seems like you're referencing the UK traditional rhyme \"Remember, Remember the 5th of November\" related to Guy Fawkes Night. Have a great day!\n",
            "\n",
            "Second response: You mentioned the date \"5th of November\" and the foods \"gumdrops, treacle, and pop.\"\n"
          ]
        }
      ],
      "source": [
        "# Try some conversations\n",
        "first_user_msg = \"Good morning AI! Remember, Remember, the 5th of November! Gumdrops, treacle and pop!\"\n",
        "print(f\"First User Input: {first_user_msg}\")\n",
        "response = count_tokens(chain_with_history, f\"{first_user_msg}\")\n",
        "print(\"First Response:\", response.content)\n",
        "\n",
        "# Get the history after the first message\n",
        "history = chain_with_history.get_session_history(session_id)\n",
        "print(\"\\nConversation History after first message:\")\n",
        "for message in history.messages:\n",
        "    print(f\"{message.type}: {message.content}\")\n",
        "\n",
        "# Try another message\n",
        "response = count_tokens(chain_with_history, \"I mentioned a date and some foods previously, what were they?\")\n",
        "print(\"\\nSecond response:\", response.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0411d739",
      "metadata": {},
      "source": [
        "Our LLM with message history can remember earlier interactions in the conversation. Let's examine how the conversation is being saved. We can do this by accessing the message history for our session:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "d10a8194",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conversation History after second message:\n",
            "human: Good morning AI! Remember, Remember, the 5th of November! Gumdrops, treacle and pop!\n",
            "ai: Good morning! It seems like you're referencing the UK traditional rhyme \"Remember, Remember the 5th of November\" related to Guy Fawkes Night. Have a great day!\n",
            "human: I mentioned a date and some foods previously, what were they?\n",
            "ai: You mentioned the date \"5th of November\" and the foods \"gumdrops, treacle, and pop.\"\n"
          ]
        }
      ],
      "source": [
        "# Get the history after the second message\n",
        "history = chain_with_history.get_session_history(session_id)\n",
        "print(\"\\nConversation History after second message:\")\n",
        "for message in history.messages:\n",
        "    print(f\"{message.type}: {message.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64867abc",
      "metadata": {},
      "source": [
        "Nice! So every piece of our conversation is being explicitly recorded and made available to the LLM through the `MessagesPlaceholder` in our prompt.\n",
        "\n",
        "The main differences from the old approach are:\n",
        "1. Uses modern LCEL composition with the `|` operator\n",
        "2. Supports multiple conversation sessions through `session_id`\n",
        "3. More explicit control over how history is stored and accessed\n",
        "4. Better integration with other LCEL components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf1a90b",
      "metadata": {
        "id": "acf1a90b"
      },
      "source": [
        "### Memory type #2: Summarization Memory with LCEL\n",
        "\n",
        "The problem with storing complete conversation history is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed.\n",
        "Key feature: Using summarization to maintain a condensed version of the conversation history while keeping token usage under control.\n",
        "Here's how we can implement this using modern LCEL:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defbab39",
      "metadata": {},
      "source": [
        "TODO: SRA_DEBUGGING: The following is just using ChatMessageHistory again, and itsn't really a proper substitute for `ConversationSummaryMemory`. The latter doesn't seem to have a proper LCEL substitute for now. See: https://github.com/langchain-ai/langchain/discussions/25904 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c458168c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.schema import SystemMessage\n",
        "import tiktoken\n",
        "\n",
        "# Create a message history to store our conversations\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "# Create our chat prompt template with a system message and history\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),  # Where history is injected\n",
        "    (\"human\", \"{input}\")  # The latest user input\n",
        "])\n",
        "\n",
        "# Create our summarization prompt\n",
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
        "    \n",
        "    EXAMPLE\n",
        "    Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
        "    \n",
        "    New lines of conversation:\n",
        "    Human: Why do you think artificial intelligence is a force for good?\n",
        "    AI: Because artificial intelligence will help humans reach their full potential.\n",
        "    \n",
        "    New summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
        "    END OF EXAMPLE\n",
        "    \n",
        "    Current summary: {current_summary}\n",
        "    \n",
        "    New lines of conversation:\n",
        "    {new_lines}\n",
        "    \n",
        "    New summary:\"\"\")\n",
        "])\n",
        "\n",
        "# Create our LLM\n",
        "# Initialize with a modern model\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-4\" or other available models\n",
        "    temperature=0,\n",
        "    \n",
        ")\n",
        "\n",
        "# Function to summarize new messages\n",
        "def summarize_messages(current_summary: str, new_messages: list) -> str:\n",
        "    # Format new messages into text\n",
        "    new_lines = \"\\n\".join([\n",
        "        f\"{'Human' if msg.type == 'human' else 'AI'}: {msg.content}\" \n",
        "        for msg in new_messages\n",
        "    ])\n",
        "    \n",
        "    # Get new summary from LLM\n",
        "    chain = summary_prompt | llm\n",
        "    result = chain.invoke({\n",
        "        \"current_summary\": current_summary,\n",
        "        \"new_lines\": new_lines\n",
        "    })\n",
        "    return result.content\n",
        "\n",
        "# Create our conversation chain\n",
        "chain = (\n",
        "    prompt \n",
        "    | llm\n",
        ")\n",
        "\n",
        "# Helper function to count tokens\n",
        "def count_tokens(text: str) -> int:\n",
        "    tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# Example usage:\n",
        "summary = \"\"  # Start with empty summary\n",
        "messages_buffer = []  # Buffer for recent messages\n",
        "\n",
        "def chat(user_input: str) -> str:\n",
        "    global summary, messages_buffer\n",
        "    \n",
        "    # Add user message to history and buffer\n",
        "    history.add_user_message(user_input)\n",
        "    messages_buffer.append(history.messages[-1])\n",
        "    \n",
        "    # Get AI response\n",
        "    response = chain.invoke({\n",
        "        \"input\": user_input,\n",
        "        \"chat_history\": history.messages\n",
        "    })\n",
        "    \n",
        "    # Add AI response to history and buffer\n",
        "    history.add_ai_message(response.content)\n",
        "    messages_buffer.append(history.messages[-1])\n",
        "    \n",
        "    # If buffer gets too large, summarize older messages\n",
        "    if len(messages_buffer) > 4:  # Adjust this number as needed\n",
        "        summary = summarize_messages(summary, messages_buffer)\n",
        "        messages_buffer = []  # Clear buffer after summarizing\n",
        "        \n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598546f7",
      "metadata": {},
      "source": [
        "This implementation offers several advantages:\n",
        "1. Uses LCEL's pipe operator (|) for cleaner chain composition\n",
        "2. Leverages ChatPromptTemplate for better prompt management\n",
        "3. Separates the summarization logic into a dedicated function\n",
        "4. Maintains both a summary and a buffer of recent messages\n",
        "5. Gives you explicit control over when summarization happens\n",
        "\n",
        "You can use it like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "db182a88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First User Input: Good morning AI! Which deadma5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\n",
            "First Response: The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"\n",
            "\n",
            "Second User Input: That's incorrect! The song is 'I Remember', featuring Kaskade. Remined me, what were the lyrics I gave you as a clue?\n",
            "Second Response: I apologize for the mistake. The lyrics you gave me as a clue were \"Feeling the past moving in. Letting a new day begin.\"\n",
            "\n",
            "Summary token count: 127\n",
            "Recent messages token count: 119\n"
          ]
        }
      ],
      "source": [
        "# Example conversation, ask about the lyrics to the song 'I Remember' by deadmau5 and Kaskade\n",
        "first_user_msg = \"Good morning AI! Which deadma5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\" \n",
        "print(f\"First User Input: {first_user_msg}\")\n",
        "response = chat(first_user_msg)\n",
        "print(f\"First Response: {response}\")\n",
        "\n",
        "response = response.lower()\n",
        "if \"i remember\" in response:\n",
        "    second_user_msg = \"That's correct! Remined me, what were the lyrics I gave you as a clue?\" \n",
        "else:\n",
        "    second_user_msg = \"That's incorrect! The song is 'I Remember', featuring Kaskade. Remined me, what were the lyrics I gave you as a clue?\" \n",
        "\n",
        "\n",
        "print(f\"\\nSecond User Input: {second_user_msg}\")\n",
        "response = chat(second_user_msg)\n",
        "print(f\"Second Response: {response}\")\n",
        "\n",
        "\n",
        "# Check token counts\n",
        "print(f'\\nSummary token count: {count_tokens(summary)}')\n",
        "print(f'Recent messages token count: {sum(count_tokens(msg.content) for msg in messages_buffer)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adea584d",
      "metadata": {},
      "source": [
        "*Practical Note: Modern LLMs like GPT-3.5-turbo and GPT-4 have context windows of 16K-128K tokens, but it's still important to manage token usage efficiently, both for cost reasons and to ensure the most relevant context is emphasized.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24e9b48",
      "metadata": {},
      "source": [
        "TOOD: SRA_DEBUGGING: An attempt at implementing an alternative that uses `RunnableWithMessageHistory`. Check if this approach is okay.\n",
        "TOOD: SRA_DEBUGGING: Break down and explain each step if it is okay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "57b249be",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "# Create stores for histories and summaries\n",
        "message_histories = {}\n",
        "summaries = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    \"\"\"Initialize or retrieve session history\"\"\"\n",
        "    if session_id not in message_histories:\n",
        "        message_histories[session_id] = ChatMessageHistory()\n",
        "        summaries[session_id] = \"\"\n",
        "    return message_histories[session_id]\n",
        "\n",
        "def print_memory_state(session_id=\"default\"):\n",
        "    \"\"\"Print the current state of memory including summary and recent messages\"\"\"\n",
        "    print(\"\\nCurrent Memory State:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if session_id in summaries:\n",
        "        summary = summaries[session_id]\n",
        "        if summary:\n",
        "            print(\"\\nSummary:\")\n",
        "            print(\"-\" * 20)\n",
        "            print(summary)\n",
        "    \n",
        "    if session_id in message_histories:\n",
        "        history = message_histories[session_id]\n",
        "        print(\"\\nRecent Messages:\")\n",
        "        print(\"-\" * 20)\n",
        "        for i, msg in enumerate(history.messages, 1):\n",
        "            print(f\"{i}. {msg.type}: {msg.content}\")\n",
        "    else:\n",
        "        print(\"\\nNo messages in history yet\")\n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(temperature=0.7)\n",
        "\n",
        "# Create prompts\n",
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Summarize the key points from this conversation, maintaining important details and context.\n",
        "    Current summary: {current_summary}\n",
        "    New conversation:\n",
        "    {new_lines}\n",
        "    New summary:\"\"\")\n",
        "])\n",
        "\n",
        "conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly and helpful AI assistant. Be truthful and concise.\"),\n",
        "    (\"system\", \"Previous conversation summary: {summary}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "def create_chain():\n",
        "    # Create base chain\n",
        "    chain = conversation_prompt | llm\n",
        "    \n",
        "    # Wrap with history\n",
        "    chain_with_history = RunnableWithMessageHistory(\n",
        "        chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\"\n",
        "    )\n",
        "    \n",
        "    return chain_with_history\n",
        "\n",
        "# Initialize chain\n",
        "chain = create_chain()\n",
        "\n",
        "def chat(message: str, session_id: str = \"default\"):\n",
        "    \"\"\"Send a message and get a response, managing history and summarization\"\"\"\n",
        "    # Ensure session exists\n",
        "    if session_id not in message_histories:\n",
        "        get_session_history(session_id)\n",
        "    \n",
        "    # Get response\n",
        "    response = chain.invoke(\n",
        "        {\"input\": message, \"summary\": summaries[session_id]},\n",
        "        {\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    \n",
        "    # After response, check if we need to summarize\n",
        "    history = message_histories[session_id]\n",
        "    if len(history.messages) > 4:  # Summarize after every 4 messages\n",
        "        messages_to_summarize = history.messages[:-2]  # Keep last 2 messages fresh\n",
        "        \n",
        "        # Format messages for summarization\n",
        "        new_lines = \"\\n\".join([\n",
        "            f\"{'Human' if msg.type == 'human' else 'AI'}: {msg.content}\"\n",
        "            for msg in messages_to_summarize\n",
        "        ])\n",
        "        \n",
        "        # Get new summary\n",
        "        summary_chain = summary_prompt | llm\n",
        "        summary_result = summary_chain.invoke({\n",
        "            \"current_summary\": summaries[session_id],\n",
        "            \"new_lines\": new_lines\n",
        "        })\n",
        "        \n",
        "        # Update summary and trim history\n",
        "        summaries[session_id] = summary_result.content\n",
        "        history.messages = history.messages[-2:]\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f9294f9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1: Initial Question\n",
            "Response: content='The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 62, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShRHJdJrWMCIoMmYBfa7FLuIG4x', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e61658eb-cff0-4bf1-8e63-d542fa7eeba0-0' usage_metadata={'input_tokens': 62, 'output_tokens': 31, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: Good morning AI! Which deadmau5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\n",
            "2. ai: The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 2: Follow-up Question\n",
            "Response: content='The lyrics you quoted are from the song \"I Remember\" by deadmau5 and Kaskade.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 121, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShSry1hovj1fqi9d1mnG9S7GR9g', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c464d91c-fa97-4884-a05e-5f22904d422c-0' usage_metadata={'input_tokens': 121, 'output_tokens': 22, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: Good morning AI! Which deadmau5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\n",
            "2. ai: The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"\n",
            "3. human: That's correct if you said 'I Remember'! Can you remind me what lyrics I quoted?\n",
            "4. ai: The lyrics you quoted are from the song \"I Remember\" by deadmau5 and Kaskade.\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 3: Adding More Context\n",
            "Response: content='\"I Remember\" is a collaboration between deadmau5 and Kaskade.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 162, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShTjFEmPatBUIfqcIRFUWOJoU0w', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--87196242-96b9-4860-989a-89405d7fba80-0' usage_metadata={'input_tokens': 162, 'output_tokens': 16, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human asked the AI about a deadmau5 song with specific lyrics, and the AI initially mentioned \"Strobe\" as the song, but the human corrected it to be \"I Remember.\" The AI then correctly identified the lyrics quoted by the human from the song \"I Remember\" by deadmau5 and Kaskade.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: Who collaborated with deadmau5 on this song?\n",
            "2. ai: \"I Remember\" is a collaboration between deadmau5 and Kaskade.\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 4: Testing Memory\n",
            "Response: content='You initially asked about a deadmau5 song with specific lyrics.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 146, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShUeqbR5r1UYdr35pVtMaJRQf38', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--72dfb472-9cc6-49b4-8fc0-d44cde380ca7-0' usage_metadata={'input_tokens': 146, 'output_tokens': 14, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human asked the AI about a deadmau5 song with specific lyrics, and the AI initially mentioned \"Strobe\" as the song, but the human corrected it to be \"I Remember.\" The AI then correctly identified the lyrics quoted by the human from the song \"I Remember\" by deadmau5 and Kaskade.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: Who collaborated with deadmau5 on this song?\n",
            "2. ai: \"I Remember\" is a collaboration between deadmau5 and Kaskade.\n",
            "3. human: What was the first thing I asked you about?\n",
            "4. ai: You initially asked about a deadmau5 song with specific lyrics.\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 5: Adding New Information\n",
            "Response: content='Got it! \"I Remember\" was released in 2008 as part of deadmau5\\'s album \\'Random Album Title\\'.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 194, 'total_tokens': 221, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShVvWIrlzshRMD1kPerctmxnno7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--69ae0d65-a21e-4ccd-89da-0171167b1871-0' usage_metadata={'input_tokens': 194, 'output_tokens': 27, 'total_tokens': 221, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human inquired about the collaboration on the song \"I Remember,\" to which the AI correctly stated it was a collaboration between deadmau5 and Kaskade. The human then questioned what they had initially asked about, prompting the AI to recall the human's inquiry regarding a deadmau5 song with specific lyrics.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: The song was released in 2008 as part of deadmau5's album 'Random Album Title'. Remember this fact.\n",
            "2. ai: Got it! \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title'.\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 6: Testing Long-term Memory\n",
            "Response: content='The song \"I Remember\" was released in 2008 as part of deadmau5\\'s album \\'Random Album Title\\'.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 169, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShXh1watI7ZWx2kZX4rL3uXYNaA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--052a23a3-2710-4546-b874-48fbaf89556a-0' usage_metadata={'input_tokens': 169, 'output_tokens': 26, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human inquired about the collaboration on the song \"I Remember,\" to which the AI correctly stated it was a collaboration between deadmau5 and Kaskade. The human then questioned what they had initially asked about, prompting the AI to recall the human's inquiry regarding a deadmau5 song with specific lyrics.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: The song was released in 2008 as part of deadmau5's album 'Random Album Title'. Remember this fact.\n",
            "2. ai: Got it! \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title'.\n",
            "3. human: When was the song released and in which album?\n",
            "4. ai: The song \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title'.\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 7: Adding More Context\n",
            "Response: content='The opening lyrics of the song \"I Remember\" by deadmau5 and Kaskade are: \"Feeling the past moving in / Letting a new day begin.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 225, 'total_tokens': 261, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShYXRVuvQQYPtn1UZdaASlWWr7t', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9a62e6ae-c80b-433e-b83d-9eb4826558ce-0' usage_metadata={'input_tokens': 225, 'output_tokens': 36, 'total_tokens': 261, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human emphasized that the song \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title,' prompting the AI to reiterate this fact.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: The song's music video features time-lapse footage of cityscapes. What were those opening lyrics again?\n",
            "2. ai: The opening lyrics of the song \"I Remember\" by deadmau5 and Kaskade are: \"Feeling the past moving in / Letting a new day begin.\"\n",
            "==================================================\n",
            "\n",
            "\n",
            "Test 8: Final Memory Test\n",
            "Response: content='\"I Remember\" is a collaborative track by deadmau5 and Kaskade released in 2008 as part of deadmau5\\'s album \\'Random Album Title.\\' The song features the opening lyrics \"Feeling the past moving in / Letting a new day begin\" and its music video includes time-lapse footage of cityscapes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 159, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BaShaf6UXQDfLddku4fGFpcvVocHM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d4e1ac02-a7a3-48a5-a3b9-5f268c125f10-0' usage_metadata={'input_tokens': 159, 'output_tokens': 70, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Current Memory State:\n",
            "==================================================\n",
            "\n",
            "Summary:\n",
            "--------------------\n",
            "The human emphasized that the song \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title,' prompting the AI to reiterate this fact.\n",
            "\n",
            "Recent Messages:\n",
            "--------------------\n",
            "1. human: The song's music video features time-lapse footage of cityscapes. What were those opening lyrics again?\n",
            "2. ai: The opening lyrics of the song \"I Remember\" by deadmau5 and Kaskade are: \"Feeling the past moving in / Letting a new day begin.\"\n",
            "3. human: Let's recap: What do you remember about the song 'I Remember' by deadmau5?\n",
            "4. ai: \"I Remember\" is a collaborative track by deadmau5 and Kaskade released in 2008 as part of deadmau5's album 'Random Album Title.' The song features the opening lyrics \"Feeling the past moving in / Letting a new day begin\" and its music video includes time-lapse footage of cityscapes.\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_memory_state(session_id=\"default\"):\n",
        "    print(\"\\nCurrent Memory State:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Print summary if it exists\n",
        "    summary = summaries.get(session_id, \"\")\n",
        "    if summary:\n",
        "        print(\"\\nSummary:\")\n",
        "        print(\"-\" * 20)\n",
        "        print(summary)\n",
        "    \n",
        "    # Print current messages in history\n",
        "    history = message_histories[session_id]\n",
        "    print(\"\\nRecent Messages:\")\n",
        "    print(\"-\" * 20)\n",
        "    for i, msg in enumerate(history.messages, 1):\n",
        "        print(f\"{i}. {msg.type}: {msg.content}\")\n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Let's test with a conversation about deadmau5\n",
        "print(\"Test 1: Initial Question\")\n",
        "response = chat(\"Good morning AI! Which deadmau5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 2: Follow-up Question\")\n",
        "response = chat(\"That's correct if you said 'I Remember'! Can you remind me what lyrics I quoted?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 3: Adding More Context\")\n",
        "response = chat(\"Who collaborated with deadmau5 on this song?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 4: Testing Memory\")\n",
        "response = chat(\"What was the first thing I asked you about?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 5: Adding New Information\")\n",
        "response = chat(\"The song was released in 2008 as part of deadmau5's album 'Random Album Title'. Remember this fact.\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 6: Testing Long-term Memory\")\n",
        "response = chat(\"When was the song released and in which album?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 7: Adding More Context\")\n",
        "response = chat(\"The song's music video features time-lapse footage of cityscapes. What were those opening lyrics again?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()\n",
        "\n",
        "print(\"\\nTest 8: Final Memory Test\")\n",
        "response = chat(\"Let's recap: What do you remember about the song 'I Remember' by deadmau5?\")\n",
        "print(f\"Response: {response}\")\n",
        "print_memory_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222c3f38",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Initial State (Tests 1-2)**\n",
        "\n",
        "In Test 1, we see only the first exchange stored with no summary:\n",
        "```\n",
        "Current Memory State:\n",
        "Recent Messages:\n",
        "1. human: Good morning AI! Which deadmau5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\n",
        "2. ai: The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"\n",
        "```\n",
        "\n",
        "By Test 2, we accumulate 4 messages but still no summary (as expected, since we haven't exceeded our threshold):\n",
        "```\n",
        "Recent Messages:\n",
        "1. human: Good morning AI! Which deadmau5 song starts with 'Feeling the past moving in. Letting a new day begin.'?\n",
        "2. ai: The deadmau5 song that starts with the lyrics \"Feeling the past moving in. Letting a new day begin.\" is \"Strobe.\"\n",
        "3. human: That's correct if you said 'I Remember'! Can you remind me what lyrics I quoted?\n",
        "4. ai: The lyrics you quoted are from the song \"I Remember\" by deadmau5 and Kaskade.\n",
        "```\n",
        "\n",
        "**First Summarization Trigger (Test 3)**\n",
        "\n",
        "When we add the fifth message, we see our first summarization:\n",
        "```\n",
        "Summary:\n",
        "The human asked the AI about a deadmau5 song with specific lyrics, and the AI initially mentioned \"Strobe\" as the song, but the human corrected it to be \"I Remember.\" The AI then correctly identified the lyrics quoted by the human from the song \"I Remember\" by deadmau5 and Kaskade.\n",
        "\n",
        "Recent Messages:\n",
        "1. human: Who collaborated with deadmau5 on this song?\n",
        "2. ai: \"I Remember\" is a collaboration between deadmau5 and Kaskade.\n",
        "```\n",
        "\n",
        "This shows the system correctly:\n",
        "1. Summarized the older conversation\n",
        "2. Kept only the most recent exchange\n",
        "3. Maintained the key correction about the song name\n",
        "\n",
        "**Memory Integration (Tests 4-5)**\n",
        "\n",
        "In Test 4, when asked about the first question, the system combines summary and recent context:\n",
        "```\n",
        "Summary:\n",
        "The human asked the AI about a deadmau5 song with specific lyrics, and the AI initially mentioned \"Strobe\" as the song...\n",
        "\n",
        "Recent Messages:\n",
        "3. human: What was the first thing I asked you about?\n",
        "4. ai: You initially asked about a deadmau5 song with specific lyrics.\n",
        "```\n",
        "\n",
        "By Test 5, we see the summary updating with new information:\n",
        "```\n",
        "Summary:\n",
        "The human inquired about the collaboration on the song \"I Remember,\" to which the AI correctly stated it was a collaboration between deadmau5 and Kaskade...\n",
        "\n",
        "Recent Messages:\n",
        "1. human: The song was released in 2008 as part of deadmau5's album 'Random Album Title'. Remember this fact.\n",
        "2. ai: Got it! \"I Remember\" was released in 2008 as part of deadmau5's album 'Random Album Title'.\n",
        "```\n",
        "\n",
        "**Final Memory Integration (Tests 7-8)**\n",
        "\n",
        "By the end, we see complete integration of information. The final response combines facts from both summary and recent messages:\n",
        "```\n",
        "Recent Messages:\n",
        "3. human: Let's recap: What do you remember about the song 'I Remember' by deadmau5?\n",
        "4. ai: \"I Remember\" is a collaborative track by deadmau5 and Kaskade released in 2008 as part of deadmau5's album 'Random Album Title.' The song features the opening lyrics \"Feeling the past moving in / Letting a new day begin\" and its music video includes time-lapse footage of cityscapes.\"\n",
        "```\n",
        "\n",
        "This final response demonstrates successful retention of:\n",
        "- Release date and album (from earlier summary)\n",
        "- Opening lyrics (from recent messages)\n",
        "- Collaboration information (from earlier summary)\n",
        "- Music video details (from recent context)\n",
        "\n",
        "The system is clearly maintaining both long-term memory through summaries and short-term memory through recent messages, exactly as designed. Each summarization preserves key information while keeping the most recent context fully intact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494830ea",
      "metadata": {
        "id": "494830ea"
      },
      "source": [
        "### Memory type #3: ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00762844",
      "metadata": {
        "id": "00762844"
      },
      "source": [
        "Another great option for these cases is the `ConversationBufferWindowMemory` where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably. We will control this window with the `k` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206a5915",
      "metadata": {
        "id": "206a5915"
      },
      "source": [
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "45be373a",
      "metadata": {
        "id": "45be373a"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "fc4dd8a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fc4dd8a0",
        "outputId": "c4ec1cc8-f218-4f7b-e27e-f5fb73e59228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 85 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b9992e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "b9992e8d",
        "outputId": "ac7ae1af-2329-4766-ac5e-8fce24a1d272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 178 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "3f2e98d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "3f2e98d9",
        "outputId": "dc60726a-4be2-480f-892b-443da9b2859e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 233 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "a2a8d062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "a2a8d062",
        "outputId": "dbb27cf0-2e87-41d0-a733-68921d250481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 245 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Data sources that could be used to give context to the model include text corpora, structured databases, and ontologies. Text corpora provide a large amount of text data that can be used to train the model and provide additional context. Structured databases provide structured data that can be used to provide additional context to the model. Ontologies provide a structured representation of knowledge that can be used to provide additional context to the model.'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ff199a3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ff199a3f",
        "outputId": "81573cf0-7f39-4a8c-8ccd-e79cd80f2523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 186 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Your aim is to use data sources to give context to the model.'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f59f77",
      "metadata": {
        "id": "f5f59f77"
      },
      "source": [
        "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be `1`, we would expect it remembers only the last interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b354c8d",
      "metadata": {
        "id": "6b354c8d"
      },
      "source": [
        "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "85266406",
      "metadata": {
        "id": "85266406"
      },
      "outputs": [],
      "source": [
        "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
        "    inputs=[]\n",
        ")['history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "5904ae2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5904ae2a",
        "outputId": "bd0aa797-7a43-4af5-a531-209aa6272dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: What is my aim again?\n",
            "AI:  Your aim is to use data sources to give context to the model.\n"
          ]
        }
      ],
      "source": [
        "print(bufw_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8b937d",
      "metadata": {
        "id": "ae8b937d"
      },
      "source": [
        "Makes sense. \n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "9fbb50fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fbb50fe",
        "outputId": "c35dca36-a7c7-4d61-da19-c28173fa8319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 334\n",
            "Summary memory conversation length: 219\n",
            "Buffer window memory conversation length: 26\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69842cc1",
      "metadata": {
        "id": "69842cc1"
      },
      "source": [
        "_Practical Note: We are using `k=2` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aea5fc8",
      "metadata": {
        "id": "2aea5fc8"
      },
      "source": [
        "### More memory types!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daeb5162",
      "metadata": {
        "id": "daeb5162"
      },
      "source": [
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0365333",
      "metadata": {
        "id": "f0365333"
      },
      "source": [
        "#### ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317f298e",
      "metadata": {
        "id": "317f298e"
      },
      "source": [
        "**Key feature:** _the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57ef5c8b",
      "metadata": {
        "id": "57ef5c8b"
      },
      "source": [
        "#### ConversationKnowledgeGraphMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40248f03",
      "metadata": {
        "id": "40248f03"
      },
      "source": [
        "This is a super cool memory type that was introduced just [recently](https://twitter.com/LangChainAI/status/1625158388824043522). It is based on the concept of a _knowledge graph_ which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. If you want to understand this memory type in more depth you can check out [this](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph) blogpost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91952cd1",
      "metadata": {
        "id": "91952cd1"
      },
      "source": [
        "**Key feature:** _the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "02241bc3",
      "metadata": {
        "id": "02241bc3"
      },
      "outputs": [],
      "source": [
        "# you may need to install this library\n",
        "# !pip install -qU networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "c5f10a89",
      "metadata": {
        "id": "c5f10a89"
      },
      "outputs": [],
      "source": [
        "conversation_kg = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=ConversationKGMemory(llm=llm)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "65957fe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "65957fe2",
        "outputId": "c9561a4a-412a-4d92-865d-9e81a09bb101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1565 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hi Human! My name is AI. It's nice to meet you. I like mangoes too! Did you know that mangoes are a great source of vitamins A and C?\""
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_kg, \n",
        "    \"My name is human and I like mangoes!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74054534",
      "metadata": {
        "id": "74054534"
      },
      "source": [
        "The memory keeps a knowledge graph of everything it learned so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "5a8c54fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8c54fb",
        "outputId": "adf96679-087b-4b77-c00d-9bf9e98f9278"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('human', 'human', 'name'), ('human', 'mangoes', 'likes')]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_kg.memory.kg.get_triples()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a1ca15",
      "metadata": {
        "id": "e1a1ca15"
      },
      "source": [
        "#### ConversationEntityMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e9aeaf",
      "metadata": {
        "id": "41e9aeaf"
      },
      "source": [
        "**Key feature:** _the conversation entity memory keeps a recollection of the main entities that have been mentioned, together with their specific attributes._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2900a385",
      "metadata": {
        "id": "2900a385"
      },
      "source": [
        "The way this works is quite similar to the `ConversationKnowledgeGraphMemory`, you can refer to the [docs](https://python.langchain.com/en/latest/modules/memory/types/entity_summary_memory.html) if you want to see it in action. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45112bd",
      "metadata": {
        "id": "d45112bd"
      },
      "source": [
        "## What else can we do with memory?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78296bff",
      "metadata": {
        "id": "78296bff"
      },
      "source": [
        "There are several cool things we can do with memory in langchain. We can:\n",
        "* implement our own custom memory module\n",
        "* use multiple memory modules in the same chain\n",
        "* combine agents with memory and other tools\n",
        "\n",
        "If this piques your interest, we suggest you to go take a look at the memory [how-to](https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html) section in the docs!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pinecone1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
