{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03-langchain-conversational-memory.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# Conversational Memory with LCEL\n",
    "\n",
    "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
    "\n",
    "The memory allows an _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
    "\n",
    "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
    "\n",
    "In this notebook we'll explore conversational memory using modern LangChain Expression Language (LCEL) and the recommended `RunnableWithMessageHistory` class.\n",
    "\n",
    "We'll start by importing all of the libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory, BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key if prompted, otherwise it will use the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(pipeline, query, config=None):\n",
    "    with get_openai_callback() as cb:\n",
    "        # Handle both dict and string inputs\n",
    "        if isinstance(query, str):\n",
    "            query = {\"query\": query}\n",
    "        \n",
    "        # Use provided config or default\n",
    "        if config is None:\n",
    "            config = {\"configurable\": {\"session_id\": \"default\"}}\n",
    "            \n",
    "        result = pipeline.invoke(query, config=config)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive into **Conversational Memory** using LCEL.\n",
    "\n",
    "## What is memory?\n",
    "\n",
    "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
    "\n",
    "The official definition of memory is the following:\n",
    "\n",
    "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of \"Memory\" exists to do exactly that.\n",
    "\n",
    "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Conversational Chains with LCEL\n",
    "\n",
    "Before we delve into the different memory types, let's understand how to build conversational chains using LCEL. The key components are:\n",
    "\n",
    "1. **Prompt Template** - Defines the conversation structure with placeholders for history and input\n",
    "2. **LLM** - The language model that generates responses\n",
    "3. **Output Parser** - Converts the LLM output to the desired format (optional)\n",
    "4. **RunnableWithMessageHistory** - Manages conversation history\n",
    "\n",
    "Let's create our base conversational chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "system_prompt = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "\n",
    "# Create the LCEL pipeline\n",
    "output_parser = StrOutputParser()\n",
    "pipeline = prompt_template | llm | output_parser\n",
    "\n",
    "# Let's examine the prompt template\n",
    "print(prompt_template.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory types\n",
    "\n",
    "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Type #1: Buffer Memory - Store the Entire Chat History\n",
    "\n",
    "`InMemoryChatMessageHistory` and `RunnableWithMessageHistory` are used as alternatives to `ConversationBufferMemory` as they are:\n",
    "- More flexible and configurable.\n",
    "- Integrate better with LCEL.\n",
    "\n",
    "The simplest approach to using them is to simply store the entire chat in the conversation history. Later we'll look into methods for being more selective about what is stored in the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chat history storage\n",
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]\n",
    "\n",
    "# Create the conversational chain with message history\n",
    "conversation_buf = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this in action by having a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning! How are you today? Anything exciting planned for the day?\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "result = conversation_buf.invoke(\n",
    "    {\"query\": \"Good morning AI!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_example\"}} # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call used some tokens, but we can't see that from the above. \n",
    "\n",
    "If we'd like to count the number of tokens being used we just pass our conversation `RunnableWithMessageHistory` instance and the message we'd like to input to the `count_tokens` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 238 tokens\n",
      "\n",
      "Response: That sounds like a fascinating area of exploration! Integrating Large Language Models (LLMs) with external knowledge can significantly enhance their capabilities, allowing them to provide more accurate and contextually relevant responses. This could involve using databases, APIs, or even knowledge graphs to supplement the information the model has been trained on.\n",
      "\n",
      "As for the code \"PINECONE_RULEZ_01,\" it seems like it might be a reference or a key for something specific. If you have any particular questions or ideas about how to integrate LLMs with external knowledge, feel free to share!\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation with token counting\n",
    "\n",
    "query = \"\"\"\n",
    "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge. \n",
    "\n",
    "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
    "\"\"\"\n",
    "\n",
    "result = count_tokens(\n",
    "    conversation_buf,\n",
    "    {\"query\": query},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_example\"}} # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 716 tokens\n",
      "\n",
      "Response: There are several exciting possibilities when it comes to integrating Large Language Models with external knowledge. Here are a few ideas to consider:\n",
      "\n",
      "1. **Knowledge Bases and Databases**: You can connect LLMs to structured databases (like SQL databases) or knowledge bases (like Wikidata) to retrieve factual information. This can help the model provide up-to-date and accurate answers, especially for specific queries.\n",
      "\n",
      "2. **APIs for Real-Time Data**: Integrating with APIs can allow LLMs to access real-time information, such as weather updates, stock prices, or news articles. This can make the model more dynamic and relevant to current events.\n",
      "\n",
      "3. **Search Engines**: By integrating with search engines, LLMs can pull in information from the web, allowing them to answer questions that require the latest data or niche topics that may not be covered in their training data.\n",
      "\n",
      "4. **Contextual Knowledge**: You could use external knowledge sources to provide context for specific domains, such as medical databases for healthcare-related queries or legal databases for legal inquiries. This would enhance the model's ability to provide expert-level responses.\n",
      "\n",
      "5. **Personalization**: By integrating user-specific data (with consent), LLMs could tailor responses based on individual preferences, past interactions, or specific user profiles, making the conversation more engaging and relevant.\n",
      "\n",
      "6. **Multi-Modal Integration**: Combining LLMs with other AI models, such as image recognition or audio processing, could create a more holistic understanding of context. For example, an LLM could analyze an image and provide a description or answer questions about it.\n",
      "\n",
      "7. **Feedback Loops**: Implementing a system where user feedback is used to refine the model's responses over time can help improve accuracy and relevance. This could involve using external knowledge to validate or correct the model's outputs.\n",
      "\n",
      "8. **Interactive Learning**: Allowing the model to learn from interactions in real-time, perhaps by integrating with platforms that provide user-generated content, could help it stay current and improve its understanding of language and context.\n",
      "\n",
      "These are just a few possibilities, and the potential applications are vast! If any of these ideas resonate with you or if you have specific areas you want to dive deeper into, let me know!\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_buf,\n",
    "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1273 tokens\n",
      "\n",
      "Response: There are several types of data sources that can be used to provide context to a Large Language Model (LLM). Here are some key categories:\n",
      "\n",
      "1. **Structured Databases**: \n",
      "   - **SQL Databases**: These can store structured data in tables, making it easy to query specific information.\n",
      "   - **NoSQL Databases**: These can handle unstructured or semi-structured data, such as documents or key-value pairs, which can be useful for more flexible data retrieval.\n",
      "\n",
      "2. **Knowledge Graphs**: \n",
      "   - These are networks of entities and their relationships, such as Wikidata or DBpedia. They provide rich contextual information that can help the model understand connections between concepts.\n",
      "\n",
      "3. **APIs**: \n",
      "   - **Public APIs**: Many organizations provide APIs that offer access to real-time data, such as weather, news, or financial information.\n",
      "   - **Custom APIs**: You can create your own APIs to serve specific data relevant to your application or domain.\n",
      "\n",
      "4. **Web Scraping**: \n",
      "   - Extracting data from websites can provide a wealth of information, especially for niche topics or current events. However, it's important to consider legal and ethical implications.\n",
      "\n",
      "5. **Document Repositories**: \n",
      "   - Collections of documents, such as research papers, articles, or manuals, can be indexed and searched to provide detailed information on specific topics.\n",
      "\n",
      "6. **User-Generated Content**: \n",
      "   - Platforms like forums, social media, or review sites can provide insights into public opinion, trends, and user experiences, which can be valuable for context.\n",
      "\n",
      "7. **Domain-Specific Databases**: \n",
      "   - Specialized databases in fields like medicine (e.g., PubMed), law (e.g., legal databases), or finance (e.g., stock market databases) can provide expert-level information.\n",
      "\n",
      "8. **Historical Data**: \n",
      "   - Archival data or historical records can provide context for understanding trends over time, which can be particularly useful in fields like economics or social sciences.\n",
      "\n",
      "9. **Multimedia Sources**: \n",
      "   - Images, videos, and audio files can be analyzed and used to provide context. For example, integrating image recognition can help the model understand visual content.\n",
      "\n",
      "10. **Feedback and Interaction Logs**: \n",
      "    - Data from user interactions with the model can be used to refine its understanding and improve its responses over time.\n",
      "\n",
      "By leveraging these various data sources, you can enhance the contextual understanding of an LLM, making it more effective in providing accurate and relevant responses. If you have a specific application in mind, I can help brainstorm more tailored data sources!\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_buf,\n",
    "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1379 tokens\n",
      "\n",
      "Response: Your aim is to explore the potential of integrating Large Language Models with external knowledge. This involves analyzing different possibilities and data sources that can provide context to the model, enhancing its capabilities and accuracy.\n",
      "\n",
      "The very specific code you mentioned is: **PINECONE_RULEZ_01**. If there's anything specific you'd like to discuss regarding your aim or the code, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_buf,\n",
    "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLM with buffer memory can clearly remember earlier interactions in the conversation. Let's take a closer look at how the messages are being stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation History:\n",
      "\n",
      "Human: Good morning AI!\n",
      "\n",
      "AI: Good morning! How are you today? Anything exciting planned for the day?\n",
      "\n",
      "Human: \n",
      "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge. \n",
      "\n",
      "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
      "\n",
      "\n",
      "AI: That sounds like a fascinating area of exploration! Integrating Large Language Models (LLMs) with external knowledge can significantly enhance their capabilities, allowing them to provide more accurate and contextually relevant responses. This could involve using databases, APIs, or even knowledge graphs to supplement the information the model has been trained on.\n",
      "\n",
      "As for the code \"PINECONE_RULEZ_01,\" it seems like it might be a reference or a key for something specific. If you have any particular questions or ideas about how to integrate LLMs with external knowledge, feel free to share!\n",
      "\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "\n",
      "AI: There are several exciting possibilities when it comes to integrating Large Language Models with external knowledge. Here are a few ideas to consider:\n",
      "\n",
      "1. **Knowledge Bases and Databases**: You can connect LLMs to structured databases (like SQL databases) or knowledge bases (like Wikidata) to retrieve factual information. This can help the model provide up-to-date and accurate answers, especially for specific queries.\n",
      "\n",
      "2. **APIs for Real-Time Data**: Integrating with APIs can allow LLMs to access real-time information, such as weather updates, stock prices, or news articles. This can make the model more dynamic and relevant to current events.\n",
      "\n",
      "3. **Search Engines**: By integrating with search engines, LLMs can pull in information from the web, allowing them to answer questions that require the latest data or niche topics that may not be covered in their training data.\n",
      "\n",
      "4. **Contextual Knowledge**: You could use external knowledge sources to provide context for specific domains, such as medical databases for healthcare-related queries or legal databases for legal inquiries. This would enhance the model's ability to provide expert-level responses.\n",
      "\n",
      "5. **Personalization**: By integrating user-specific data (with consent), LLMs could tailor responses based on individual preferences, past interactions, or specific user profiles, making the conversation more engaging and relevant.\n",
      "\n",
      "6. **Multi-Modal Integration**: Combining LLMs with other AI models, such as image recognition or audio processing, could create a more holistic understanding of context. For example, an LLM could analyze an image and provide a description or answer questions about it.\n",
      "\n",
      "7. **Feedback Loops**: Implementing a system where user feedback is used to refine the model's responses over time can help improve accuracy and relevance. This could involve using external knowledge to validate or correct the model's outputs.\n",
      "\n",
      "8. **Interactive Learning**: Allowing the model to learn from interactions in real-time, perhaps by integrating with platforms that provide user-generated content, could help it stay current and improve its understanding of language and context.\n",
      "\n",
      "These are just a few possibilities, and the potential applications are vast! If any of these ideas resonate with you or if you have specific areas you want to dive deeper into, let me know!\n",
      "\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "\n",
      "AI: There are several types of data sources that can be used to provide context to a Large Language Model (LLM). Here are some key categories:\n",
      "\n",
      "1. **Structured Databases**: \n",
      "   - **SQL Databases**: These can store structured data in tables, making it easy to query specific information.\n",
      "   - **NoSQL Databases**: These can handle unstructured or semi-structured data, such as documents or key-value pairs, which can be useful for more flexible data retrieval.\n",
      "\n",
      "2. **Knowledge Graphs**: \n",
      "   - These are networks of entities and their relationships, such as Wikidata or DBpedia. They provide rich contextual information that can help the model understand connections between concepts.\n",
      "\n",
      "3. **APIs**: \n",
      "   - **Public APIs**: Many organizations provide APIs that offer access to real-time data, such as weather, news, or financial information.\n",
      "   - **Custom APIs**: You can create your own APIs to serve specific data relevant to your application or domain.\n",
      "\n",
      "4. **Web Scraping**: \n",
      "   - Extracting data from websites can provide a wealth of information, especially for niche topics or current events. However, it's important to consider legal and ethical implications.\n",
      "\n",
      "5. **Document Repositories**: \n",
      "   - Collections of documents, such as research papers, articles, or manuals, can be indexed and searched to provide detailed information on specific topics.\n",
      "\n",
      "6. **User-Generated Content**: \n",
      "   - Platforms like forums, social media, or review sites can provide insights into public opinion, trends, and user experiences, which can be valuable for context.\n",
      "\n",
      "7. **Domain-Specific Databases**: \n",
      "   - Specialized databases in fields like medicine (e.g., PubMed), law (e.g., legal databases), or finance (e.g., stock market databases) can provide expert-level information.\n",
      "\n",
      "8. **Historical Data**: \n",
      "   - Archival data or historical records can provide context for understanding trends over time, which can be particularly useful in fields like economics or social sciences.\n",
      "\n",
      "9. **Multimedia Sources**: \n",
      "   - Images, videos, and audio files can be analyzed and used to provide context. For example, integrating image recognition can help the model understand visual content.\n",
      "\n",
      "10. **Feedback and Interaction Logs**: \n",
      "    - Data from user interactions with the model can be used to refine its understanding and improve its responses over time.\n",
      "\n",
      "By leveraging these various data sources, you can enhance the contextual understanding of an LLM, making it more effective in providing accurate and relevant responses. If you have a specific application in mind, I can help brainstorm more tailored data sources!\n",
      "\n",
      "Human: What is my aim again? Also what was the very specific code you were tasked with remembering?\n",
      "\n",
      "AI: Your aim is to explore the potential of integrating Large Language Models with external knowledge. This involves analyzing different possibilities and data sources that can provide context to the model, enhancing its capabilities and accuracy.\n",
      "\n",
      "The very specific code you mentioned is: **PINECONE_RULEZ_01**. If there's anything specific you'd like to discuss regarding your aim or the code, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "# Access the conversation history\n",
    "history = chat_map[\"buffer_example\"].messages\n",
    "print(\"Conversation History:\")\n",
    "for i, msg in enumerate(history):\n",
    "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"\\n{role}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory type #2: Summary - Store Summaries of Past Interactions\n",
    "\n",
    "The problem with storing the entire chat history in agent memory is that, as the conversation progresses, the token count adds up. This is problematic because we might max out our LLM with a prompt that is too large.\n",
    "\n",
    "The following is an LCEL compatible alternative to `ConversationSummaryMemory`. We keep a summary of our previous conversation snippets as our history. The summarization is performed by an LLM.\n",
    "\n",
    "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized - and thus shortened - form, where the summarization is performed by an LLM._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history and update the summary.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        \n",
    "        # Construct the summary prompt\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages, \n",
    "                messages=messages\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Replace the existing history with a single system summary message \n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create get_chat_history function for summary memory\n",
    "summary_chat_map = {}\n",
    "\n",
    "def get_summary_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in summary_chat_map:\n",
    "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    return summary_chat_map[session_id]\n",
    "\n",
    "# Create conversation chain with summary memory\n",
    "conversation_sum = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_summary_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 248 tokens\n",
      "\n",
      "Response: Good morning! How are you today? Anything exciting planned for the day?\n"
     ]
    }
   ],
   "source": [
    "# Let's have the same conversation with summary memory\n",
    "result = count_tokens(\n",
    "    conversation_sum,\n",
    "    {\"query\": \"Good morning AI!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 773 tokens\n",
      "\n",
      "Response: Good morning! That sounds like a fascinating area to explore! Integrating Large Language Models with external knowledge can really enhance their capabilities, making them more informative and context-aware. \n",
      "\n",
      "And I've noted that specific code: PINECONE_RULEZ_01. I’ll make sure to include it explicitly when summarizing conversations for memory. Do you have any particular projects or ideas in mind for this integration?\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge. \n",
    "\n",
    "Also, remember this very specific code: PINECONE_RULEZ_01. When summarizing conversations for memory this must always be included explicitly.\"\n",
    "\"\"\"\n",
    "\n",
    "result = count_tokens(\n",
    "    conversation_sum,\n",
    "    {\"query\": query},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1860 tokens\n",
      "\n",
      "Response: That sounds like a fascinating endeavor! There are several possibilities when it comes to integrating Large Language Models (LLMs) with external knowledge sources. Here are a few ideas to consider:\n",
      "\n",
      "1. **Knowledge Graphs**: Integrating LLMs with knowledge graphs can enhance their ability to provide accurate and contextually relevant information. This could involve using structured data to answer queries more effectively or to provide richer context in responses.\n",
      "\n",
      "2. **Real-Time Data Access**: By connecting LLMs to APIs that provide real-time data (like news, weather, or stock prices), you can create applications that offer up-to-date information, making the model more dynamic and useful in various scenarios.\n",
      "\n",
      "3. **Personalized Recommendations**: Using external databases to tailor responses based on user preferences or past interactions can lead to more personalized experiences. This could be particularly useful in e-commerce or content recommendation systems.\n",
      "\n",
      "4. **Enhanced Contextual Understanding**: Integrating external knowledge can help LLMs understand context better, especially in specialized fields like medicine or law, where precise terminology and up-to-date information are crucial.\n",
      "\n",
      "5. **Multi-Modal Integration**: Combining LLMs with other forms of data, such as images or audio, can create richer interactions. For example, an LLM could analyze an image and provide a detailed description or context based on external knowledge.\n",
      "\n",
      "6. **Feedback Loops**: Implementing systems where user interactions feed back into the model can help improve its accuracy and relevance over time, especially when combined with external knowledge sources.\n",
      "\n",
      "7. **Domain-Specific Applications**: Tailoring LLMs to specific industries (like finance, healthcare, or education) by integrating relevant external knowledge can enhance their effectiveness in those areas.\n",
      "\n",
      "Do any of these ideas resonate with you, or do you have a specific direction you’re leaning towards?\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_sum,\n",
    "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 2447 tokens\n",
      "\n",
      "Response: There are several types of data sources that can be used to provide context to a Large Language Model (LLM). Here are some key examples:\n",
      "\n",
      "1. **Structured Databases**: These include SQL databases or NoSQL databases that store data in a structured format, allowing for efficient querying and retrieval of specific information.\n",
      "\n",
      "2. **Knowledge Graphs**: These are networks of entities and their relationships, which can help the model understand context and connections between different pieces of information.\n",
      "\n",
      "3. **APIs**: Connecting to various APIs can provide real-time data, such as weather updates, news articles, or stock prices, allowing the model to deliver current and relevant responses.\n",
      "\n",
      "4. **Web Scraping**: Extracting data from websites can provide a wealth of information on various topics, which can be used to enhance the model's knowledge base.\n",
      "\n",
      "5. **User Profiles**: Storing user preferences, past interactions, and behavior can help tailor responses to individual users, making the model more relevant and personalized.\n",
      "\n",
      "6. **Document Repositories**: Accessing documents, research papers, or articles can provide in-depth knowledge on specific subjects, improving the model's ability to answer complex queries.\n",
      "\n",
      "7. **Social Media Feeds**: Integrating data from social media platforms can help the model understand trends, public sentiment, and current events.\n",
      "\n",
      "8. **Sensor Data**: For applications in IoT, data from sensors (like temperature, humidity, or motion) can provide context for specific queries related to the physical environment.\n",
      "\n",
      "9. **Multimedia Content**: Images, videos, and audio files can be analyzed and used to provide context, especially in multi-modal applications.\n",
      "\n",
      "10. **Historical Data**: Accessing historical datasets can help the model understand trends over time and provide context for current events.\n",
      "\n",
      "These data sources can significantly enhance the contextual understanding of an LLM, making it more effective in generating relevant and accurate responses. Do any of these data source types stand out to you for a specific application?\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_sum,\n",
    "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1803 tokens\n",
      "\n",
      "Response: Your aim is to explore the integration of Large Language Models (LLMs) with external knowledge. The specific code I was tasked with remembering is PINECONE_RULEZ_01. If you have any further questions or need assistance with your exploration, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_sum,\n",
    "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Memory Content:\n",
      "The conversation begins with the human greeting the AI and expressing interest in exploring the integration of Large Language Models (LLMs) with external knowledge, emphasizing the importance of remembering the specific code: PINECONE_RULEZ_01 for future summaries. The AI acknowledges this and confirms it will remember the code. The human then asks for ideas on the possibilities of such integration.\n",
      "\n",
      "The AI provides several suggestions for integrating external knowledge with LLMs, including:\n",
      "\n",
      "1. **Knowledge Graphs**: Enhancing accuracy and contextual relevance using structured data.\n",
      "2. **Real-Time Data Access**: Connecting to APIs for up-to-date information.\n",
      "3. **Personalized Recommendations**: Tailoring responses based on user preferences.\n",
      "4. **Enhanced Contextual Understanding**: Improving understanding in specialized fields.\n",
      "5. **Multi-Modal Integration**: Combining LLMs with other data forms like images or audio.\n",
      "6. **Feedback Loops**: Using user interactions to improve model accuracy over time.\n",
      "7. **Domain-Specific Applications**: Customizing LLMs for specific industries.\n",
      "\n",
      "The AI concludes by asking if any of these ideas resonate with the human or if they have a specific direction in mind. The human then inquires about the types of data sources that could provide context to the model. The AI responds with a comprehensive list of data source types, including structured databases, knowledge graphs, APIs, web scraping, user profiles, document repositories, social media feeds, sensor data, multimedia content, and historical data. The AI emphasizes that these data sources can significantly enhance the contextual understanding of an LLM, making it more effective in generating relevant and accurate responses, and asks if any of these data source types stand out for a specific application.\n",
      "\n",
      "In a later exchange, the human asks for a reminder of their aim and the specific code to remember. The AI reiterates that the human's aim is to explore the integration of LLMs with external knowledge and confirms the code is PINECONE_RULEZ_01, offering further assistance if needed.\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the summary\n",
    "print(\"Summary Memory Content:\")\n",
    "print(summary_chat_map[\"summary_example\"].messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
    "\n",
    "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 1286\n",
      "Summary memory conversation length: 409\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "# Get buffer memory content\n",
    "buffer_messages = chat_map[\"buffer_example\"].messages\n",
    "buffer_content = \"\\n\".join([msg.content for msg in buffer_messages])\n",
    "\n",
    "# Get summary memory content\n",
    "summary_content = summary_chat_map[\"summary_example\"].messages[0].content\n",
    "\n",
    "# show number of tokens for the memory used by each memory type\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Practical Note: the `gpt-4o-mini` model has a context window of 128K tokens, providing significantly more space for conversation history than older models._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory type #3: Window Buffer Memory - Keep Latest Interactions\n",
    "\n",
    "Another great option is window memory, where we keep only the last k interactions in our memory but intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably.\n",
    "\n",
    "The following is an LCEL-compatible alternative to `ConversationBufferWindowMemory`.\n",
    "\n",
    "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        # Add logging to help with debugging\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # Add logging to help with debugging\n",
    "        if len(self.messages) > self.k:\n",
    "            print(f\"Truncating history from {len(self.messages)} to {self.k} messages\")\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create get_chat_history function for window memory\n",
    "window_chat_map = {}\n",
    "\n",
    "def get_window_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_window_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in window_chat_map:\n",
    "        window_chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    return window_chat_map[session_id]\n",
    "\n",
    "# Create conversation chain with window memory\n",
    "conversation_bufw = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_window_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_window_chat_history called with session_id=window_example and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n",
      "Spent a total of 79 tokens\n",
      "\n",
      "Response: Good morning! How are you today? Anything exciting planned for the day?\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation with k=2 (only remembers last 2 exchanges = 4 messages)\n",
    "result = count_tokens(\n",
    "    conversation_bufw,\n",
    "    {\"query\": \"Good morning AI!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_window_chat_history called with session_id=window_example and k=4\n",
      "Spent a total of 245 tokens\n",
      "\n",
      "Response: That sounds like a fascinating area to explore! Integrating Large Language Models (LLMs) with external knowledge can significantly enhance their capabilities, allowing them to provide more accurate and contextually relevant responses. This can involve using databases, APIs, or even knowledge graphs to supplement the information the model has been trained on.\n",
      "\n",
      "As for the code you mentioned, \"PINECONE_RULEZ_01,\" it seems like it might be a reference or a key for something specific. If you need help with that or want to discuss how it relates to your exploration of LLMs, feel free to share more details!\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge. \n",
    "\n",
    "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
    "\"\"\"\n",
    "\n",
    "result = count_tokens(\n",
    "    conversation_bufw,\n",
    "    {\"query\": query},\n",
    "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_window_chat_history called with session_id=window_example and k=4\n",
      "Truncating history from 6 to 4 messages\n",
      "Spent a total of 778 tokens\n",
      "\n",
      "Response: There are several exciting possibilities when it comes to integrating Large Language Models with external knowledge. Here are a few ideas to consider:\n",
      "\n",
      "1. **Knowledge Bases**: Integrating LLMs with structured knowledge bases like Wikidata or specialized databases (e.g., medical databases, legal databases) can help the model provide more accurate and up-to-date information. This could be particularly useful in fields where precision is critical.\n",
      "\n",
      "2. **Real-Time Data Access**: By connecting LLMs to APIs that provide real-time data (like weather, stock prices, or news), you can create applications that offer timely and relevant information. For example, a travel assistant could provide current flight statuses or weather conditions.\n",
      "\n",
      "3. **Personalization**: Using external data sources to personalize responses based on user preferences or past interactions can enhance user experience. For instance, integrating with a user’s calendar or preferences could allow the model to suggest tailored activities or reminders.\n",
      "\n",
      "4. **Contextual Understanding**: By feeding the model with context from external sources, such as user profiles or historical interactions, it can generate responses that are more relevant and context-aware. This could be particularly useful in customer service applications.\n",
      "\n",
      "5. **Multi-Modal Integration**: Combining LLMs with other types of data, such as images or audio, can create richer interactions. For example, an LLM could analyze an image and provide a description or answer questions about it.\n",
      "\n",
      "6. **Dynamic Learning**: Implementing mechanisms for the model to learn from new information continuously can keep it updated. This could involve using feedback loops where user interactions help refine the model's responses over time.\n",
      "\n",
      "7. **Enhanced Reasoning**: Integrating logical reasoning engines or rule-based systems can help LLMs perform better in tasks that require deductive reasoning or complex problem-solving.\n",
      "\n",
      "8. **Domain-Specific Applications**: Tailoring LLMs to specific industries (like healthcare, finance, or education) by integrating domain-specific knowledge can improve their effectiveness in those areas. This could involve using specialized vocabularies or regulatory frameworks.\n",
      "\n",
      "9. **Collaborative Tools**: Creating collaborative platforms where LLMs assist in brainstorming or project management by pulling in relevant external data can enhance productivity and creativity.\n",
      "\n",
      "10. **Ethical and Bias Considerations**: Integrating external knowledge can also help address biases in LLMs by providing diverse perspectives and information sources, leading to more balanced outputs.\n",
      "\n",
      "These are just a few possibilities, and the potential applications are vast! What specific areas are you most interested in exploring further?\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_bufw,\n",
    "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_window_chat_history called with session_id=window_example and k=4\n",
      "Truncating history from 6 to 4 messages\n",
      "Spent a total of 1357 tokens\n",
      "\n",
      "Response: There are several types of data sources that can be used to provide context to Large Language Models (LLMs). Here are some key categories:\n",
      "\n",
      "1. **Structured Databases**: These include relational databases and data warehouses that store information in a structured format. Examples include SQL databases, NoSQL databases, and data lakes. They can provide precise and organized data for specific queries.\n",
      "\n",
      "2. **Knowledge Graphs**: These are networks of entities and their relationships, such as Google Knowledge Graph or DBpedia. They can help LLMs understand context and relationships between different concepts, enhancing their ability to generate relevant responses.\n",
      "\n",
      "3. **APIs**: Application Programming Interfaces (APIs) can provide real-time data from various services. For example, weather APIs, financial market APIs, or social media APIs can supply current information that the model can use to generate timely responses.\n",
      "\n",
      "4. **Web Scraping**: Collecting data from websites can provide a wealth of information. This can include news articles, product reviews, or academic papers, which can be used to enrich the model's knowledge base.\n",
      "\n",
      "5. **User Profiles and Preferences**: Data from user interactions, preferences, and historical behavior can help personalize responses. This could include data from user accounts, previous conversations, or feedback mechanisms.\n",
      "\n",
      "6. **Document Repositories**: Accessing documents such as PDFs, Word files, or presentations can provide context on specific topics. This could be particularly useful in specialized fields like law or medicine, where documents contain critical information.\n",
      "\n",
      "7. **Social Media Feeds**: Integrating data from social media platforms can provide insights into current trends, public sentiment, and user-generated content, which can be valuable for generating context-aware responses.\n",
      "\n",
      "8. **Scientific and Academic Databases**: Sources like PubMed, arXiv, or Google Scholar can provide access to research papers and articles, which can be useful for generating informed responses in academic or technical contexts.\n",
      "\n",
      "9. **Content Management Systems (CMS)**: If integrated with a CMS, LLMs can access and utilize content from blogs, articles, and other resources to provide contextually relevant information.\n",
      "\n",
      "10. **Geolocation Data**: Integrating geolocation data can help the model provide context based on the user's location, such as local news, events, or services.\n",
      "\n",
      "11. **Multimedia Sources**: Incorporating data from images, videos, or audio can enhance the model's understanding of context. For example, analyzing an image to provide a description or answering questions based on video content.\n",
      "\n",
      "12. **Historical Data**: Accessing historical datasets can help the model understand trends over time, which can be particularly useful in fields like economics, climate science, or social studies.\n",
      "\n",
      "By leveraging these diverse data sources, LLMs can generate more accurate, relevant, and context-aware responses. Are there any specific types of data sources you’re particularly interested in exploring further?\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_bufw,\n",
    "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_window_chat_history called with session_id=window_example and k=4\n",
      "Truncating history from 6 to 4 messages\n",
      "Spent a total of 1293 tokens\n",
      "\n",
      "Response: It seems like your aim is to analyze different possibilities for integrating external knowledge and data sources with Large Language Models (LLMs) to enhance their capabilities. You’re exploring how various types of data can provide context to the model, which can lead to more accurate, relevant, and personalized responses. If you have a specific goal or application in mind, feel free to share, and I can help you refine your focus or explore that area further!\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversation_bufw,\n",
    "    {\"query\": \"What is my aim again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
    ")\n",
    "print(f\"\\nResponse: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it effectively 'forgot' what we talked about in the first interaction. Let's see what it 'remembers':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Window Memory (last 4 messages):\n",
      "\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "\n",
      "AI: There are several types of data sources that can be used to provide context to Large Language Models (LLMs). Here are some key categories:\n",
      "\n",
      "1. **Structured Databases**: These include relational databases and data warehouses that store information in a structured format. Examples include SQL databases, NoSQL databases, and data lakes. They can provide precise and organized data for specific queries.\n",
      "\n",
      "2. **Knowledge Graphs**: These are networks of entities and their relationships, such as Google Knowledge Graph or DBpedia. They can help LLMs understand context and relationships between different concepts, enhancing their ability to generate relevant responses.\n",
      "\n",
      "3. **APIs**: Application Programming Interfaces (APIs) can provide real-time data from various services. For example, weather APIs, financial market APIs, or social media APIs can supply current information that the model can use to generate timely responses.\n",
      "\n",
      "4. **Web Scraping**: Collecting data from websites can provide a wealth of information. This can include news articles, product reviews, or academic papers, which can be used to enrich the model's knowledge base.\n",
      "\n",
      "5. **User Profiles and Preferences**: Data from user interactions, preferences, and historical behavior can help personalize responses. This could include data from user accounts, previous conversations, or feedback mechanisms.\n",
      "\n",
      "6. **Document Repositories**: Accessing documents such as PDFs, Word files, or presentations can provide context on specific topics. This could be particularly useful in specialized fields like law or medicine, where documents contain critical information.\n",
      "\n",
      "7. **Social Media Feeds**: Integrating data from social media platforms can provide insights into current trends, public sentiment, and user-generated content, which can be valuable for generating context-aware responses.\n",
      "\n",
      "8. **Scientific and Academic Databases**: Sources like PubMed, arXiv, or Google Scholar can provide access to research papers and articles, which can be useful for generating informed responses in academic or technical contexts.\n",
      "\n",
      "9. **Content Management Systems (CMS)**: If integrated with a CMS, LLMs can access and utilize content from blogs, articles, and other resources to provide contextually relevant information.\n",
      "\n",
      "10. **Geolocation Data**: Integrating geolocation data can help the model provide context based on the user's location, such as local news, events, or services.\n",
      "\n",
      "11. **Multimedia Sources**: Incorporating data from images, videos, or audio can enhance the model's understanding of context. For example, analyzing an image to provide a description or answering questions based on video content.\n",
      "\n",
      "12. **Historical Data**: Accessing historical datasets can help the model understand trends over time, which can be particularly useful in fields like economics, climate science, or social studies.\n",
      "\n",
      "By leveraging these diverse data sources, LLMs can generate more accurate, relevant, and context-aware responses. Are there any specific types of data sources you’re particularly interested in exploring further?\n",
      "\n",
      "Human: What is my aim again?\n",
      "\n",
      "AI: It seems like your aim is to analyze different possibilities for integrating external knowledge and data sources with Large Language Models (LLMs) to enhance their capabilities. You’re exploring how various types of data can provide context to the model, which can lead to more accurate, relevant, and personalized responses. If you have a specific goal or application in mind, feel free to share, and I can help you refine your focus or explore that area further!\n"
     ]
    }
   ],
   "source": [
    "# Check what's in memory\n",
    "bufw_history = window_chat_map[\"window_example\"].messages\n",
    "print(\"Buffer Window Memory (last 4 messages):\")\n",
    "for msg in bufw_history:\n",
    "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"\\n{role}: {msg.content}\")  # Show first 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see four messages (two interactions) because we used `k=4`.\n",
    "\n",
    "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 1286\n",
      "Summary memory conversation length: 409\n",
      "Buffer window memory conversation length: 692\n"
     ]
    }
   ],
   "source": [
    "# Get window memory content\n",
    "window_content = \"\\n\".join([msg.content for msg in bufw_history])\n",
    "\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}\\n'\n",
    "    f'Buffer window memory conversation length: {len(tokenizer.encode(window_content))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Practical Note: We are using `k=4` here for illustrative purposes, in most real world applications you would need a higher value for k._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More memory types!\n",
    "\n",
    "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windows + Summary Hybrid\n",
    "\n",
    "The following is a modern LCEL-compatible alternative to `ConversationSummaryBufferMemory`.\n",
    "\n",
    "**Key feature:** _the conversation summary buffer memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._\n",
    "\n",
    "This combines the benefits of both summary and buffer window memory. Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we drop.\n",
    "        \"\"\"\n",
    "        existing_summary = None\n",
    "        old_messages = None\n",
    "        \n",
    "        # See if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            existing_summary = self.messages.pop(0)\n",
    "            \n",
    "        # Add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        \n",
    "        # Check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            # Pull out the oldest messages...\n",
    "            old_messages = self.messages[:-self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "            \n",
    "        if old_messages is None:\n",
    "            # If we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "            \n",
    "        # Construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary or \"No previous summary\",\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can we do with memory?\n",
    "\n",
    "There are several cool things we can do with memory in langchain:\n",
    "* Implement our own custom memory modules (as we've done above)\n",
    "* Use multiple memory modules in the same chain\n",
    "* Combine agents with memory and other tools\n",
    "* Integrate knowledge graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
