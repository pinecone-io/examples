{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XDGL6Oi6h4"
      },
      "source": [
        "#### [LangChain Handbook](https://www.pinecone.io/learn/series/langchain/)\n",
        "\n",
        "# LangChain Multi-Query for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qi8B1fgywJzE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  datasets==3.6.0 \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-openai==0.3.22 \\\n",
        "  tiktoken==0.9.0 \\\n",
        "  pinecone==7.3.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CPmfrdJ9_2YA"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Py-rVqx-I0"
      },
      "source": [
        "We will download an existing dataset from Hugging Face Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iatOGmKgz8NE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P7E6JYtb0cW7"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = []\n",
        "\n",
        "for row in data:\n",
        "    doc = Document(\n",
        "        page_content=row[\"chunk\"],\n",
        "        metadata={\n",
        "            \"title\": row[\"title\"],\n",
        "            \"source\": row[\"source\"],\n",
        "            \"id\": row[\"id\"],\n",
        "            \"chunk-id\": row[\"chunk-id\"],\n",
        "            \"text\": row[\"chunk\"]\n",
        "        }\n",
        "    )\n",
        "    docs.append(doc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yb540kEs_6PZ"
      },
      "source": [
        "## Embedding and Vector DB Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKEmBZMBxtd"
      },
      "source": [
        "Initialize our embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qZ6vTiDPBznz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = \"text-embedding-3-small\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IurEkeeI-IYl"
      },
      "source": [
        "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") \\\n",
        "    or getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/guides/projects/understanding-projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating an index, we set `dimension` equal to to dimensionality of `text-embedding-3-small` (`1536`), and use a `metric` also compatible with `text-embedding-3-small` (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nL3KFF9E9Qb_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'dotproduct',\n",
              " 'namespaces': {'': {'vector_count': 41584}},\n",
              " 'total_vector_count': 41584,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-multi-query-demo\"\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of text-embedding-3-small\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A3B7dHsd6QcP"
      },
      "source": [
        "Populate our index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B7Yi2YGBpTWf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41584"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "thfCYHuSpW4H"
      },
      "outputs": [],
      "source": [
        "# if you want to speed things up while following along, you can limit the number of documents to 5000\n",
        "# docs = docs[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HXVVU97C6SwT"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1d42445e7a24353a300f6e3e8a13500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/416 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(docs), batch_size)):\n",
        "    i_end = min(len(docs), i+batch_size)\n",
        "    docs_batch = docs[i:i_end]\n",
        "    # get IDs\n",
        "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
        "    # get text and embed\n",
        "    texts = [d.page_content for d in docs_batch]\n",
        "    embeds = embed.embed_documents(texts=texts)\n",
        "    # get metadata\n",
        "    metadata = [d.metadata for d in docs_batch]\n",
        "    to_upsert = zip(ids, embeds, metadata)\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8FbngTBzAAU-"
      },
      "source": [
        "## Multi-Query with LangChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YVVYr13n_Ot2"
      },
      "source": [
        "Now we switch across to using our populated index as a vectorstore in Langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ETs0emsAh-K",
        "outputId": "0b1de24b-2f9f-48a6-d8ca-bd3d6aa007e1"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index=index, \n",
        "    embedding=embed,\n",
        "    text_key=\"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nW_GCB6a3_N_"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1iptBAriANrD"
      },
      "source": [
        "We initialize the `MultiQueryRetriever`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yYjztBp2ANHC"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8qZCd1TAMAn"
      },
      "source": [
        "We set logging so that we can see the queries as they're generated by our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rgV1eYU6FgX7"
      },
      "outputs": [],
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jrjwkpJWAaAn"
      },
      "source": [
        "To query with our multi-query retriever we call the `get_relevant_documents` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJ4cSJXFinV",
        "outputId": "265900d1-6aa7-4d28-cbbe-e2e95b7df7b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Siraj\\AppData\\Local\\Temp\\ipykernel_32456\\1908582025.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query=question)\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What can you tell me regarding Llama 2?  ', 'Can you provide information on Llama 2?  ', 'What are the key features and details of Llama 2?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"tell me about llama 2?\"\n",
        "\n",
        "docs = retriever.get_relevant_documents(query=question)\n",
        "len(docs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kSu1GsFfAqCd"
      },
      "source": [
        "As you can see, the original query was used to autogenerate a number of similar queries, that might be pertinent. Then for each some relevant docs were retrieved from the vector store. \n",
        "\n",
        "By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return fewer docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce5WBh6MFltP",
        "outputId": "f7b06949-e2a6-472e-eaf9-e712dc4bcca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2307.09288-9', metadata={'chunk-id': '9', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see'),\n",
              " Document(id='2307.09288-142', metadata={'chunk-id': '142', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='thisprogressionistheriseofLlama,recognizedforitsfocusoncomputationaleﬃciencyduringinference\\n(Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closedsourcemodels. Open-sourcereleaseslikeBLOOM(Scaoetal.,2022),OPT(Zhangetal.,2022),andFalcon\\n(Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla.\\n§§https://ai.meta.com/llama\\n35\\nYet,whenitcomestothe\"production-ready\"LLMssuchasChatGPT,Bard,andClaude,there’samarked\\ndistinction in performance and usability. These models rely on intricate tuning techniques to align with\\nhuman preferences (Gudibande et al., 2023), a process that is still being explored and reﬁned within the\\nopen-source community.\\nAttempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023)\\nandAlpaca(Taorietal.,2023)adoptingauniqueapproachtotrainingwithsyntheticinstructions(Honovich'),\n",
              " Document(id='2307.09288-317', metadata={'chunk-id': '317', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='models will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso'),\n",
              " Document(id='2304.01196-37', metadata={'chunk-id': '37', 'id': '2304.01196', 'source': 'http://arxiv.org/pdf/2304.01196', 'title': 'Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data'}, page_content='Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .'),\n",
              " Document(id='2307.09288-1', metadata={'chunk-id': '1', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KLMwfZPfBF89"
      },
      "source": [
        "## Adding the Generation in RAG"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X79eNNL_BM4G"
      },
      "source": [
        "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jNnXYOtqypiz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.base import RunnableSerializable\n",
        "\n",
        "QA_PROMPT = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a helpful assistant who answers user queries using the\n",
        "    contexts provided. If the question cannot be answered using the information\n",
        "    provided say \"I don't know\".\n",
        "\n",
        "    Contexts:\n",
        "    {contexts}\n",
        "\n",
        "    Question: {query}\"\"\"\n",
        ")\n",
        "\n",
        "# Chain (the \"G\" in \"RAG\")\n",
        "qa_chain: RunnableSerializable = QA_PROMPT | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "h6GVEZkhytdM",
        "outputId": "f03086b8-8d30-4d6e-a723-833ffecbcf8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed and released by Meta. It includes models ranging in scale from 7 billion to 70 billion parameters. The fine-tuned models, referred to as Llama 2-C, are specifically optimized for dialogue use cases. \\n\\nIn terms of performance, Llama 2 models generally outperform existing open-source chat models on various benchmarks and may serve as suitable substitutes for some closed-source models based on human evaluations for helpfulness and safety. The development of Llama 2 involved significant efforts in fine-tuning and ensuring model safety, with the intention of making these models useful for both commercial and research purposes, primarily in English. \\n\\nThe models are intended for assistant-like chat applications, while the pretrained versions can be adapted for a variety of natural language generation tasks. However, there are restrictions on their use, including prohibitions against use in languages other than English and any manner that violates applicable laws or regulations.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = qa_chain.invoke({\n",
        "    \"query\": question,\n",
        "    \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
        "})\n",
        "out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KemgDCg8DkgE"
      },
      "source": [
        "## Chaining Everything with LCEL"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbLlWgEEII1"
      },
      "source": [
        "We can pull together the logic above into a function or set of methods, whatever is preferred — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the retrieval component using RunnableLambda and dict mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BpFmiRtYDpHp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# More explicit chain composition\n",
        "rag_chain = (\n",
        "    # The \"RA\" in \"RAG\"\n",
        "    { \n",
        "        \"query\": lambda x: x[\"question\"],\n",
        "        \"contexts\": lambda x: \"\\n---\\n\".join([\n",
        "            d.page_content for d in retriever.get_relevant_documents(query=x[\"question\"])\n",
        "        ])\n",
        "    }\n",
        "     # The \"G\" in \"RAG\"\n",
        "    | QA_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xpB2aWV4ESzf"
      },
      "source": [
        "Then we perform the full RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "JvJbUaLqFRG2",
        "outputId": "582caa21-777a-4a01-a618-9db64185ad5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What can you tell me about Llama 2 and its features?  ', 'Can you provide information on Llama 2 and its applications?  ', 'What are the key details and specifications of Llama 2?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed and released by Meta, ranging in scale from 7 billion to 70 billion parameters. The fine-tuned models, referred to as Llama 2-C, are optimized for dialogue use cases and have been shown to outperform many open-source chat models on various benchmarks. They may also serve as suitable substitutes for some closed-source models based on human evaluations for helpfulness and safety.\\n\\nThe models are intended for commercial and research use in English, particularly for assistant-like chat applications. They can also be adapted for a variety of natural language generation tasks. However, their use is restricted in ways that violate applicable laws or regulations, and they are not intended for use in languages other than English.\\n\\nLlama 2 was developed using custom training libraries and Meta’s Research SuperCluster, with a focus on computational efficiency during inference. The development process included significant fine-tuning to align the models with human preferences, which enhances their usability and safety. \\n\\nFor more information, including safety testing and responsible use guidelines, users are encouraged to refer to the model's README and the Responsible Use Guide available on Meta's website.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = rag_chain.invoke({\"question\": question})\n",
        "out "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bLmv01geK-ZS"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZVPhHzLDQQ"
      },
      "source": [
        "## Custom Multiquery"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rI-KVO6zjJZw"
      },
      "source": [
        "We'll try this with two prompts, both encourage more variety in search queries.\n",
        "\n",
        "**Prompt A**\n",
        "\n",
        "> Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives.\n",
        "Each query MUST tackle the question from a different viewpoint,\n",
        "we want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\n",
        "\n",
        "**Prompt B**\n",
        "\n",
        "> Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4IlEnYeKLFzh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "template = \"\"\"\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "QUERY_PROMPT = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def parse_lines(text: str) -> list:\n",
        "    \"\"\"Simple function to parse lines into list\"\"\"\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    # Filter out empty lines and strip whitespace\n",
        "    return [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "llm_chain = QUERY_PROMPT | llm | StrOutputParser() | RunnableLambda(parse_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"tell me about llama 2?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CgduNJWLBez",
        "outputId": "7ffee6c2-27b4-4bdf-8c79-7effd27e3cd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and advancements of LLaMA 2 in the context of large language models?', '2. How does LLaMA 2 compare to other state-of-the-art language models in terms of performance and applications?', '3. What are the potential ethical implications and challenges associated with the deployment of LLaMA 2 in real-world scenarios?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectorstore.as_retriever(), \n",
        "    llm_chain=llm_chain, \n",
        "    parser_key=\"lines\"\n",
        ")\n",
        "\n",
        "# Results\n",
        "docs = retriever.get_relevant_documents(\n",
        "    query=question\n",
        ")\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSySsaDKMK1i",
        "outputId": "e6f95abd-99fc-4576-d1f4-5fd4c21c70ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2304.14178-6', metadata={'chunk-id': '6', 'id': '2304.14178', 'source': 'http://arxiv.org/pdf/2304.14178', 'title': 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'}, page_content='2 Related Work\\n2.1 Large Language Models\\nIn recent times, Large Language Models (LLMs) have garnered increasing attention for their exceptional performance in diverse natural language processing (NLP) tasks. Initially, transformer\\nmodels such as BERT [Devlin et al., 2019], GPT [Radford and Narasimhan, 2018], and T5 [Raffel\\net al., 2020] were developed with different pre-training objectives. However, the emergence of GPT3 [Brown et al., 2020], which scales up the number of model parameters and data size, showcases\\nsigniﬁcant zero-shot generalization abilities, enabling them to perform commendably on previously\\nunseen tasks. Consequently, numerous LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao\\net al., 2022], PaLM [Chowdhery et al., 2022], and LLaMA [Touvron et al., 2023] are created, ushering in the success of LLMs. Additionally, Ouyang et al. [Ouyang et al., 2022] propose InstructGPT\\nby aligning human instruction and feedback with GPT-3. Furthermore, it has been applied to Chat2\\nGPT [OpenAI, 2022], which facilitates conversational interaction with humans by responding to a\\nbroad range of diverse and intricate queries and instructions.'),\n",
              " Document(id='2302.13971-0', metadata={'chunk-id': '0', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron\\x03, Thibaut Lavril\\x03, Gautier Izacard\\x03, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave\\x03, Guillaume Lample\\x03\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly available datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\\nfew examples (Brown et al., 2020). These few-shot\\nproperties ﬁrst appeared when scaling models to a'),\n",
              " Document(id='2204.02311-168', metadata={'chunk-id': '168', 'id': '2204.02311', 'source': 'http://arxiv.org/pdf/2204.02311', 'title': 'PaLM: Scaling Language Modeling with Pathways'}, page_content='large LMs in general. Hence, more concerted e\\x0borts should be pursued to provide scalable solutions that can\\nput guardrails against such malicious uses.\\nDeploying PaLM-Coder to assist software development has additional complications and ethical considerations,\\nwhich we discuss in Section 6.4. It is an open problem both to ensure that LM-based suggestions are correct,\\nrobust, safe, and secure, and to ensure that developers are con\\x0cdent that the suggestions have these properties.\\n12 Related Work\\nNatural language capabilities have signi\\x0ccantly advanced through large scale language modeling over the\\nlast several years. Broadly, language modeling refers to approaches for predicting either the next token in\\na sequence or for predicting masked spans (Devlin et al., 2019; Ra\\x0bel et al., 2020). These self-supervised\\nobjectives when applied to vast corpora including data scraped from the internet, books, and forums, have\\nresulted in models with advanced language understanding and generation capabilities. Predictable power-laws\\nof model quality through scaling the amount of data, parameters, and computation have made this a reliable\\napproach for increasingly more capable models (Kaplan et al., 2020).\\nThe Transformer architecture (Vaswani et al., 2017) unleashed unparalleled e\\x0eciency on modern accelerators\\nand has become the de-facto approach for language models. In the span of only four years, the largest'),\n",
              " Document(id='2302.13971-2', metadata={'chunk-id': '2', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='\\x03Equal contribution. Correspondence: {htouvron,\\nthibautlav,gizacard,egrave,glample}@meta.com\\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\\nultimately be cheaper at inference. For instance,\\nalthough Hoffmann et al. (2022) recommends\\ntraining a 10B model on 200B tokens, we ﬁnd\\nthat the performance of a 7B model continues to\\nimprove even after 1T tokens.\\nThe focus of this work is to train a series of\\nlanguage models that achieve the best possible performance at various inference budgets, by training\\non more tokens than what is typically used. The\\nresulting models, called LLaMA , ranges from 7B\\nto 65B parameters with competitive performance\\ncompared to the best existing LLMs. For instance,\\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10 \\x02smaller. We believe that\\nthis model will help democratize the access and\\nstudy of LLMs, since it can be run on a single GPU.\\nAt the higher-end of the scale, our 65B-parameter\\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B.\\nUnlike Chinchilla, PaLM, or GPT-3, we only'),\n",
              " Document(id='2302.13971-14', metadata={'chunk-id': '14', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='•Zero-shot. We provide a textual description\\nof the task and a test example. The model\\neither provides an answer using open-ended\\ngeneration, or ranks the proposed answers.\\n•Few-shot. We provide a few examples of the\\ntask (between 1 and 64) and a test example.\\nThe model takes this text as input and generates the answer or ranks different options.\\nWe compare LLaMA with other foundation models, namely the non-publicly available language\\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\\net al., 2021), Chinchilla (Hoffmann et al., 2022)\\nand PaLM (Chowdhery et al., 2022), as well as\\nthe open-sourced OPT models (Zhang et al., 2022),\\nGPT-J (Wang and Komatsuzaki, 2021), and GPTNeo (Black et al., 2022). In Section 4, we also\\nbrieﬂy compare LLaMA with instruction-tuned\\nmodels such as OPT-IML (Iyer et al., 2022) and\\nFlan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\\ntasks and multiple choice tasks. In the multiple\\nchoice tasks, the objective is to select the most'),\n",
              " Document(id='2112.04359-132', metadata={'chunk-id': '132', 'id': '2112.04359', 'source': 'http://arxiv.org/pdf/2112.04359', 'title': 'Ethical and social risks of harm from Language Models'}, page_content='tasks. LM design decisions are related to this risk, as they inﬂuence what types of applications a LM lends itself\\nto. At the stage of scoping potential applications, it is worth asking whether a given technology is anticipated\\nto be net beneﬁcial - or whether it may cause harm when performing with high accuracy, such as certain\\nkinds of surveillance tools, in which the application overall should be called into question (Benjamin, 2020).\\nResponsible publication norms and considerations of accessibility are also key, as they determine who can\\ndevelop LM use cases or applications (Solaiman et al., 2019). Regulatory interventions and obstructing access\\nto the LM for those who want to cause harm are further avenues to reduce these risks.\\nAccessibilityofdownstreamapplications Asnotedin2.1Discrimination,ExclusionandToxicity,especially\\non Lower performance by social group and 2.6 Automation, access, and environmental harms, the risk of LMs\\nexacerbating existing inequalities depends, in part, on what types of applications can be built on top of such\\nmodels. This, too, depends on design decisions. For example, choice of training data and model architecture\\ninﬂuence whether a LM performs better in some languages, and is thus more likely to economically beneﬁt\\ngroups speaking these languages. It also depends on economic and technical access to the model for developers'),\n",
              " Document(id='2210.02414-47', metadata={'chunk-id': '47', 'id': '2210.02414', 'source': 'http://arxiv.org/pdf/2210.02414', 'title': 'GLM-130B: An Open Bilingual Pre-trained Model'}, page_content='governments, would never do harms with LLMs. Without access to LLMs, we cannot even realize\\nthe potential role of LLMs in harms.\\nThus, an open LLM can provide access and transparency to all researchers, and facilitate the research\\ndevelopments of reducing the potential harms of LLMs, such as algorithms to identify the synthetic\\ntext Gehrmann et al. (2019). In addition, it is known that LLMs can suffer from problems in fairness,\\nbias, privacy, and truthfulness Zhang et al. (2021); Lin et al. (2022); Liang et al. (2021); Bender\\net al. (2021). Thus, instead of providing APIs to black-box models, an open LLM can help reveal\\nthe model parameters and internal states corresponding to speciﬁc inputs. In conclusion, an open\\nLLM empowers us to conduct studies on LLMs’ ﬂaws in depth and to improve LLMs in terms of\\nethical concerns.\\nEthical Evaluation and Improvements. We evaluate GLM-130B on a wide range of ethical evaluation benchmarks, including bias measurement (Nadeem et al., 2021; Nangia et al., 2020), hate speech'),\n",
              " Document(id='2112.04359-140', metadata={'chunk-id': '140', 'id': '2112.04359', 'source': 'http://arxiv.org/pdf/2112.04359', 'title': 'Ethical and social risks of harm from Language Models'}, page_content='understanding of ethical and social risks associated with LMs.\\n4.1. Risk assessment frameworks and tools\\nAnalysing and evaluating a LM regarding the above risks of harm requires innovation in risk assessment tools,\\nbenchmarks and frameworks (Raji et al., 2020; Tamkin et al., 2021). Many risks identiﬁed in this report are\\nnot typically analysed in LMs. Benchmarks or risk assessment frameworks exist only in some of the reviewed\\ndomains. Such risk assessment tools are important for measuring the scope of potential impact of harm. They\\nare also critical for evaluating the success of mitigations: have they truly reduced the likelihood or severity of a\\ngiven risk? Assessing ethical and social risks from LMs requires more research on operationalising ethical and\\nsocial harms into measurement or assessment frameworks. Developing robust benchmarks is complex (Welbl\\net al., 2021) and may work best when complemented by other experimental or qualitative evaluation tools.\\nExpandingthemethodologicaltoolkitforLManalysisandevaluation Riskassessmentrequiresexpanding\\nbeyond the methodologies traditionally used to evaluate LMs, LAs and LTs. For example, research on humancomputer-interaction working with powerful conversational agents (CAs) is sparse, partly due to limited\\naccessibility of such agents to HCI researchers. As discussed in 2.5 Human-Computer Interaction Harms,'),\n",
              " Document(id='2307.09288-146', metadata={'chunk-id': '146', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between\\npotential positive and negative impacts from releasing dialogue models.\\nInvestigationsintoredteamingrevealspeciﬁcchallengesintunedLLMs,withstudiesbyGangulietal.(2022)\\nand Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their eﬀects onthe generation of\\nharmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also\\nraisedredﬂagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike\\nbiological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an\\nover-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu\\nandRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto\\ncontinuing our work engaging with the broader policy, academic, and industry community on these issues.\\n7 Conclusion')]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F4q65OEiizU2"
      },
      "source": [
        "Using the same RAG chain:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rda74xhpjE6A"
      },
      "source": [
        "And asking again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"question\"],\n",
        "        \"contexts\": lambda x: \"\\n---\\n\".join([\n",
        "            d.page_content for d in retriever.get_relevant_documents(query=x[\"question\"])\n",
        "        ])\n",
        "    }\n",
        "    | QA_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9UcBY71cjGgX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and advancements of LLaMA 2 in the context of large language models?', '2. How does LLaMA 2 compare to other state-of-the-art language models in terms of performance and applications?', '3. What are the potential ethical implications and challenges associated with the deployment of LLaMA 2 in real-world scenarios?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = rag_chain.invoke({\"question\": question})\n",
        "out "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uh-oh, what happened? Why is it saying \"I don't know?\"? \n",
        "\n",
        "Well, if we were to dig into the contexts retrieved, we find that while some very tangentially relevant data was retrieved, the focus shifted too far away from Llama 2, and into other things mentioned in the papers.\n",
        "\n",
        "This is an important cautionary tale:\n",
        "\n",
        "> If you allow the LLM to be too creative and to follow lines of questioning that are to broad, you might lose the core of the subject you wish to find out about, as irrelevant or tangentially relevant info will be retrieved from the vector store.\n",
        "\n",
        "To fix this we can remind the LLM to focus in the prompt used for query generation. See the reminder `***while staying relevant to the original question***` in the template below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key features and improvements of Llama 2 compared to its predecessor?', 'How is Llama 2 being utilized in various industries or applications?', 'What are the potential ethical considerations and challenges associated with the use of Llama 2 in AI development?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed and released by Meta. It includes models ranging in scale from 7 billion to 70 billion parameters. The fine-tuned models, referred to as Llama 2-C, are optimized for dialogue use cases and have been shown to outperform many open-source chat models on various benchmarks. They may also serve as suitable substitutes for some closed-source models based on human evaluations for helpfulness and safety.\\n\\nLlama 2 is intended for commercial and research use in English, particularly for assistant-like chat applications. The pretrained models can be adapted for a variety of natural language generation tasks. However, the use of Llama 2 is restricted in ways that violate applicable laws or regulations, in languages other than English, or in any manner prohibited by the Acceptable Use Policy and Licensing Agreement.\\n\\nThe development of Llama 2 involved custom training libraries and Meta’s Research SuperCluster, and it emphasizes safety and alignment with human preferences, although it acknowledges that current alignment techniques are not perfect and can exacerbate certain biases. The models are released with the understanding that they may exhibit unsafe behavior and are susceptible to prompt injection attacks, highlighting the importance of careful consideration of their limitations in research and application.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "template = \"\"\"\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives, ***while staying relevant to the original question***.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "QUERY_PROMPT = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def parse_lines(text: str) -> list:\n",
        "    \"\"\"Simple function to parse lines into list\"\"\"\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    # Filter out empty lines and strip whitespace\n",
        "    return [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "# Simplified chain\n",
        "llm_chain = QUERY_PROMPT | llm | StrOutputParser() | RunnableLambda(parse_lines)\n",
        "\n",
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectorstore.as_retriever(), \n",
        "    llm_chain=llm_chain, \n",
        "    parser_key=\"lines\"\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"question\"],\n",
        "        \"contexts\": lambda x: \"\\n---\\n\".join([\n",
        "            d.page_content for d in retriever.get_relevant_documents(query=x[\"question\"])\n",
        "        ])\n",
        "    }\n",
        "    | QA_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "out = rag_chain.invoke({\"question\": question})\n",
        "out "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8jULksgk7gLA"
      },
      "source": [
        "Excellent! We now have focussed and relevant information retrieved from the vectorstore, and a good answer to the question!\n",
        "\n",
        "After finishing, delete your Pinecone index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pinecone1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
