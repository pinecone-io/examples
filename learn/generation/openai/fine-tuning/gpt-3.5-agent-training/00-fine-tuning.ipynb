{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/openai/fine-tuning/gpt-3.5-agent-training/00-fine-tuning.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/openai/fine-tuning/gpt-3.5-agent-training/00-fine-tuning.ipynb)\n",
        "\n",
        "# Fine-tuning GPT-3.5 with a retrieval tool\n",
        "\n",
        "This notebook walks through fine-tuning GPT-3.5 Turbo on conversations that use a Pinecone-backed vector search tool. You will load a dataset of tool-using conversations, run a fine-tuning job with the OpenAI API, then use the fine-tuned model with LangChain and Pinecone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prereq"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Python with `datasets`, `langchain`, `pinecone`, and `openai` (install in the next cell).\n",
        "- [OpenAI API key](https://platform.openai.com/api-keys) for fine-tuning and inference.\n",
        "- [Pinecone API key](https://app.pinecone.io/) for the vector search tool used by the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "\n",
        "!pip install -qU datasets==2.14.4 langchain==0.0.274 pinecone==8.0.0 openai==0.27.9 requests==2.32.3\n",
        "\n",
        "res = requests.get(\n",
        "    \"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/openai/fine-tuning/gpt-3.5-agent-training/chains.py\"\n",
        ")\n",
        "with open(\"chains.py\", \"w\") as fp:\n",
        "    fp.write(res.text)\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from time import sleep\n",
        "\n",
        "import openai\n",
        "from chains import VectorDBChain\n",
        "from datasets import load_dataset\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup complete (imports and chains.py in previous cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wjqYNbLXQ2x",
        "outputId": "90d81eda-5280-4138-8381-db2ec0eda4bb"
      },
      "source": [
        "data = load_dataset(\"jamescalam/agent-conversations-retrieval-tool\", split=\"train\")\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWUKWk_GdkjG",
        "outputId": "24bfb862-ade8-437a-c217-db86cc80c81d"
      },
      "source": [
        "data[\"messages\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "39364e874e5c4e7baa01c08ac31165fb",
            "cf43f35611b444b498153f8d659ce153",
            "f2a10ce29d894e74a22842953fb8bc59",
            "58fac49a766a4233b513bc05a30da756",
            "7b5bcd804aa14aaca9d835c1a6262111",
            "fe26f0a8030b40528b5036bb8d994db5",
            "221b7605257a4235b77fdd828e7fd6e6",
            "de5ce44aeb78464a9be9c6b7392b6969",
            "280c7b6c0e4d42249a9adb5a0ca1d553",
            "8996a369a00a447093e6866183ef8648",
            "eece5f66123d4ded8181c0373781da5b"
          ]
        },
        "id": "0sIMkzT4eJXO",
        "outputId": "48786185-2818-4d3a-bb58-e2e696f3a662"
      },
      "source": [
        "data.to_json(\"conversations.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQPO963iXQ2z"
      },
      "source": [
        "## Running Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4mZk_vlXQ2z"
      },
      "source": [
        "First we upload the files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLMSn9EJXQ2z",
        "outputId": "57afc952-fb55-421d-e338-91a8a633a234"
      },
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "\n",
        "res = openai.File.create(file=open(\"conversations.jsonl\", \"r\"), purpose=\"fine-tune\")\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y_HCXuCeXQ2z",
        "outputId": "2b5c0b65-fb41-4676-c37a-804b4403c69e"
      },
      "source": [
        "file_id = res[\"id\"]\n",
        "file_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuGmK_pLXQ2z"
      },
      "source": [
        "We then create the fine-tuning job _(note, it can take some time before the file above is ready)_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxv-abQYXQ2z",
        "outputId": "0f9081d5-ed62-498f-d94c-96ade0344fb8"
      },
      "source": [
        "res = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gFjR5USVXQ20",
        "outputId": "ced7e1fa-c38e-4491-c946-60682dd3e754"
      },
      "source": [
        "job_id = res[\"id\"]\n",
        "job_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZByfStbHXQ20"
      },
      "source": [
        "We can retrieve info for a our fine-tuning job like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s64fq_nMXQ20",
        "outputId": "3c063a59-fe56-4dfa-ca7a-0f1253923954"
      },
      "source": [
        "openai.FineTuningJob.retrieve(job_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TPYgQ4_XQ20"
      },
      "source": [
        "The `\"finished_at\"` value is still `null`, so fine-tuning isn't yet complete. We can check for events from our fine-tuning job while we wait:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QejzgDcQXQ20",
        "outputId": "2b9645a0-ba2c-461b-e2cc-45918f210102"
      },
      "source": [
        "openai.FineTuningJob.list_events(id=job_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqxpSFftXQ20"
      },
      "source": [
        "We can setup a check for fine-tuning completion (or wait for OpenAI to send you an email telling you that the job has completed):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "SAt5Eq6-XQ20",
        "outputId": "1e01a3ea-94f0-4ff4-9d64-d6661efd6336"
      },
      "source": [
        "while True:\n",
        "    res = openai.FineTuningJob.retrieve(job_id)\n",
        "    if res[\"finished_at\"] is not None:\n",
        "        break\n",
        "    else:\n",
        "        print(\".\", end=\"\")\n",
        "        sleep(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNCuwKPMXQ20"
      },
      "source": [
        "Once complete, we can see our model details in the `res`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xef-ZRAoXQ20"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K3P1eGlXQ20"
      },
      "source": [
        "We access our fine-tuned model name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu6bjioRXQ20"
      },
      "source": [
        "ft_model = res[\"fine_tuned_model\"]\n",
        "ft_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfsVLrePXQ20"
      },
      "source": [
        "Finally, we use our new model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwkWKgvcXQ20"
      },
      "source": [
        "ft_model = \"ft:gpt-3.5-turbo-0613:pinecone::7s8gnk9R\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43IjsFNXQ2x"
      },
      "source": [
        "llm = ChatOpenAI(temperature=0.5, model_name=ft_model)\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\") or getpass(\n",
        "    \"Enter your Pinecone API key: \"\n",
        ")\n",
        "vdb = VectorDBChain(\n",
        "    index_name=\"llama-2-arxiv-papers\",\n",
        "    environment=os.getenv(\"PINECONE_ENV\") or \"us-east-1\",\n",
        "    pinecone_api_key=pinecone_api_key,\n",
        ")\n",
        "\n",
        "vdb_tool = Tool(\n",
        "    name=vdb.name,\n",
        "    func=vdb.query,\n",
        "    description=\"This tool allows you to get research information about LLMs.\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xndHtjmAXQ20"
      },
      "source": [
        "agent = initialize_agent(\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    tools=[vdb_tool],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    return_intermediate_steps=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdFVEhYQXQ21",
        "outputId": "aa2dd898-a0eb-4579-ec5a-b02e6b035d0e"
      },
      "source": [
        "agent(\"tell me about Llama 2?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVytkznkXQ21",
        "outputId": "41aa81d9-a0f3-4f2a-d24e-1b6e8997d727"
      },
      "source": [
        "agent(\"what makes llama 2 so special?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhH69sYdXQ21",
        "outputId": "bb91e4db-e673-41be-8182-17043988618d"
      },
      "source": [
        "agent(\"tell me about llama 2 red teaming?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYzR178ofUFJ"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "redacre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}