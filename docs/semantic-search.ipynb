{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Lc9I6taO3k"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)\n",
        "\n",
        "# Semantic Search\n",
        "\n",
        "In this walkthrough, we'll learn how to use Pinecone for semantic search using a multilingual translation dataset. We'll grab English sentences and search over a corpus of related sentences, aiming to find the relevant subset to our query.\n",
        "\n",
        "\n",
        "Semantic search is a form of retrieval that allows you to find documents that are similar in meaning to a given query, irrespective of the words used in each query. Semantic search is often in opposition to lexical search, where keywords are used to identify relevant documents to a given query, though it doesn't have to always be this way!\n",
        "\n",
        " It's super helpful for applications that require an understanding of a query's intent (such as when a user queries with a question over a corpus), or for when traditional lexical search doesn't work (such as in multimodal or multilingual applications).\n",
        "\n",
        "\n",
        "To begin, let's install the following libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q03L1BYEZQfe",
        "outputId": "19ee9678-91b3-4b53-ab2e-f1c6156fa01e"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  pinecone==6.0.2 \\\n",
        "  pinecone-notebooks==0.1.1 \\\n",
        "  datasets==3.5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xmobBcgqKEL"
      },
      "source": [
        "---\n",
        "\n",
        "ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrSfFiIC5roI"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API key not found in environment.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "def get_pinecone_api_key():\n",
        "    \"\"\"\n",
        "    Get Pinecone API key from environment variable or prompt user for input.\n",
        "    Returns the API key as a string.\n",
        "\n",
        "    Only necessary for this example. When using Pinecone yourself, \n",
        "    you can use environment variables or the like to set your API key.\n",
        "    \"\"\"\n",
        "    api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "    \n",
        "    if api_key is None:\n",
        "        try:\n",
        "            # Try Colab authentication if available\n",
        "            from pinecone_notebooks.colab import Authenticate\n",
        "            Authenticate()\n",
        "            # If successful, key will now be in environment\n",
        "            api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "        except ImportError:\n",
        "            # If not in Colab or authentication fails, prompt user for API key\n",
        "            print(\"Pinecone API key not found in environment.\")\n",
        "            api_key = getpass(\"Please enter your Pinecone API key: \")\n",
        "            # Save to environment for future use in session\n",
        "            os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "    \n",
        "    return api_key\n",
        "\n",
        "api_key = get_pinecone_api_key()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mc66NEBAcQHY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/pinecone-examples/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# Initialize client\n",
        "\n",
        "pc = Pinecone(\n",
        "        # You can remove this for your own projects!\n",
        "        source_tag=\"semantic_search_example_nb\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Pinecone Index with Integrated Inference\n",
        "\n",
        "Typically, semantic search requires three pieces: a processed data source (chunks, or records in Pinecone), an embedding model, and a vector database.\n",
        "\n",
        "Integrated Inference allows you to specify the creation of a Pinecone index with a specific Pinecone-hosted embedding model, which makes it easy to interact with the index. To learn more about Integrated Inference, including what other models are available, take a [look here](https://docs.pinecone.io/guides/get-started/overview#integrated-embedding).\n",
        "\n",
        "\n",
        "Here, we specify a free-tier index with the [multilingual-e5-large](https://docs.pinecone.io/models/multilingual-e5-large) embedding model. We also specify a mapping for what field in our\n",
        "records we will embed with this model. Then, we grab the index we just created for embedding later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT8pfoO46Iwg",
        "outputId": "0fec19be-c74d-4602-bec6-24d61cfc5bb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'english-sentences': {'vector_count': 416}},\n",
              " 'total_vector_count': 416,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "index_name = \"semantic-search\"\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index_for_model(\n",
        "        name=index_name,\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        embed={\n",
        "            \"model\":\"multilingual-e5-large\",\n",
        "            \"field_map\":{\"text\": \"chunk_text\"}\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Initialize index client\n",
        "index = pc.Index(name=index_name)\n",
        "\n",
        "# View index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're working with a small subset of a large multilingual dataset called Tatoeba. Tatoeba consists of hundreds of thousands of sentence translation pairs, and sometimes serves as a benchmark for crosslingual semantic search capabilities.\n",
        "\n",
        "In this notebook, we're just testing semantic search, so we'll grab a subset of english sentences that include the word \"park\".\n",
        "\n",
        "Why \"park\"? In English, park has multiple meanings which occur in different contexts. It could mean a place, such as a public park. Or, it could mean an actoin with a car (to park) or a place (park-ing spot). Semantic search using embedding models will naturally distinguise between these contexts, without invervention or lablel from our selves!\n",
        "\n",
        "This is the key benefit for semantic search; a way to abstract and represent the meaning of user queries without any additional work. \n",
        "\n",
        "And, since our embedding model is inherently multilingual, we can even do this semantic search across several languages without any additional work!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# specify that we want the english-spanish translation pairs\n",
        "tatoeba = load_dataset(\"Helsinki-NLP/tatoeba\", lang1=\"en\", lang2=\"es\", trust_remote_code=True, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords= [\"park\"]\n",
        "\n",
        "def simple_keyword_filter(sentence, keywords):\n",
        "  # filter for a list of keywords by sentence\n",
        "    for keyword in keywords:\n",
        "        if keyword in sentence:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def transform_dataset_for_pinecone(dataset, use_filter=True):\n",
        "    # Feel free to adjust this code to simulate a larger search!\n",
        "\n",
        "    if use_filter:\n",
        "        # filter for a list of keywords by sentence, helpful for building intuition on semantic search\n",
        "        translation_pairs = dataset.filter(lambda x: simple_keyword_filter(\n",
        "        sentence = x[\"translation\"][\"en\"], keywords=keywords))\n",
        "    else:\n",
        "        # use the full 200k+ dataset. Run only if you want to embed this many records!\n",
        "        translation_pairs = dataset\n",
        "\n",
        "    # flatten and shuffle for ease of use\n",
        "    translation_pairs = translation_pairs.flatten()\n",
        "    translation_pairs = translation_pairs.shuffle(seed=1)\n",
        "\n",
        "    english_sentences = translation_pairs.rename_column(\"translation.en\", \"text\").remove_columns(\"translation.es\")\n",
        "\n",
        "    # add lang column to indicate embedding origin\n",
        "    english_sentences = english_sentences.add_column(\"lang\", [\"en\"]*len(english_sentences))\n",
        "\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for idx, sentence in enumerate(english_sentences):\n",
        "        # Here, we create a record for each sentence in the dataset\n",
        "        # The record contains an ID and metadata fields which we can use to filter if desired\n",
        "        # The chunk_text field is the text we will embed\n",
        "        records.append(\n",
        "            {\n",
        "                \"id\": str(idx),\n",
        "                \"chunk_text\": sentence[\"text\"],\n",
        "                \"lang\": sentence[\"lang\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # convert to record format\n",
        "    return records\n",
        "\n",
        "\n",
        "records = transform_dataset_for_pinecone(tatoeba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upserting data into the Pinecone index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we embed and upsert the data into Pinecone. What this means is that each record we formatted above will interact with our embedding model we specified prior, and produce a vector embedding. Then, we take these embedding batches and store them in Pinecone with the additional information we specified, which is also known as metadata. \n",
        "\n",
        "Metadata is handy for things like filtering, like for if you stored several languages in the same index and want to return just one based on metadata. To learn more about metadata, take a [look here](https://docs.pinecone.io/guides/index-data/indexing-overview#metadata).\n",
        "\n",
        "We specify and create a namespace called \"english-sentences\", which is a higher level unit of organization when interacting with Pinecone. \n",
        "\n",
        "Querying on namespaces performs a sort of broad filter to only records that exist in that namespace, which has the nice effect of speeding up searches too. \n",
        "\n",
        "To learn more about namespaces, [look here](https://docs.pinecone.io/guides/index-data/indexing-overview#namespaces)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6c9efda7c7394404814e7466d4a9108c",
            "ee94a2a6c50b4abd89fdea7dc2c51b0b",
            "1af0d3d6e7a0465a91fd27a916a0508a",
            "86cf0f742de14c4f95fa5ddb74e07985",
            "ab1007c06e414b2da35507a90e91e8cb",
            "640ee2f088814e6c80014be268749820",
            "e1d5e0366eee4a96bf4a52ee84698c6a",
            "726dc10996c5403d8b0d661a3a6cdb63",
            "1e213f554e8f4c18a96bc71b56d8582a",
            "c62d67ae4d0e498fbf330db34ca8f9e3",
            "15e5e36e202c40fba34e92adfe8aa8de"
          ]
        },
        "id": "RhR6WOi1huXZ",
        "outputId": "9ae50bfe-3ebf-4d8b-e28e-3230aed0e1d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Upserting records batch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.18it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 96\n",
        "namespace = \"english-sentences\"\n",
        "\n",
        "\n",
        "# We upsert in batches of 96 to avoid hitting the embedding model's rate limit.\n",
        "# Libraries like backoff can be used here to handler large embedding jobs.\n",
        "\n",
        "for start in tqdm(range(0, len(records), batch_size), f\"Upserting records batch: \"):\n",
        "    index.upsert_records(records=records[start:start+batch_size], namespace = namespace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrK_IN079Vuu"
      },
      "source": [
        "## Making Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr4unPAq9alb"
      },
      "source": [
        "Now that our index is populated we can begin making queries. \n",
        "\n",
        "The tricky part about querying with semantic search is that we'd normally need to involve an embedding model here again too! \n",
        "\n",
        "But with Pinecone's integrated infernece, we can just invoke our index we created and send the text we want to search with there. Neat!\n",
        "\n",
        "Our goal here is to write a query sentence that uses one form of the word park, and find sentences that use park in a semantically similar manner. So, let's look for sentences about going to parks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: I have the afternoon off today, so I plan to go to the park, sit under a tree and read a book. Semantic Similarity Score: 0.8618775606155396\n",
            "\n",
            "Sentence: Let's go to the park where it's not noisy. Semantic Similarity Score: 0.8588659167289734\n",
            "\n",
            "Sentence: Let's go to the park where it is not noisy. Semantic Similarity Score: 0.8588587045669556\n",
            "\n",
            "Sentence: Let's go to the park where it isn't noisy. Semantic Similarity Score: 0.858812153339386\n",
            "\n",
            "Sentence: I go to the park. Semantic Similarity Score: 0.858041524887085\n",
            "\n",
            "Sentence: I'll go to the park. Semantic Similarity Score: 0.8502914905548096\n",
            "\n",
            "Sentence: I like going for a walk in the park. Semantic Similarity Score: 0.847651481628418\n",
            "\n",
            "Sentence: Let's take a walk in the park. Semantic Similarity Score: 0.8399631977081299\n",
            "\n",
            "Sentence: Who wants to go to the park? Semantic Similarity Score: 0.8391842842102051\n",
            "\n",
            "Sentence: Do you like to walk in the park? Semantic Similarity Score: 0.8343247771263123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "search_query = \"I want to go to the park and relax\"\n",
        "\n",
        "results = index.search(\n",
        "    namespace=namespace,\n",
        "    query={\n",
        "        \"top_k\": 10,\n",
        "        \"inputs\": {\n",
        "            'text': search_query\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "for result in results[\"result\"][\"hits\"]:\n",
        "    print(f'Sentence: {result[\"fields\"][\"chunk_text\"]} Semantic Similarity Score: {result[\"_score\"]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, let's use the other meaning of the word, park!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Where can I park? Semantic Similarity Score: 0.8843114376068115\n",
            "\n",
            "Sentence: Where can I park? Semantic Similarity Score: 0.8841626048088074\n",
            "\n",
            "Sentence: Where can we park? Semantic Similarity Score: 0.8696897625923157\n",
            "\n",
            "Sentence: Where can I park my car? Semantic Similarity Score: 0.8663355112075806\n",
            "\n",
            "Sentence: May I park here for a while? Semantic Similarity Score: 0.864980161190033\n",
            "\n",
            "Sentence: I'm double-parked. Could you hurry it up? Semantic Similarity Score: 0.8629273176193237\n",
            "\n",
            "Sentence: I'm double-parked. Could you hurry it up? Semantic Similarity Score: 0.8629273176193237\n",
            "\n",
            "Sentence: I'm double-parked. Could you hurry it up? Semantic Similarity Score: 0.8626684546470642\n",
            "\n",
            "Sentence: I'm double-parked. Could you hurry it up? Semantic Similarity Score: 0.8626684546470642\n",
            "\n",
            "Sentence: \"May I park here?\" \"No, you can't.\" Semantic Similarity Score: 0.8602052927017212\n",
            "\n"
          ]
        }
      ],
      "source": [
        "search_query = \"I need a place to park\"\n",
        "\n",
        "results = index.search(\n",
        "    namespace=namespace,\n",
        "    query={\n",
        "        \"top_k\": 10,\n",
        "        \"inputs\": {\n",
        "            'text': search_query\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "for result in results[\"result\"][\"hits\"]:\n",
        "    print(f'Sentence: {result[\"fields\"][\"chunk_text\"]} Semantic Similarity Score: {result[\"_score\"]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wait, how is this working?\n",
        "\n",
        "When performing semantic search with Pinecone's vector database, you are asking the following question: Given this query vector, what are the closest vectors to it in the database?\n",
        "\n",
        "Because of the way embedding model are trained, this closeness in vector space corresponds to similarity in meaning. The exact metric used for our implementation is cosine similarity, which is simply the angle between the input vector and a document vector. For small amounts of vectors, this task is trivial, but what happens when you have hundreds of thousands, millions or even billions? And what about query latency?\n",
        "\n",
        "The magic of Pinecone's vector database is advanced algorithms that can quickly index and do this search on billion-scale vectors effectively!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo Cleanup\n",
        "\n",
        "You can go ahead and ask more queries above. When you're done, delete the index to save resources. \n",
        "\n",
        "Congrats, you've just implemented semantic search with Pinecone!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-cWdeKzhAtww"
      },
      "outputs": [],
      "source": [
        "#pc.delete_index(name=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B0zxR6hbf5d"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
