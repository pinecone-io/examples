{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87700e2",
   "metadata": {},
   "source": [
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/startakovsky/pinecone-examples-fork/blob/may-2022-semantic-text-search-refresh/semantic_text_search/semantic_text_search_refresh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24e16b",
   "metadata": {},
   "source": [
    "# Semantic Text Search Demo, with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8253e2",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010267d",
   "metadata": {},
   "source": [
    "### What is Semantic Search?\n",
    "\n",
    "_Semantic search_ is exactly the kind of search where the _meaning_ of the search query is the thing that's used, rather than it being done by keyword lookups. Pretrained neural networks on large sets of text data have been shown to be very effective at encoding the _meaning_ of a particular phrase, sentence, paragraph or long document into a data structure known as a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68935be8",
   "metadata": {},
   "source": [
    "### How will we demonstrate and apply this example.\n",
    "\n",
    "We are going to use Pinecone's semantic search capabilities with an off-the-shelf and a pretrained model to curate custom categories of previously-aired Jeopardy questions. We will show how Pinecone makes it easy to ensure that question difficulty is on par with how the question was originally priced by filtering on question metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228d497",
   "metadata": {},
   "source": [
    "### Learning Goals\n",
    "_By the end of this demo, you will have:_\n",
    " 1. Learned about Pinecone's value for solving realtime semantic search requirements!\n",
    " 2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.\n",
    " 3. Encoded Jeopardy Questions as 384-dimensional vectors using a pretrained, encoder-only, model (i.e. no model training necessary).\n",
    " 4. Queried Pinecone's Vector Database on Jeopardy Questions that are semantically similar to the query.\n",
    " 5. Used Pinecone's metadata filtering capability to ensure that each category you create will be on parity with difficulty used when the question originally aired (each category will contain questions of ranging difficulty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99b115",
   "metadata": {},
   "source": [
    "## Setup: Prerequisites and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60687f1",
   "metadata": {},
   "source": [
    "### Python 3.7+\n",
    "\n",
    "This code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4382e8",
   "metadata": {},
   "source": [
    "### Importing the helper modules\n",
    "\n",
    "This notebook is self-contained, and as such, if background Python modules are not present, they will be imported from [Pinecone's Example repository](https://github.com/pinecone-io/examples/tree/master/semantic_text_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609277d",
   "metadata": {},
   "source": [
    "### Acquiring your Pinecone API Key\n",
    "\n",
    "A Pinecone API key is required. You can obtain one for free on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cb344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Extracting API Key from environmental variable `PINECONE_EXAMPLE_API_KEY`..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Pinecone API Key available at `h.pinecone_api_key`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import httpimport\n",
    "\n",
    "if os.path.isfile('helper.py'):\n",
    "    import helper as h\n",
    "else:\n",
    "    print('importing `helper.py` from https://github.com/pinecone-io')\n",
    "    with httpimport.github_repo(\n",
    "        username='startakovsky', \n",
    "        repo='pinecone-examples-fork',\n",
    "        module=['semantic_text_search'],\n",
    "        branch='master'):\n",
    "        from semantic_text_search import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfb9e5",
   "metadata": {},
   "source": [
    "### Installing and Importing Prerequisite Libraries:\n",
    "Python libraries [pinecone-client](https://pypi.org/project/pinecone-client/), [sentence_transformers](https://pypi.org/project/sentence-transformers/), [pandas](https://pypi.org/project/pandas/), and [tqdm](https://pypi.org/project/tqdm/) are required for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12400a",
   "metadata": {},
   "source": [
    "#### Installing via `pip`\n",
    "The next line is equivalent to `pip install pinecone-client sentence-transformers pandas tqdm`. Note that _sys.executable_ is a way of ensuring it's the version of pip associated with this Jupyter Notebook's Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1fa96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone-client sentence-transformers pandas tqdm datasets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441c1df",
   "metadata": {},
   "source": [
    "#### Importing and Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7281780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import tqdm\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "INDEX_NAME, INDEX_DIMENSION = 'semantic-text-search', 384\n",
    "MODEL_NAME = 'sentence-transformers/msmarco-MiniLM-L6-cos-v5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb545fe0",
   "metadata": {},
   "source": [
    "### Downloading and Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5472c",
   "metadata": {},
   "source": [
    "#### Downloading data\n",
    "The [Jeopardy Dataset](https://huggingface.co/datasets/jeopardy) has over 200,000 rows and will be downloaded using the `datasets` library from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648bb292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jeopardy (/Users/steven/.cache/huggingface/datasets/jeopardy/default/0.1.0/774efb3257b2f482b1974faa754e6ce11853ad625a9b364e29f106052afe0204)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e552762e124eb9ab442bf6ae1c6937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"jeopardy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f916fa",
   "metadata": {},
   "source": [
    "#### The preprocessing step is self-explanatory and defined in the helper module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4faa10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df = h.get_processed_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc22766",
   "metadata": {},
   "source": [
    "#### Sample row from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff4d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>146354</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>AUTOBIOGRAPHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_date</th>\n",
       "      <td>2007-11-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>'A psychologist:&lt;br /&gt;1962's \"Memories, Dreams, Reflections\"'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Carl Jung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <td>Double Jeopardy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show_number</th>\n",
       "      <td>5330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_to_encode</th>\n",
       "      <td>'A psychologist:&lt;br /&gt;1962's \"Memories, Dreams, Reflections\"' Carl Jung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 146354\n",
       "category                                                                AUTOBIOGRAPHERS\n",
       "air_date                                                            2007-11-09 00:00:00\n",
       "question                  'A psychologist:<br />1962's \"Memories, Dreams, Reflections\"'\n",
       "amount                                                                             1200\n",
       "answer                                                                        Carl Jung\n",
       "round                                                                  Double Jeopardy!\n",
       "show_number                                                                        5330\n",
       "year                                                                               2007\n",
       "month                                                                                11\n",
       "text_to_encode  'A psychologist:<br />1962's \"Memories, Dreams, Reflections\"' Carl Jung"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.iloc[123456])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57c70f",
   "metadata": {},
   "source": [
    "### Creating your Pinecone Index\n",
    "The process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector. As we will see below, the model we are using maps each piece of text to a 384-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d03750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=h.pinecone_api_key, environment='us-west1-gcp')\n",
    "# pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION)\n",
    "index = pinecone.Index(index_name=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f0219",
   "metadata": {},
   "source": [
    "## Generate embeddings and send them to your Pinecone Index\n",
    "This will all be done in batches. We will compute embeddings in batch, followed by taking each batch and sending it to Pinecone, also in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbb5e8",
   "metadata": {},
   "source": [
    "### Loading a Pretrained Encoder model.\n",
    "We will generate embeddings by using [this Sentence Transformers model](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5). It is one of hundreds encoder models available. Downloads happen automatically with SentenceTransformer, and may take up to a minute the first time. After this first import, the model is cached and available on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ed4a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading model from _Sentence Transformers_: `sentence-transformers/msmarco-MiniLM-L6-cos-v5` from Sentence Transformers..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Model loaded."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h.printmd(f'Loading model from _Sentence Transformers_: `{MODEL_NAME}` from Sentence Transformers...')\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "h.printmd('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d4005",
   "metadata": {},
   "source": [
    "### MSMARCO model v5 and Embeddings\n",
    "\n",
    "In this example, we created an index with 384 dimensions because that is what the output is of this MSMARCO model. In fact, particular MSMARCO model used in this example generates [unit vectors](https://en.wikipedia.org/wiki/Unit_vector), which make [vector comparisons](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) agnostic to one's choice of similarity scores. In other words, when defining the index, it does not matter whether we use `euclidean`, `cosine` or `dotproduct` as a metric, so we left it blank, using the `cosine` default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a13a56",
   "metadata": {},
   "source": [
    "#### On Embeddings\n",
    "\n",
    "The output of this model's encodings are 384-dimensional, which was known in advance of creating above index.\n",
    "\n",
    "So, when a piece of text such as \"A quick fox jumped around\" gets encoded into a vector embedding, the result is a sequence of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16d277",
   "metadata": {},
   "source": [
    "#### On Comparing Embeddings aka _how_ Semantic Search works\n",
    "\n",
    "Two 15-dimensional text embeddings might look like something like: \n",
    " - _\\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\]_\n",
    " - _\\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]_\n",
    " \n",
    "In order to determine how _similar_ we may use something like [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity). This calculation is trivial when comparing two vectors, but nontrivial when needing to compare one vector against millions or billions of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71e442",
   "metadata": {},
   "source": [
    "### What is Pinecone for?\n",
    "Often, there is a technical requirement to run a comparison of one vector to millions of others and return the most similar results in real time, with a latency of tens of milliseconds and at a high throughput. Pinecone solves this  problem with its managed vector database service, and we will demonstrate this below. Additionally, Pinecone offers the ability to filter by metadata as well as providing high-availability replication that will scale up (and down) as needed. We will demonstrate the metadata capability below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46933021",
   "metadata": {},
   "source": [
    "### Prepare vector embeddings for upload\n",
    "\n",
    "This may take a while depending on your machine. If on a recent MacBookPro or Google Colab, this may take up to one hour, sometimes longer.\n",
    "\n",
    "#### Prepare metadata\n",
    "\n",
    "The function below creates metadata from a single row of the dataframe. We will demonstrate Pinecone's ability to filter by this metadata shortly. This is going to be important for forming the Jeopardy Categories with questions ranging from `$200` up to `$2000` difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330d389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_metadata_from_dataframe_row(df_row):\n",
    "    \"\"\"Return pinecone vector.\"\"\"\n",
    "    vector_metadata = {\n",
    "        'year': df_row['year'],\n",
    "        'month': df_row['month'],\n",
    "        'round': df_row['round'],\n",
    "        'amount': df_row['amount']\n",
    "    }\n",
    "    return vector_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e0478",
   "metadata": {},
   "source": [
    "#### Prepare all vector data for upload\n",
    "\n",
    "The function below will take a portion of the dataframe and create the full vector data as Pinecone expects it for [upsert](https://www.pinecone.io/docs/insert-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c3862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_to_upload_to_pinecone(df_chunk, model):\n",
    "    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n",
    "    # create embeddings\n",
    "    pool = model.start_multi_process_pool()\n",
    "    vector_values = model.encode_multi_process(df_chunk['text_to_encode'], pool).tolist()\n",
    "    model.stop_multi_process_pool(pool)\n",
    "    # create vector ids and metadata\n",
    "    vector_ids = df_chunk.index.tolist()\n",
    "    vector_metadata = df_chunk.apply(get_vector_metadata_from_dataframe_row,axis=1).tolist()\n",
    "    return list(zip(vector_ids, vector_values, vector_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cc6d7",
   "metadata": {},
   "source": [
    "### Upload data to Pinecone in asynchronous batches\n",
    "\n",
    "The function below iterates through the dataframe in chunks, and for each of those chunks, will upload asynchronously in sub-chunks to your Pinecone Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256e593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_pinecone_in_chunks(\n",
    "    dataframe, \n",
    "    pinecone_index, \n",
    "    model, \n",
    "    chunk_size=20000, \n",
    "    upsert_size=500):\n",
    "    \"\"\"Encode dataframe column `text_to_encode` to dense vector and upsert to Pinecone.\"\"\"\n",
    "    tqdm_kwargs = h.get_tqdm_kwargs(dataframe, chunk_size)\n",
    "    async_results = collections.defaultdict(list)\n",
    "    for df_chunk in tqdm.notebook.tqdm(h.chunks(dataframe, chunk_size), **tqdm_kwargs):\n",
    "        vectors = get_vectors_to_upload_to_pinecone(df_chunk, model)\n",
    "        # upload to Pinecone in batches of `upsert_size`\n",
    "        for vectors_chunk in h.chunks(vectors, upsert_size):\n",
    "            start_index_chunk = df_chunk.index[0]\n",
    "            async_result = pinecone_index.upsert(vectors_chunk, async_req=True)\n",
    "            async_results[start_index_chunk].append(async_result)\n",
    "        # wait for results\n",
    "        _ = [async_result.get() for async_result in async_results[start_index_chunk]]\n",
    "        is_all_successful = all(map(lambda x: x.successful(), async_results[start_index_chunk]))\n",
    "        # report chunk upload status\n",
    "        print(\n",
    "        f'All upserts in chunk successful with index starting with {start_index_chunk:>7}: '\n",
    "        f'{is_all_successful}. Vectors uploaded: {len(vectors):>3}.'\n",
    "        )\n",
    "    return async_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ef5c8",
   "metadata": {},
   "source": [
    "#### Asynchronous Upload\n",
    "Computing the embeddings may take up to an hour depending on hardware capabilities. The Pinecone API responds right away with the status of each request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba75643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# async_results = upload_dataframe_to_pinecone_in_chunks(df, index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eddac13",
   "metadata": {},
   "source": [
    "### Visualize the status of your upserts in the Pinecone Console\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/startakovsky/pinecone-examples-fork/may-2022-semantic-text-search-refresh/semantic_text_search/pinecone_console.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6de60",
   "metadata": {},
   "source": [
    "## Querying Pinecone\n",
    "\n",
    "Now that all the embeddings of the texts are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast semantic search query capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b95d3",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Below we have some helper functions, that convert text to a vector embedding, send the vector to Pinecone along with a metadata rule of how to filter the request, and converting the Pinecone API JSON response to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbd6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(response):\n",
    "    \"\"\"Return ids from results.\"\"\"\n",
    "    matches = response['results'][0]['matches']\n",
    "    return [match['id'] for match in matches]\n",
    "\n",
    "def get_query_results_from_api_response(dataframe, response, query):\n",
    "    \"\"\"Return pandas.DataFrame containing query_results with original text.\"\"\"\n",
    "    response_df = dataframe.loc[get_ids(response), ['question', 'answer', 'amount']]\n",
    "    response_df['query'] = query\n",
    "    return response_df\n",
    "\n",
    "def get_query_results(query, pinecone_index, dataframe, model, top_k=1, filter_criteria=None):\n",
    "    embedding = model.encode(query).tolist()\n",
    "    response = pinecone_index.query(\n",
    "        [embedding],\n",
    "        top_k=top_k,\n",
    "        filter=filter_criteria,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    return get_query_results_from_api_response(df, response, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f98f5",
   "metadata": {},
   "source": [
    "### Example Usage for a single query\n",
    "\n",
    "#### _**I'd like 'Ocean Gods' for \\\\$800 please, Alex\\!**_\n",
    "\n",
    "In the below example we query Pinecone's API with an embedding of the term _ocean gods_ find a few embeddings that have the highest similarity score to the embedding of _ocean gods_ such that it was valued at \\\\$800."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee4dc9",
   "metadata": {},
   "source": [
    "#### Pinecone Example Request\n",
    "\n",
    "Note how the [metadata](https://www.pinecone.io/learn/vector-search-filtering/) is taken into account because here we were looking for questions of `$800` difficulty.\n",
    "\n",
    "```python\n",
    "pinecone_index.query(\n",
    "        [embedding], \n",
    "        top_k=5, \n",
    "        filter={'amount': {'$eq': 800}},\n",
    "        include_metadata=True,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737b87a",
   "metadata": {},
   "source": [
    "#### Pinecone Example Response\n",
    "\n",
    "```json\n",
    "{'results': [{'matches': [{'id': '28440',\n",
    "                           'metadata': {'amount': 800.0,\n",
    "                                        'month': '03',\n",
    "                                        'round': 'Double Jeopardy!',\n",
    "                                        'year': '2010'},\n",
    "                           'score': 0.517167628,\n",
    "                           'values': []},\n",
    "                          {'id': '15671',\n",
    "                           'metadata': {'amount': 800.0,\n",
    "                                        'month': '12',\n",
    "                                        'round': 'Double Jeopardy!',\n",
    "                                        'year': '2011'},\n",
    "                           'score': 0.45932579,\n",
    "                           'values': []},\n",
    "                          {'id': '86169',\n",
    "                           'metadata': {'amount': 800.0,\n",
    "                                        'month': '09',\n",
    "                                        'round': 'Double Jeopardy!',\n",
    "                                        'year': '2002'},\n",
    "                           'score': 0.450626254,\n",
    "                           'values': []}],\n",
    "              'namespace': ''}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93ba5f",
   "metadata": {},
   "source": [
    "#### Pinecone Enriched Example Response\n",
    "Let's run the original code, processing the response style we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0906ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>amount</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28440</th>\n",
       "      <td>'The Angolan sea god Kianda is the guardian of this nearby ocean &amp; all its creatures'</td>\n",
       "      <td>the Atlantic Ocean</td>\n",
       "      <td>800</td>\n",
       "      <td>ocean gods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15671</th>\n",
       "      <td>'Quoth the Bible, Noah released 2 birds from the Ark to test for dry land, a dove &amp; one of these'</td>\n",
       "      <td>a raven</td>\n",
       "      <td>800</td>\n",
       "      <td>ocean gods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86169</th>\n",
       "      <td>'Pantheists say the universe &amp; this are identical; by definition, atheists don't believe in it at all'</td>\n",
       "      <td>God</td>\n",
       "      <td>800</td>\n",
       "      <td>ocean gods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175086</th>\n",
       "      <td>'Despite their name, spring these, caused by alignment of the Sun, Moon &amp; Earth, happen in the ocean in every season'</td>\n",
       "      <td>tides</td>\n",
       "      <td>800</td>\n",
       "      <td>ocean gods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181889</th>\n",
       "      <td>'(&lt;a href=\"http://www.j-archive.com/media/2010-06-11_DJ_12.jpg\" target=\"_blank\"&gt;Jimmy of the Clue Crew shows a map on the monitor.&lt;/a&gt;)  The Red Sea, the Gulf of Aden &amp; the Indian Ocean form the eastern coast of the region seen &lt;a href=\"http://www.j-archive.com/media/2010-06-11_DJ_12a.jpg\" target=\"_blank\"&gt;here&lt;/a&gt;, known by this 3-word term'</td>\n",
       "      <td>the Horn of Africa</td>\n",
       "      <td>800</td>\n",
       "      <td>ocean gods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                          question  \\\n",
       "vector_id                                                                                                                                                                                                                                                                                                                                                            \n",
       "28440                                                                                                                                                                                                                                                                        'The Angolan sea god Kianda is the guardian of this nearby ocean & all its creatures'   \n",
       "15671                                                                                                                                                                                                                                                            'Quoth the Bible, Noah released 2 birds from the Ark to test for dry land, a dove & one of these'   \n",
       "86169                                                                                                                                                                                                                                                       'Pantheists say the universe & this are identical; by definition, atheists don't believe in it at all'   \n",
       "175086                                                                                                                                                                                                                                       'Despite their name, spring these, caused by alignment of the Sun, Moon & Earth, happen in the ocean in every season'   \n",
       "181889     '(<a href=\"http://www.j-archive.com/media/2010-06-11_DJ_12.jpg\" target=\"_blank\">Jimmy of the Clue Crew shows a map on the monitor.</a>)  The Red Sea, the Gulf of Aden & the Indian Ocean form the eastern coast of the region seen <a href=\"http://www.j-archive.com/media/2010-06-11_DJ_12a.jpg\" target=\"_blank\">here</a>, known by this 3-word term'   \n",
       "\n",
       "                       answer  amount       query  \n",
       "vector_id                                          \n",
       "28440      the Atlantic Ocean     800  ocean gods  \n",
       "15671                 a raven     800  ocean gods  \n",
       "86169                     God     800  ocean gods  \n",
       "175086                  tides     800  ocean gods  \n",
       "181889     the Horn of Africa     800  ocean gods  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_query_results('ocean gods', index, df, model, top_k=5, filter_criteria={'amount': {'$eq': 800}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8f76c",
   "metadata": {},
   "source": [
    "## Building Custom Jeopardy Boards\n",
    "\n",
    "Here is a summary of the following helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553130a4",
   "metadata": {},
   "source": [
    "### Pinecone Query for All Question Difficulties\n",
    "The following functions scale up the previous example, wrangles the output into the form of two Jeopardy rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fad697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_jeopardy_questions(queries, pinecone_index, dataframe, model):\n",
    "    \"\"\"Return the questions to be used by making API requests to Pinecone.\"\"\"\n",
    "    df_list = []\n",
    "    get_single_result = lambda query, amount: get_query_results(\n",
    "        query,\n",
    "        pinecone_index,\n",
    "        dataframe,\n",
    "        model,\n",
    "        filter_criteria={'amount': {'$eq': amount}}\n",
    "    )\n",
    "    for query, amount in itertools.product(queries, h.JEOPARDY_STANDARD_AMOUNTS):\n",
    "        results = get_single_result(query, amount)\n",
    "        df_list.append(results)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c1cc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"over the moon\", \"united states\", \"invention of computer\"]\n",
    "jeopardy_questions = get_jeopardy_questions(queries, index, df, model)\n",
    "jeopardy_board, double_jeopardy_board = h.get_jeopardy_boards(jeopardy_questions, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480cf1b",
   "metadata": {},
   "source": [
    "### Jeopardy! Round 1 Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860e5675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>over the moon</th>\n",
       "      <th>united states</th>\n",
       "      <th>invention of computer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>'Any song about Earth's natural satellite'</td>\n",
       "      <td>'Canada'</td>\n",
       "      <td>'The Woz, Steve Wozniak, built the first computer for this company'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>'The only moon in our solar system that astrologists say has an influence circles this planet'</td>\n",
       "      <td>'Number of U.S. states divided by the number of noncontiguous states'</td>\n",
       "      <td>'On April 1, 1976 2 engineers with $1,300 in capital began this computer company in Cupertino, California'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>'On July 20, 1969 Ryan Seacrest described the lunar surface as \"magnificent desolation\"; 2nd man on Moon, out!'</td>\n",
       "      <td>'The westernmost country in North America'</td>\n",
       "      <td>'Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>'The Apollo lunar mission with this number was aborted en route to the Moon in 1970 due to an in-flight explosion'</td>\n",
       "      <td>'Skyy'</td>\n",
       "      <td>'Originally an adding machine maker, in 1944 IBM made its first steps toward one of these with the Harvard Mark I'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>'You could say we sent this Greek god to the moon in 1969'</td>\n",
       "      <td>'It's the only U.S. state that fits the category'</td>\n",
       "      <td>'Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             over the moon  \\\n",
       "amount                                                                                                                       \n",
       "200                                                                             'Any song about Earth's natural satellite'   \n",
       "400                         'The only moon in our solar system that astrologists say has an influence circles this planet'   \n",
       "600        'On July 20, 1969 Ryan Seacrest described the lunar surface as \"magnificent desolation\"; 2nd man on Moon, out!'   \n",
       "800     'The Apollo lunar mission with this number was aborted en route to the Moon in 1970 due to an in-flight explosion'   \n",
       "1000                                                            'You could say we sent this Greek god to the moon in 1969'   \n",
       "\n",
       "                                                                united states  \\\n",
       "amount                                                                          \n",
       "200                                                                  'Canada'   \n",
       "400     'Number of U.S. states divided by the number of noncontiguous states'   \n",
       "600                                'The westernmost country in North America'   \n",
       "800                                                                    'Skyy'   \n",
       "1000                        'It's the only U.S. state that fits the category'   \n",
       "\n",
       "                                                                                                     invention of computer  \n",
       "amount                                                                                                                      \n",
       "200                                                    'The Woz, Steve Wozniak, built the first computer for this company'  \n",
       "400             'On April 1, 1976 2 engineers with $1,300 in capital began this computer company in Cupertino, California'  \n",
       "600           'Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401'  \n",
       "800     'Originally an adding machine maker, in 1944 IBM made its first steps toward one of these with the Harvard Mark I'  \n",
       "1000                      'Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes'  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy_board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40f4d1",
   "metadata": {},
   "source": [
    "### Double Jeopardy! Round 2 Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1877d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>over the moon</th>\n",
       "      <th>united states</th>\n",
       "      <th>invention of computer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>'High-aiming standard heard here'</td>\n",
       "      <td>'The third largest'</td>\n",
       "      <td>'Long before it was the name of a computer, it was Brit speak for a raincoat'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>'It's the name shared by an Irish revolutionary &amp; the pilot of the command module during the first moon landing'</td>\n",
       "      <td>'Number of U.S. states that begin with the letter I'</td>\n",
       "      <td>'In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>'The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon &amp; the sun'</td>\n",
       "      <td>'On a map of the U.S., 1 of the 2 states that each border 8 other states'</td>\n",
       "      <td>'As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>'On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1'</td>\n",
       "      <td>'The Rio Grande forms part of the border between these 2 U.S. states'</td>\n",
       "      <td>'This technology used for wireless headsets is named after a Danish king who united parts of Scandinavia'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>'Lunar object over a Gulf of Guinea nation'</td>\n",
       "      <td>'Total number of U.S. states that begin and end with the same letter; (hint: they all start with vowels)'</td>\n",
       "      <td>'The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             over the moon  \\\n",
       "amount                                                                                                                       \n",
       "1200                                                                                     'High-aiming standard heard here'   \n",
       "1400      'It's the name shared by an Irish revolutionary & the pilot of the command module during the first moon landing'   \n",
       "1600             'The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon & the sun'   \n",
       "1800    'On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1'   \n",
       "2000                                                                           'Lunar object over a Gulf of Guinea nation'   \n",
       "\n",
       "                                                                                                    united states  \\\n",
       "amount                                                                                                              \n",
       "1200                                                                                          'The third largest'   \n",
       "1400                                                         'Number of U.S. states that begin with the letter I'   \n",
       "1600                                    'On a map of the U.S., 1 of the 2 states that each border 8 other states'   \n",
       "1800                                        'The Rio Grande forms part of the border between these 2 U.S. states'   \n",
       "2000    'Total number of U.S. states that begin and end with the same letter; (hint: they all start with vowels)'   \n",
       "\n",
       "                                                                                                         invention of computer  \n",
       "amount                                                                                                                          \n",
       "1200                                             'Long before it was the name of a computer, it was Brit speak for a raincoat'  \n",
       "1400    'In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant'  \n",
       "1600           'As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne'  \n",
       "1800                 'This technology used for wireless headsets is named after a Danish king who united parts of Scandinavia'  \n",
       "2000                'The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market'  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_jeopardy_board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50b53e",
   "metadata": {},
   "source": [
    "### Looking Up Answers\n",
    "Before running the next cell, see if you can look up the answer to this question: \"Over the moon for 400\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae7f4b69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a Cameroon moon'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, amount = \"over the moon\", 2000\n",
    "id_ = jeopardy_questions.index[(jeopardy_questions['query'] == query) & (jeopardy_questions['amount'] == amount)][0]\n",
    "jeopardy_questions.at[id_, 'answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
