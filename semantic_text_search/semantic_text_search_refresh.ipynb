{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87700e2",
   "metadata": {},
   "source": [
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/startakovsky/pinecone-examples-fork/blob/may-2022-semantic-text-search-refresh/semantic_text_search/semantic_text_search_refresh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24e16b",
   "metadata": {},
   "source": [
    "# Do-it-yourself Jeopardy Board Curation with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8253e2",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010267d",
   "metadata": {},
   "source": [
    "### What is Semantic Search and how will we use it?\n",
    "\n",
    "_Semantic search_ is exactly the kind of search where the _meaning_ of the search query is the thing that's used, rather than it being done by keyword lookups. Pretrained neural networks on large sets of text data have been shown to be very effective at encoding the _meaning_ of a particular phrase, sentence, paragraph or long document into a data structure known as a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/).\n",
    "\n",
    "We are going to use Pinecone's semantic search capabilities with an off-the-shelf and a pretrained model to curate custom categories of previously-aired Jeopardy questions. We will show how Pinecone makes it easy to ensure that question difficulty is on par with how the question was originally priced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228d497",
   "metadata": {},
   "source": [
    "### Learning Goals and Estimated Reading Time\n",
    "_By the end of this 10 minute demo, you will have:_\n",
    " 1. Learned about Pinecone's value for solving realtime semantic search requirements!\n",
    " 2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.\n",
    " 3. Encoded Jeopardy Questions as 384-dimensional vectors using a pretrained, encoder-only, model (i.e. no model training necessary).\n",
    " 4. Queried Pinecone's Vector Database on Jeopardy Questions that are semantically similar to the query.\n",
    " 5. Bonus for the Interested Reader: Near-Instant Custom Jeopardy Board Creation With Increasing Question Difficulty!\n",
    " \n",
    " If you want to execute the code yourself either in Google Colab or your computer, it may take up to an hour depending on processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99b115",
   "metadata": {},
   "source": [
    "## Setup: Prerequisites and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60687f1",
   "metadata": {},
   "source": [
    "### Python 3.7+\n",
    "\n",
    "This code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4382e8",
   "metadata": {},
   "source": [
    "### Importing the helper modules\n",
    "\n",
    "This notebook is self-contained, and as such, if background Python modules are not present, they will be imported from [Pinecone's Example repository](https://github.com/pinecone-io/examples/tree/master/semantic_text_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609277d",
   "metadata": {},
   "source": [
    "### Acquiring your Pinecone API Key\n",
    "\n",
    "A Pinecone API key is required. You can obtain one for free on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cb344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Extracting API Key from environmental variable `PINECONE_EXAMPLE_API_KEY`..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Pinecone API Key available at `h.pinecone_api_key`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import httpimport\n",
    "\n",
    "if os.path.isfile('helper.py'):\n",
    "    import helper as h\n",
    "else:\n",
    "    print('importing `helper.py` from https://github.com/pinecone-io')\n",
    "    with httpimport.github_repo(\n",
    "        username='startakovsky', \n",
    "        repo='pinecone-examples-fork',\n",
    "        module=['semantic_text_search'],\n",
    "        branch='master'):\n",
    "        from semantic_text_search import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfb9e5",
   "metadata": {},
   "source": [
    "### Installing and Importing Prerequisite Libraries:\n",
    "Python libraries [pinecone-client](https://pypi.org/project/pinecone-client/), [sentence_transformers](https://pypi.org/project/sentence-transformers/), [pandas](https://pypi.org/project/pandas/), and [tqdm](https://pypi.org/project/tqdm/) are required for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12400a",
   "metadata": {},
   "source": [
    "#### Installing via `pip`\n",
    "The next line is equivalent to `pip install pinecone-client sentence-transformers pandas tqdm`. Note that _sys.executable_ is a way of ensuring it's the version of pip associated with this Jupyter Notebook's Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1fa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-client sentence-transformers pandas tqdm datasets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441c1df",
   "metadata": {},
   "source": [
    "#### Importing and Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7281780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import tqdm\n",
    "import pinecone\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "INDEX_NAME, INDEX_DIMENSION = 'semantic-text-search', 384\n",
    "MODEL_NAME = 'sentence-transformers/msmarco-MiniLM-L6-cos-v5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfefda6",
   "metadata": {},
   "source": [
    "### Downloading and Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb4ccc",
   "metadata": {},
   "source": [
    "#### Downloading data\n",
    "The [Jeopardy Dataset](https://huggingface.co/datasets/jeopardy) has over 200,000 rows and will be downloaded using the `datasets` library from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jeopardy (/Users/steven/.cache/huggingface/datasets/jeopardy/default/0.1.0/774efb3257b2f482b1974faa754e6ce11853ad625a9b364e29f106052afe0204)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899f6cfbe30d4c2f9f2729521f699ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"jeopardy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f916fa",
   "metadata": {},
   "source": [
    "#### The preprocessing step is self-explanatory and defined in the helper module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919c98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df = h.get_processed_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc22766",
   "metadata": {},
   "source": [
    "#### Sample row from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff4d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>146354</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>AUTOBIOGRAPHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_date</th>\n",
       "      <td>2007-11-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>'A psychologist:&lt;br /&gt;1962's \"Memories, Dreams, Reflections\"'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Carl Jung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <td>Double Jeopardy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show_number</th>\n",
       "      <td>5330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_to_encode</th>\n",
       "      <td>'A psychologist:&lt;br /&gt;1962's \"Memories, Dreams, Reflections\"' Carl Jung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 146354\n",
       "category                                                                AUTOBIOGRAPHERS\n",
       "air_date                                                            2007-11-09 00:00:00\n",
       "question                  'A psychologist:<br />1962's \"Memories, Dreams, Reflections\"'\n",
       "amount                                                                             1200\n",
       "answer                                                                        Carl Jung\n",
       "round                                                                  Double Jeopardy!\n",
       "show_number                                                                        5330\n",
       "year                                                                               2007\n",
       "month                                                                                11\n",
       "text_to_encode  'A psychologist:<br />1962's \"Memories, Dreams, Reflections\"' Carl Jung"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.iloc[123456])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57c70f",
   "metadata": {},
   "source": [
    "### Creating your Pinecone Index\n",
    "The process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector. As we will see below, the model we are using maps each piece of text to a 384-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d03750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=h.pinecone_api_key, environment='us-west1-gcp')\n",
    "# pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION)\n",
    "index = pinecone.Index(index_name=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f0219",
   "metadata": {},
   "source": [
    "## Generate embeddings and send them to your Pinecone Index\n",
    "This will all be done in batches. We will compute embeddings in batch, followed by taking each batch and sending it to Pinecone, also in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbb5e8",
   "metadata": {},
   "source": [
    "### Loading a Pretrained Encoder model.\n",
    "We will generate embeddings by using [this Sentence Transformers model](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5). It is one of hundreds encoder models available. Downloads happen automatically with SentenceTransformer, and may take up to a minute the first time. After this first import, the model is cached and available on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ed4a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading model from _Sentence Transformers_: `sentence-transformers/msmarco-MiniLM-L6-cos-v5` from Sentence Transformers..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Model loaded."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h.printmd(f'Loading model from _Sentence Transformers_: `{MODEL_NAME}` from Sentence Transformers...')\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "h.printmd('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d4005",
   "metadata": {},
   "source": [
    "### MSMARCO model v5 and Embeddings\n",
    "\n",
    "In this example, we created an index with 384 dimensions because that is what the output is of this MSMARCO model. In fact, particular MSMARCO model used in this example generates [unit vectors](https://en.wikipedia.org/wiki/Unit_vector), which make [vector comparisons](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) agnostic to one's choice of similarity scores. In other words, when defining the index, it does not matter whether we use `euclidean`, `cosine` or `dotproduct` as a metric, so we left it blank, using the `cosine` default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a13a56",
   "metadata": {},
   "source": [
    "#### On Embeddings\n",
    "\n",
    "The output of this model's encodings are 384-dimensional, which was known in advance of creating above index.\n",
    "\n",
    "So, when a piece of text such as \"A quick fox jumped around\" gets encoded into a vector embedding, the result is a sequence of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16d277",
   "metadata": {},
   "source": [
    "#### On Comparing Embeddings aka _how_ Semantic Search works\n",
    "\n",
    "Two 15-dimensional text embeddings might look like something like: \n",
    " - _\\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\]_\n",
    " - _\\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]_\n",
    " \n",
    "In order to determine how _similar_ we may use something like [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity). This calculation is trivial when comparing two vectors, but nontrivial when needing to compare one vector against millions or billions of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71e442",
   "metadata": {},
   "source": [
    "### What is Pinecone for?\n",
    "Often, there is a technical requirement to run a comparison of one vector to millions of others and return the most similar results in real time, with a latency of tens of milliseconds and at a high throughput. Pinecone solves this  problem with its managed vector database service, and we will demonstrate this below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46933021",
   "metadata": {},
   "source": [
    "### Prepare vector embeddings for upload\n",
    "\n",
    "This may take a while depending on your machine. If on a recent MacBookPro or Google Colab, this may take up to one hour, sometimes longer.\n",
    "\n",
    "#### Prepare metadata\n",
    "\n",
    "The function below creates metadata from a single row of the dataframe. This is going to be important further down this notebook for additional filter requirements we will may want to employ in our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330d389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_metadata_from_dataframe_row(df_row):\n",
    "    \"\"\"Return pinecone vector.\"\"\"\n",
    "    vector_metadata = {\n",
    "        'year': df_row['year'],\n",
    "        'month': df_row['month'],\n",
    "        'round': df_row['round'],\n",
    "        'amount': df_row['amount']\n",
    "    }\n",
    "    return vector_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e0478",
   "metadata": {},
   "source": [
    "#### Prepare all vector data for upload\n",
    "\n",
    "The function below will take a portion of the dataframe and create the full vector data as Pinecone expects it for [upsert](https://www.pinecone.io/docs/insert-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c3862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_to_upload_to_pinecone(df_chunk, model):\n",
    "    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n",
    "    # create embeddings\n",
    "    pool = model.start_multi_process_pool()\n",
    "    vector_values = model.encode_multi_process(df_chunk['text_to_encode'], pool).tolist()\n",
    "    model.stop_multi_process_pool(pool)\n",
    "    # create vector ids and metadata\n",
    "    vector_ids = df_chunk.index.tolist()\n",
    "    vector_metadata = df_chunk.apply(get_vector_metadata_from_dataframe_row,axis=1).tolist()\n",
    "    return list(zip(vector_ids, vector_values, vector_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cc6d7",
   "metadata": {},
   "source": [
    "### Upload data to Pinecone in asynchronous batches\n",
    "\n",
    "The function below iterates through the dataframe in chunks, and for each of those chunks, will upload asynchronously in sub-chunks to your Pinecone Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256e593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_pinecone_in_chunks(\n",
    "    dataframe, \n",
    "    pinecone_index, \n",
    "    model, \n",
    "    chunk_size=20000, \n",
    "    upsert_size=500):\n",
    "    \"\"\"Encode dataframe column `text_to_encode` to dense vector and upsert to Pinecone.\"\"\"\n",
    "    tqdm_kwargs = h.get_tqdm_kwargs(dataframe, chunk_size)\n",
    "    async_results = collections.defaultdict(list)\n",
    "    for df_chunk in tqdm.notebook.tqdm(h.chunks(dataframe, chunk_size), **tqdm_kwargs):\n",
    "        vectors = get_vectors_to_upload_to_pinecone(df_chunk, model)\n",
    "        # upload to Pinecone in batches of `upsert_size`\n",
    "        for vectors_chunk in h.chunks(vectors, upsert_size):\n",
    "            start_index_chunk = df_chunk.index[0]\n",
    "            async_result = pinecone_index.upsert(vectors_chunk, async_req=True)\n",
    "            async_results[start_index_chunk].append(async_result)\n",
    "        # wait for results\n",
    "        _ = [async_result.get() for async_result in async_results[start_index_chunk]]\n",
    "        is_all_successful = all(map(lambda x: x.successful(), async_results[start_index_chunk]))\n",
    "        # report chunk upload status\n",
    "        print(\n",
    "        f'All upserts in chunk successful with index starting with {start_index_chunk:>7}: '\n",
    "        f'{is_all_successful}. Vectors uploaded: {len(vectors):>3}.'\n",
    "        )\n",
    "    return async_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ef5c8",
   "metadata": {},
   "source": [
    "#### Asynchronous Upload\n",
    "Computing the embeddings may take up to an hour depending on hardware capabilities. The Pinecone API responds right away with its [async](https://www.pinecone.io/docs/insert-data/#sending-upserts-in-parallel) requests. \n",
    "\n",
    "Note: You may see the following output a few times when executing, it is not an error: \n",
    "```\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba75643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# async_results = upload_dataframe_to_pinecone_in_chunks(df, index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eddac13",
   "metadata": {},
   "source": [
    "### Visualize the status of your upserts in the Pinecone Console\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/startakovsky/pinecone-examples-fork/may-2022-semantic-text-search-refresh/semantic_text_search/pinecone_console.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6de60",
   "metadata": {},
   "source": [
    "## Querying Pinecone\n",
    "\n",
    "Now that all the embeddings of the texts are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast semantic search query capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5033f",
   "metadata": {},
   "source": [
    "### Pinecone Example Usage\n",
    "\n",
    "#### _**Show me Jeopardy questions that are semantically similar to \"ancient attitudes\"\\!**_\n",
    "\n",
    "In the below example we query Pinecone's API with an embedding of a query term to return the vector embeddings that have the highest similarity score. In other words, Pinecone does all the work to effeciently determine which of the uploaded vector embeddings have the highest similarity when paired with the query term's embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c62f6d",
   "metadata": {},
   "source": [
    "#### Example: Pinecone API Request\n",
    "\n",
    "A sample request for questions that that a similar semantic meaning to _ancient attitudes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f6d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'ancient attitudes'\n",
    "vector_embedding = model.encode(query).tolist()\n",
    "response = index.query([vector_embedding], top_k=3, include_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460d6f6",
   "metadata": {},
   "source": [
    "#### Pinecone API Response\n",
    "A typical Pinecone response to the above query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0947f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{'matches': [],\n",
       " 'namespace': '',\n",
       " 'results': [{'matches': [{'id': '131253',\n",
       "                           'metadata': {'amount': 1200.0,\n",
       "                                        'month': '01',\n",
       "                                        'round': 'Double Jeopardy!',\n",
       "                                        'year': '2006'},\n",
       "                           'score': 0.510472894,\n",
       "                           'values': []},\n",
       "                          {'id': '116689',\n",
       "                           'metadata': {'amount': 1600.0,\n",
       "                                        'month': '04',\n",
       "                                        'round': 'Double Jeopardy!',\n",
       "                                        'year': '2006'},\n",
       "                           'score': 0.500562906,\n",
       "                           'values': []},\n",
       "                          {'id': '71462',\n",
       "                           'metadata': {'amount': 800.0,\n",
       "                                        'month': '05',\n",
       "                                        'round': 'Double Jeopardy!',\n",
       "                                        'year': '2007'},\n",
       "                           'score': 0.460552633,\n",
       "                           'values': []}],\n",
       "              'namespace': ''}]}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A hacky way to print the response object in color.\n",
    "h.printmd(f\"```python\\n{response}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93ba5f",
   "metadata": {},
   "source": [
    "#### Enriched Response\n",
    "To show which questions we retreived, the above response needs to be enriched using the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35cff62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>amount</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131253</th>\n",
       "      <td>'Modern cultural movement emphasizing alternative approaches to spirituality'</td>\n",
       "      <td>New Age</td>\n",
       "      <td>1200</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116689</th>\n",
       "      <td>'The god Sobek of this ancient culture was often depicted as a crocodile wearing a headdress'</td>\n",
       "      <td>Ancient Egypt</td>\n",
       "      <td>1600</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71462</th>\n",
       "      <td>'An ancient language:&lt;br /&gt;Skt.'</td>\n",
       "      <td>Sanskrit</td>\n",
       "      <td>800</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                question  \\\n",
       "vector_id                                                                                                  \n",
       "131253                     'Modern cultural movement emphasizing alternative approaches to spirituality'   \n",
       "116689     'The god Sobek of this ancient culture was often depicted as a crocodile wearing a headdress'   \n",
       "71462                                                                   'An ancient language:<br />Skt.'   \n",
       "\n",
       "                  answer  amount              query  \n",
       "vector_id                                            \n",
       "131253           New Age    1200  ancient attitudes  \n",
       "116689     Ancient Egypt    1600  ancient attitudes  \n",
       "71462           Sanskrit     800  ancient attitudes  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.get_query_results_from_api_response(df, response, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108f73",
   "metadata": {},
   "source": [
    "### Pinecone Example Usage With [Metadata](https://www.pinecone.io/docs/metadata-filtering/)\n",
    "\n",
    "The above questions do turn out to be related to _ancient attidues_, which is pretty spectacular! _Note that this is **not a keyword search** but rather a **search for semantically similar results**.\n",
    "\n",
    "But we can do better! In Jeopardy, you choose a question **and** a price point, where, in general, higher prices indicate harder questions. So it is natural to want to choose semantically similar questions from a specific price point.\n",
    "\n",
    "#### _Can I see 5 questions related to \"ancient attitudes\" for \\\\$1000?_\n",
    "\n",
    "Yes. Pinecone's [metadata feature](https://www.pinecone.io/docs/metadata-filtering/) makes this request trivial. We've already uploaded the metadata so filtering is just a Pinecone API request away. The only difference we make to the api request is to add the `filter_criteria` keyword argument like so: \n",
    "\n",
    "```python\n",
    "index.query([vector_embedding], top_k=5, include_metadata=True, filter_criteria={'amount': {'$eq': 1000}})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd61f365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>amount</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22432</th>\n",
       "      <td>'The name of this most recent geological era is from the Greek for \"new animals\"'</td>\n",
       "      <td>Cenozoic</td>\n",
       "      <td>1000</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189904</th>\n",
       "      <td>'It's the only ancient wonder that fits the category'</td>\n",
       "      <td>Lighthouse at Alexandria</td>\n",
       "      <td>1000</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177958</th>\n",
       "      <td>'Language of ancient India that's related to Greek &amp; Latin'</td>\n",
       "      <td>Sanskrit</td>\n",
       "      <td>1000</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105535</th>\n",
       "      <td>'Ancient priests checked these animal innards for signs &amp; portents'</td>\n",
       "      <td>entrails</td>\n",
       "      <td>1000</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139436</th>\n",
       "      <td>'This \"elder\" Roman writer died in 79 A.D. while rescuing people from mount Vesuvius' eruption'</td>\n",
       "      <td>Pliny (the Elder)</td>\n",
       "      <td>1000</td>\n",
       "      <td>ancient attitudes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  question  \\\n",
       "vector_id                                                                                                    \n",
       "22432                    'The name of this most recent geological era is from the Greek for \"new animals\"'   \n",
       "189904                                               'It's the only ancient wonder that fits the category'   \n",
       "177958                                         'Language of ancient India that's related to Greek & Latin'   \n",
       "105535                                 'Ancient priests checked these animal innards for signs & portents'   \n",
       "139436     'This \"elder\" Roman writer died in 79 A.D. while rescuing people from mount Vesuvius' eruption'   \n",
       "\n",
       "                             answer  amount              query  \n",
       "vector_id                                                       \n",
       "22432                      Cenozoic    1000  ancient attitudes  \n",
       "189904     Lighthouse at Alexandria    1000  ancient attitudes  \n",
       "177958                     Sanskrit    1000  ancient attitudes  \n",
       "105535                     entrails    1000  ancient attitudes  \n",
       "139436            Pliny (the Elder)    1000  ancient attitudes  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.get_query_results_from_query(query, index, df, model, top_k=5, filter_criteria={'amount': {'$eq': 1000}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2fdb6",
   "metadata": {},
   "source": [
    "Pretty good, right? Every question above is from the dataset, each one of them previously aired on Jeopardy for \\\\$1000, and, while subjective, most of them have to do with _antient attidues_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2442b3",
   "metadata": {},
   "source": [
    "### An Additional Note on Metadata\n",
    "\n",
    "This is a basic demonstration of metadata. Extensive predicate logic can be applied to metadata filtering, just like the [WHERE clause](https://www.pinecone.io/learn/vector-search-filtering/) in SQL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a42e36",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how trivial Pinecone makes instant retrieval of similar vector embeddings to create custom Jeopardy questions of a pre-assigned difficulty. We did not need to train any models or develop any algorithms to allow for this type of instant computation. This example is illustrative of how to use a pre-trained transformer-encoder model with Pinecone to achieve realtime similarity retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fed76d",
   "metadata": {},
   "source": [
    "### Like what you see? Explore our [community](https://www.pinecone.io/community/)\n",
    "Learn more about semantic search and the rich, performant, and production-level feature set of Pinecone's Vector Database by visiting https://pinecone.io, connect with us [here](https://www.pinecone.io/contact/) and [follow us](https://www.linkedin.com/company/pinecone-io) on LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8f76c",
   "metadata": {},
   "source": [
    "## Bonus Material: Jeopardy Building Custom Jeopardy Boards\n",
    "\n",
    "For the interested reader, we've created a few functions in the helper module that will automatically generate Jeopardy Boards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553130a4",
   "metadata": {},
   "source": [
    "### Pinecone Query for All Question Difficulties\n",
    "Now, we scale up the previous example and wrangle the output into the form of two Jeopardy Boards (First and second round)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480cf1b",
   "metadata": {},
   "source": [
    "### Jeopardy! Round 1 Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860e5675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>over the moon</th>\n",
       "      <th>ancient atitudes</th>\n",
       "      <th>invention of computer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>'Any song about Earth's natural satellite'</td>\n",
       "      <td>'Etruscan civilization had gone past tense by the 453 A.D. wedding-night death of this Hun leader'</td>\n",
       "      <td>'The Woz, Steve Wozniak, built the first computer for this company'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>'The only moon in our solar system that astrologists say has an influence circles this planet'</td>\n",
       "      <td>'In Arthurian legend, the sword in the stone was stuck in one of these blacksmith aids on top of the stone'</td>\n",
       "      <td>'On April 1, 1976 2 engineers with $1,300 in capital began this computer company in Cupertino, California'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>'On July 20, 1969 Ryan Seacrest described the lunar surface as \"magnificent desolation\"; 2nd man on Moon, out!'</td>\n",
       "      <td>'(&lt;a href=\"http://www.j-archive.com/media/2010-02-05_J_28.wmv\"&gt;Jimmy of the Clue Crew reports from Israel.&lt;/a&gt;)  The Church of the Beatitudes is on the hilltop long considered the site where Jesus delivered this, which contained the Beatitudes'</td>\n",
       "      <td>'Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>'The Apollo lunar mission with this number was aborted en route to the Moon in 1970 due to an in-flight explosion'</td>\n",
       "      <td>'According to the Beatitudes, this group shall \"inherit the Earth\"'</td>\n",
       "      <td>'Originally an adding machine maker, in 1944 IBM made its first steps toward one of these with the Harvard Mark I'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>'You could say we sent this Greek god to the moon in 1969'</td>\n",
       "      <td>'The aye-aye'</td>\n",
       "      <td>'Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             over the moon  \\\n",
       "amount                                                                                                                       \n",
       "200                                                                             'Any song about Earth's natural satellite'   \n",
       "400                         'The only moon in our solar system that astrologists say has an influence circles this planet'   \n",
       "600        'On July 20, 1969 Ryan Seacrest described the lunar surface as \"magnificent desolation\"; 2nd man on Moon, out!'   \n",
       "800     'The Apollo lunar mission with this number was aborted en route to the Moon in 1970 due to an in-flight explosion'   \n",
       "1000                                                            'You could say we sent this Greek god to the moon in 1969'   \n",
       "\n",
       "                                                                                                                                                                                                                                            ancient atitudes  \\\n",
       "amount                                                                                                                                                                                                                                                         \n",
       "200                                                                                                                                                       'Etruscan civilization had gone past tense by the 453 A.D. wedding-night death of this Hun leader'   \n",
       "400                                                                                                                                              'In Arthurian legend, the sword in the stone was stuck in one of these blacksmith aids on top of the stone'   \n",
       "600     '(<a href=\"http://www.j-archive.com/media/2010-02-05_J_28.wmv\">Jimmy of the Clue Crew reports from Israel.</a>)  The Church of the Beatitudes is on the hilltop long considered the site where Jesus delivered this, which contained the Beatitudes'   \n",
       "800                                                                                                                                                                                      'According to the Beatitudes, this group shall \"inherit the Earth\"'   \n",
       "1000                                                                                                                                                                                                                                           'The aye-aye'   \n",
       "\n",
       "                                                                                                     invention of computer  \n",
       "amount                                                                                                                      \n",
       "200                                                    'The Woz, Steve Wozniak, built the first computer for this company'  \n",
       "400             'On April 1, 1976 2 engineers with $1,300 in capital began this computer company in Cupertino, California'  \n",
       "600           'Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401'  \n",
       "800     'Originally an adding machine maker, in 1944 IBM made its first steps toward one of these with the Harvard Mark I'  \n",
       "1000                      'Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes'  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"over the moon\", \"ancient atitudes\", \"invention of computer\"]\n",
    "jeopardy_questions = h.get_jeopardy_questions(queries, index, df, model)\n",
    "jeopardy_board, double_jeopardy_board = h.get_jeopardy_boards(jeopardy_questions, queries)\n",
    "jeopardy_board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40f4d1",
   "metadata": {},
   "source": [
    "### Double Jeopardy! Round 2 Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1877d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>over the moon</th>\n",
       "      <th>ancient atitudes</th>\n",
       "      <th>invention of computer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>'High-aiming standard heard here'</td>\n",
       "      <td>'She wrote the classic anthropology text \"Coming of Age in Samoa\"'</td>\n",
       "      <td>'Long before it was the name of a computer, it was Brit speak for a raincoat'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>'It's the name shared by an Irish revolutionary &amp; the pilot of the command module during the first moon landing'</td>\n",
       "      <td>'Adopted by an Irish regiment, Kipling's Kimball O'Hara was from this country'</td>\n",
       "      <td>'In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>'The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon &amp; the sun'</td>\n",
       "      <td>'Traditional adjective for saintly 8th century historian Bede'</td>\n",
       "      <td>'As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>'On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1'</td>\n",
       "      <td>'Keats' \"Ode on\" this says, \"Sylvan historian, who canst thus express a flowery tale more sweetly than our rhyme\"'</td>\n",
       "      <td>'This technology used for wireless headsets is named after a Danish king who united parts of Scandinavia'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>'Lunar object over a Gulf of Guinea nation'</td>\n",
       "      <td>'Ancient Roman name for the region of Scotland'</td>\n",
       "      <td>'The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             over the moon  \\\n",
       "amount                                                                                                                       \n",
       "1200                                                                                     'High-aiming standard heard here'   \n",
       "1400      'It's the name shared by an Irish revolutionary & the pilot of the command module during the first moon landing'   \n",
       "1600             'The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon & the sun'   \n",
       "1800    'On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1'   \n",
       "2000                                                                           'Lunar object over a Gulf of Guinea nation'   \n",
       "\n",
       "                                                                                                          ancient atitudes  \\\n",
       "amount                                                                                                                       \n",
       "1200                                                    'She wrote the classic anthropology text \"Coming of Age in Samoa\"'   \n",
       "1400                                        'Adopted by an Irish regiment, Kipling's Kimball O'Hara was from this country'   \n",
       "1600                                                        'Traditional adjective for saintly 8th century historian Bede'   \n",
       "1800    'Keats' \"Ode on\" this says, \"Sylvan historian, who canst thus express a flowery tale more sweetly than our rhyme\"'   \n",
       "2000                                                                       'Ancient Roman name for the region of Scotland'   \n",
       "\n",
       "                                                                                                         invention of computer  \n",
       "amount                                                                                                                          \n",
       "1200                                             'Long before it was the name of a computer, it was Brit speak for a raincoat'  \n",
       "1400    'In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant'  \n",
       "1600           'As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne'  \n",
       "1800                 'This technology used for wireless headsets is named after a Danish king who united parts of Scandinavia'  \n",
       "2000                'The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market'  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_jeopardy_board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50b53e",
   "metadata": {},
   "source": [
    "### Looking Up Answers!\n",
    "See if you can think of the answer to this question which you can view in the above jeopardy board: \n",
    "#### _Over the moon for 400 please, Alex!_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "603c7b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667c1f39987b4380ad075af7c90ea18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='query:', options=('over the moon', 'ancient atitudes', 'invention of computer'), value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bad15c29cae4082a87a355234e0f8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='amount:', options=(200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000), value=200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2269caec70a64d82b218fc179515add2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a92bbbd4ad4448a7afe7c1dae23637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h.show_answer_widget(jeopardy_questions, queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
