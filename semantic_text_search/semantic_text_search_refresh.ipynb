{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a87700e2",
      "metadata": {},
      "source": [
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/startakovsky/pinecone-examples-fork/blob/master/semantic_text_search/semantic_text_search_refresh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df24e16b",
      "metadata": {},
      "source": [
        "# Semantic Text Search Demo, with Pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8253e2",
      "metadata": {},
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8010267d",
      "metadata": {},
      "source": [
        "### What is Semantic Search?\n",
        "\n",
        "_Semantic search_ is exactly the kind of search where the _meaning_ of the search query is the thing that's used, rather than it being done by keyword lookups. Pretrained neural networks on large sets of text data have been shown to be very effective at encoding the _meaning_ of a particular phrase, sentence, paragraph or long document into a data structure known as a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68935be8",
      "metadata": {},
      "source": [
        "### How will we demonstrate and apply this example.\n",
        "\n",
        "We are going to use Pinecone's semantic search capabilities with an off-the-shelf and a pretrained model to curate custom categories of previously-aired Jeopardy questions. We will show how Pinecone makes it easy to ensure that question difficulty is on par with how the question was originally priced by filtering on question metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a228d497",
      "metadata": {},
      "source": [
        "### Learning Goals\n",
        "_By the end of this demo, you will have:_\n",
        " 1. Learned about Pinecone's value for solving realtime semantic search requirements!\n",
        " 2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.\n",
        " 3. Encoded Jeopardy Questions as 384-dimensional vectors using a pretrained, encoder-only, model (i.e. no model training necessary).\n",
        " 4. Queried Pinecone's Vector Database on Jeopardy Questions that are semantically similar to the query.\n",
        " 5. Used Pinecone's metadata filtering capability to ensure that each category you create will be on parity with difficulty used when the question originally aired (each category will contain questions of ranging difficulty)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a99b115",
      "metadata": {},
      "source": [
        "## Setup: Prerequisites and Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26157e6b",
      "metadata": {},
      "source": [
        "### Download the Data\n",
        "Download [this Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) from Kaggle and place it in a `tmp` folder relative to this notebook (see `CSV_FILEPATH` variable defined below). A free Kaggle account is required, and you will be prompted to sign-in or create an account before download can start."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60687f1",
      "metadata": {},
      "source": [
        "### Python 3.7+\n",
        "\n",
        "This code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4382e8",
      "metadata": {},
      "source": [
        "### Importing the helper modules\n",
        "\n",
        "This notebook is self-contained, and as such, if background Python modules are not present, they will be imported from [Pinecone's Example repository](https://github.com/pinecone-io/examples/tree/master/semantic_text_search)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5609277d",
      "metadata": {},
      "source": [
        "### Acquiring your Pinecone API Key\n",
        "\n",
        "A Pinecone API key is required. You can obtain one for free on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96cb344b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Extracting API Key from environmental variable `PINECONE_EXAMPLE_API_KEY`..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Pinecone API Key available at `h.pinecone_api_key`"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import httpimport\n",
        "\n",
        "if os.path.isfile('helper.py'):\n",
        "    import helper as h\n",
        "else:\n",
        "    print('importing `helper.py` from https://github.com/pinecone-io')\n",
        "    with httpimport.github_repo(\n",
        "        username='startakovsky', \n",
        "        repo='pinecone-examples-fork',\n",
        "        module=['semantic_text_search'],\n",
        "        branch='master'):\n",
        "        from semantic_text_search import helper as h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edfb9e5",
      "metadata": {},
      "source": [
        "### Installing and Importing Prerequisite Libraries:\n",
        "Python libraries [pinecone-client](https://pypi.org/project/pinecone-client/), [sentence_transformers](https://pypi.org/project/sentence-transformers/), [pandas](https://pypi.org/project/pandas/), and [tqdm](https://pypi.org/project/tqdm/) are required for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a12400a",
      "metadata": {},
      "source": [
        "#### Installing via `pip`\n",
        "The next line is equivalent to `pip install pinecone-client sentence-transformers pandas tqdm`. Note that _sys.executable_ is a way of ensuring it's the version of pip associated with this Jupyter Notebook's Python kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a1fa96d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install pinecone-client sentence-transformers pandas tqdm -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9441c1df",
      "metadata": {},
      "source": [
        "#### Importing and Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7281780",
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import tqdm\n",
        "import pinecone\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "CSV_FILEPATH = './tmp/jeopardy_csv.zip'\n",
        "INDEX_NAME, INDEX_DIMENSION = 'semantic-text-search', 384\n",
        "MODEL_NAME = 'sentence-transformers/msmarco-MiniLM-L6-cos-v5'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f916fa",
      "metadata": {},
      "source": [
        " ### Processing Data\n",
        "The preprocessing step is self-explanatory and defined in the helper module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3e83a3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(CSV_FILEPATH)\n",
        "df = h.get_processed_df(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc22766",
      "metadata": {},
      "source": [
        "#### Sample row from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fff4d03e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>146354</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>show_id</th>\n",
              "      <td>5330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <td>2007-11-09 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>round</th>\n",
              "      <td>Double Jeopardy!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <td>AUTOBIOGRAPHERS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amount</th>\n",
              "      <td>1200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>A psychologist: 1962's \"Memories, Dreams, Reflections\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <td>Carl Jung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text_to_encode</th>\n",
              "      <td>A psychologist: 1962's \"Memories, Dreams, Reflections\" Carl Jung</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                          146354\n",
              "show_id                                                                     5330\n",
              "date                                                         2007-11-09 00:00:00\n",
              "round                                                           Double Jeopardy!\n",
              "category                                                         AUTOBIOGRAPHERS\n",
              "amount                                                                      1200\n",
              "question                  A psychologist: 1962's \"Memories, Dreams, Reflections\"\n",
              "answer                                                                 Carl Jung\n",
              "year                                                                        2007\n",
              "month                                                                         11\n",
              "text_to_encode  A psychologist: 1962's \"Memories, Dreams, Reflections\" Carl Jung"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(df.iloc[123456])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e57c70f",
      "metadata": {},
      "source": [
        "### Creating your Pinecone Index\n",
        "The process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector. As we will see below, the model we are using maps each piece of text to a 384-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7d03750c",
      "metadata": {},
      "outputs": [],
      "source": [
        "pinecone.init(api_key=h.pinecone_api_key, environment='us-west1-gcp')\n",
        "# pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION))\n",
        "index = pinecone.Index(index_name=INDEX_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546f0219",
      "metadata": {},
      "source": [
        "## Generate embeddings and send them to your Pinecone Index\n",
        "This will all be done in batches. We will compute embeddings in batch, followed by taking each batch and sending it to Pinecone, also in batches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecbb5e8",
      "metadata": {},
      "source": [
        "### Loading a Pretrained Encoder model.\n",
        "We will generate embeddings by using [this Sentence Transformers model](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5). It is one of hundreds encoder models available. Downloads happen automatically with SentenceTransformer, and may take up to a minute the first time. After this first import, the model is cached and available on a local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4ed4a8c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Loading model from _Sentence Transformers_: `sentence-transformers/msmarco-MiniLM-L6-cos-v5` from Sentence Transformers..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Model loaded."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "h.printmd(f'Loading model from _Sentence Transformers_: `{MODEL_NAME}` from Sentence Transformers...')\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "h.printmd('Model loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4d4005",
      "metadata": {},
      "source": [
        "### MSMARCO model v5 and Embeddings\n",
        "\n",
        "In this example, we created an index with 384 dimensions because that is what the output is of this MSMARCO model. In fact, particular MSMARCO model used in this example generates [unit vectors](https://en.wikipedia.org/wiki/Unit_vector), which make [vector comparisons](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) agnostic to one's choice of similarity scores. In other words, when defining the index, it does not matter whether we use `euclidean`, `cosine` or `dotproduct` as a metric, so we left it blank, using the `cosine` default. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a13a56",
      "metadata": {},
      "source": [
        "#### On Embeddings\n",
        "\n",
        "The output of this model's encodings are 384-dimensional, which was known in advance of creating above index.\n",
        "\n",
        "So, when a piece of text such as \"A quick fox jumped around\" gets encoded into a vector embedding, the result is a sequence of floats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb16d277",
      "metadata": {},
      "source": [
        "#### On Comparing Embeddings aka _how_ Semantic Search works\n",
        "\n",
        "Two 15-dimensional text embeddings might look like something like: \n",
        " - _\\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\]_\n",
        " - _\\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]_\n",
        " \n",
        "In order to determine how _similar_ we may use something like [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity). This calculation is trivial when comparing two vectors, but nontrivial when needing to compare one vector against millions or billions of vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d71e442",
      "metadata": {},
      "source": [
        "### What is Pinecone for?\n",
        "Often, there is a technical requirement to run a comparison of one vector to millions of others and return the most similar results in real time, with a latency of tens of milliseconds and at a high throughput. Pinecone solves this  problem with its managed vector database service, and we will demonstrate this below. Additionally, Pinecone offers the ability to filter by metadata as well as providing high-availability replication that will scale up (and down) as needed. We will demonstrate the metadata capability below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46933021",
      "metadata": {},
      "source": [
        "### Prepare vector embeddings for upload\n",
        "\n",
        "This may take a while depending on your machine. If on a recent MacBookPro or Google Colab, this may take up to one hour, sometimes longer.\n",
        "\n",
        "#### Prepare metadata\n",
        "\n",
        "The function below creates metadata from a single row of the dataframe. We will demonstrate Pinecone's ability to filter by this metadata shortly. This is going to be important for forming the Jeopardy Categories with questions ranging from `$200` up to `$2000` difficulty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "330d389c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vector_metadata_from_dataframe_row(df_row):\n",
        "    \"\"\"Return pinecone vector.\"\"\"\n",
        "    vector_metadata = {\n",
        "        'year': df_row['year'],\n",
        "        'month': df_row['month'],\n",
        "        'round': df_row['round'],\n",
        "        'amount': df_row['amount']\n",
        "    }\n",
        "    return vector_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1e0478",
      "metadata": {},
      "source": [
        "#### Prepare all vector data for upload\n",
        "\n",
        "The function below will take a portion of the dataframe and create the full vector data as Pinecone expects it for [upsert](https://www.pinecone.io/docs/insert-data/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "40c3862c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vectors_to_upload_to_pinecone(df_chunk, model):\n",
        "    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n",
        "    # create embeddings\n",
        "    pool = model.start_multi_process_pool()\n",
        "    vector_values = model.encode_multi_process(df_chunk['text_to_encode'], pool).tolist()\n",
        "    model.stop_multi_process_pool(pool)\n",
        "    # create vector ids and metadata\n",
        "    vector_ids = df_chunk.index.tolist()\n",
        "    vector_metadata = df_chunk.apply(get_vector_metadata_from_dataframe_row,axis=1).tolist()\n",
        "    return list(zip(vector_ids, vector_values, vector_metadata))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "040cc6d7",
      "metadata": {},
      "source": [
        "### Upload data to Pinecone in asynchronous batches\n",
        "\n",
        "The function below iterates through the dataframe in chunks, and for each of those chunks, will upload asynchronously in sub-chunks to your Pinecone Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "256e593c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_dataframe_to_pinecone_in_chunks(\n",
        "    dataframe, \n",
        "    pinecone_index, \n",
        "    model, \n",
        "    chunk_size=20000, \n",
        "    upsert_size=500):\n",
        "    \"\"\"Encode dataframe column `text_to_encode` to dense vector and upsert to Pinecone.\"\"\"\n",
        "    tqdm_kwargs = h.get_tqdm_kwargs(dataframe, chunk_size)\n",
        "    async_results = collections.defaultdict(list)\n",
        "    for df_chunk in tqdm.notebook.tqdm(h.chunks(dataframe, chunk_size), **tqdm_kwargs):\n",
        "        vectors = get_vectors_to_upload_to_pinecone(df_chunk, model)\n",
        "        # upload to Pinecone in batches of `upsert_size`\n",
        "        for vectors_chunk in h.chunks(vectors, upsert_size):\n",
        "            start_index_chunk = df_chunk.index[0]\n",
        "            async_result = pinecone_index.upsert(vectors_chunk, async_req=True)\n",
        "            async_results[start_index_chunk].append(async_result)\n",
        "        # wait for results\n",
        "        _ = [async_result.get() for async_result in async_results[start_index_chunk]]\n",
        "        is_all_successful = all(map(lambda x: x.successful(), async_results[start_index_chunk]))\n",
        "        # report chunk upload status\n",
        "        print(\n",
        "        f'All upserts in chunk successful with index starting with {start_index_chunk:>7}: '\n",
        "        f'{is_all_successful}. Vectors uploaded: {len(vectors):>3}.'\n",
        "        )\n",
        "    return async_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091ef5c8",
      "metadata": {},
      "source": [
        "#### Asynchronous Upload\n",
        "Computing the embeddings may take up to an hour depending on hardware capabilities. The Pinecone API responds right away with the status of each request. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5ba75643",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# async_results = upload_dataframe_to_pinecone_in_chunks(df, index, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eddac13",
      "metadata": {},
      "source": [
        "### Visualize the status of your upserts in the Pinecone Console\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/startakovsky/pinecone-examples-fork/may-2022-semantic-text-search-refresh/semantic_text_search/pinecone_console.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f6de60",
      "metadata": {},
      "source": [
        "## Querying Pinecone\n",
        "\n",
        "Now that all the embeddings of the texts are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast semantic search query capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651b95d3",
      "metadata": {},
      "source": [
        "### Helper Functions\n",
        "Below we have some helper functions, that convert text to a vector embedding, send the vector to Pinecone along with a metadata rule of how to filter the request, and converting the Pinecone API JSON response to a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2fbd6b47",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_query_ids_map_from_response(response, queries):\n",
        "    \"\"\"Return pandas.DataFrame containing query_results.\"\"\"\n",
        "    num_queries = len(queries)\n",
        "    matches = [response['results'][i]['matches'] for i in range(len(queries))]\n",
        "    return {query: map(lambda x: x.get('id'), matches[i]) for i, query in enumerate(queries)}\n",
        "\n",
        "def get_query_results_from_api_response(dataframe, response, queries):\n",
        "    \"\"\"Return pandas.DataFrame containing query_results with original text.\"\"\"\n",
        "    df_list = []\n",
        "    query_ids_map = get_query_ids_map_from_response(response, queries)\n",
        "    for query, ids in query_ids_map.items():\n",
        "        single_query_response_df = dataframe.loc[ids, ['question', 'answer', 'amount']]\n",
        "        single_query_response_df['query'] = query\n",
        "        df_list.append(single_query_response_df)\n",
        "    enriched_response_df = pd.concat(df_list)\n",
        "    return enriched_response_df\n",
        "\n",
        "def get_query_results_from_queries(queries, pinecone_index, dataframe, model, filter_criteria=None):\n",
        "    embedding = model.encode(queries).tolist()\n",
        "    response = pinecone_index.query(\n",
        "        embedding, \n",
        "        top_k=1, \n",
        "        filter=filter_criteria,\n",
        "        include_metadata=True,\n",
        "    )\n",
        "    return get_query_results_from_api_response(df, response, queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a05ea49",
      "metadata": {},
      "source": [
        "### Example Usage for a single query\n",
        "\n",
        "In the below example we get the following response from Pinecone that the above helper functions convert to a Pandas DataFrame. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ee4dc9",
      "metadata": {},
      "source": [
        "#### Pinecone Example Request\n",
        "\n",
        "Note how the [metadata](https://www.pinecone.io/learn/vector-search-filtering/) is taken into account because here we were looking for questions of `$600` difficulty.\n",
        "\n",
        "```python\n",
        "pinecone_index.query(\n",
        "        [embedding1, embedding2], \n",
        "        top_k=1, \n",
        "        filter={'amount': {'$eq': 600}},\n",
        "        include_metadata=True,\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9737b87a",
      "metadata": {},
      "source": [
        "#### Pinecone Example Response\n",
        "\n",
        "```\n",
        "{'results': [{'matches': [{'id': '45481',\n",
        "                           'metadata': {'amount': 600.0,\n",
        "                                        'month': '03',\n",
        "                                        'round': 'Jeopardy!',\n",
        "                                        'year': '2007'},\n",
        "                           'score': 0.59718585,\n",
        "                           'values': []}],\n",
        "              'namespace': ''},\n",
        "             {'matches': [{'id': '6247',\n",
        "                           'metadata': {'amount': 600.0,\n",
        "                                        'month': '10',\n",
        "                                        'round': 'Jeopardy!',\n",
        "                                        'year': '2007'},\n",
        "                           'score': 0.480220914,\n",
        "                           'values': []}],\n",
        "              'namespace': ''}]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c93ba5f",
      "metadata": {},
      "source": [
        "#### Pinecone Enriched Example Response\n",
        "Do `love` and `ocean life` make sense based on the results? Note the ability to send one request to Pinecone's Vector Database for multiple queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ce0906ae",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>amount</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45481</th>\n",
              "      <td>A rose has long been a symbol of love; rearrange its letters &amp; you get the name of this Greek god of love</td>\n",
              "      <td>Eros</td>\n",
              "      <td>600</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6247</th>\n",
              "      <td>Check out exotic marine life at one of these, like the National one in Baltimore</td>\n",
              "      <td>an aquarium</td>\n",
              "      <td>600</td>\n",
              "      <td>ocean life</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                        question  \\\n",
              "45481  A rose has long been a symbol of love; rearrange its letters & you get the name of this Greek god of love   \n",
              "6247                            Check out exotic marine life at one of these, like the National one in Baltimore   \n",
              "\n",
              "            answer  amount       query  \n",
              "45481         Eros     600        love  \n",
              "6247   an aquarium     600  ocean life  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_query_results_from_queries(['love', 'ocean life'], index, df, model, filter_criteria={'amount': {'$eq': 600}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a8f76c",
      "metadata": {},
      "source": [
        "## Building Custom Jeopardy Boards\n",
        "\n",
        "Here is a summary of the following helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553130a4",
      "metadata": {},
      "source": [
        "### Pinecone Query for All Question Difficulties\n",
        "The following functions scale up the previous example, wrangles the output into the form of two Jeopardy rounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2fad697d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_jeopardy_questions(queries, pinecone_index, dataframe, model):\n",
        "    \"\"\"Return the questions to be used by making API requests to Pinecone.\"\"\"\n",
        "    df_list = []\n",
        "    get_result_from_amount = lambda amount: get_query_results_from_queries(\n",
        "        queries,\n",
        "        pinecone_index,\n",
        "        dataframe,\n",
        "        model,\n",
        "        filter_criteria={'amount': {'$eq': amount}}\n",
        "    )\n",
        "    for amount in h.JEOPARDY_STANDARD_AMOUNTS:\n",
        "        results = get_result_from_amount(amount)\n",
        "        df_list.append(results)\n",
        "    return pd.concat(df_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6c1cc5f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = [\"over the moon\", \"united states\", \"invention of computer\"]\n",
        "jeopardy_questions = get_jeopardy_questions(queries, index, df, model)\n",
        "jeopardy_board, double_jeopardy_board = h.get_jeopardy_boards(jeopardy_questions, queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4480cf1b",
      "metadata": {},
      "source": [
        "### Jeopardy! Round 1 Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "860e5675",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>over the moon</th>\n",
              "      <th>united states</th>\n",
              "      <th>invention of computer</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amount</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Any song about Earth's natural satellite</td>\n",
              "      <td>Canada, United States, Russia</td>\n",
              "      <td>The Woz, Steve Wozniak, built the first computer for this company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>The only moon in our solar system that astrologists say has an influence circles this planet</td>\n",
              "      <td>Number of U.S. states divided by the number of noncontiguous states</td>\n",
              "      <td>\"iWOZ\" tells how he went \"from computer geek to cult icon\" by inventing the personal computer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>U can't touch this lunar lander Neil Armstrong used; it was released &amp; crashed back into the moon</td>\n",
              "      <td>The westernmost country in North America</td>\n",
              "      <td>Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>Occurring the first new moon after the sun enters Aquarius, it's Vietnamese new year</td>\n",
              "      <td>Skyy</td>\n",
              "      <td>Originally a computer term, it now refers to doing any number of jobs at the same time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>You could say we sent this Greek god to the moon in 1969</td>\n",
              "      <td>It's the only U.S. state that fits the category</td>\n",
              "      <td>Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                            over the moon  \\\n",
              "amount                                                                                                      \n",
              "200                                                              Any song about Earth's natural satellite   \n",
              "400          The only moon in our solar system that astrologists say has an influence circles this planet   \n",
              "600     U can't touch this lunar lander Neil Armstrong used; it was released & crashed back into the moon   \n",
              "800                  Occurring the first new moon after the sun enters Aquarius, it's Vietnamese new year   \n",
              "1000                                             You could say we sent this Greek god to the moon in 1969   \n",
              "\n",
              "                                                              united states  \\\n",
              "amount                                                                        \n",
              "200                                           Canada, United States, Russia   \n",
              "400     Number of U.S. states divided by the number of noncontiguous states   \n",
              "600                                The westernmost country in North America   \n",
              "800                                                                    Skyy   \n",
              "1000                        It's the only U.S. state that fits the category   \n",
              "\n",
              "                                                                                             invention of computer  \n",
              "amount                                                                                                              \n",
              "200                                              The Woz, Steve Wozniak, built the first computer for this company  \n",
              "400                  \"iWOZ\" tells how he went \"from computer geek to cult icon\" by inventing the personal computer  \n",
              "600     Founded by Ross Perot in 1962, Electronic Data Systems got its first computer in 1965, this company's 1401  \n",
              "800                         Originally a computer term, it now refers to doing any number of jobs at the same time  \n",
              "1000                Announced on February 14, 1946, this first electronic digital computer had 18,000 vacuum tubes  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jeopardy_board"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc40f4d1",
      "metadata": {},
      "source": [
        "### Double Jeopardy! Round 2 Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1877d59a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>over the moon</th>\n",
              "      <th>united states</th>\n",
              "      <th>invention of computer</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amount</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>High-aiming standard heard here</td>\n",
              "      <td>The third largest</td>\n",
              "      <td>Long before it was the name of a computer, it was Brit speak for a raincoat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>It's the name shared by an Irish revolutionary &amp; the pilot of the command module during the first moon landing</td>\n",
              "      <td>Number of U.S. states that begin with the letter I</td>\n",
              "      <td>In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon &amp; the sun</td>\n",
              "      <td>On a map of the U.S., 1 of the 2 states that each border 8 other states</td>\n",
              "      <td>As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1</td>\n",
              "      <td>The Rio Grande forms part of the border between these 2 U.S. states</td>\n",
              "      <td>This metal used to make semiconductors was discovered by Clemens Winkler &amp; named for his homeland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>Lunar object over a Gulf of Guinea nation</td>\n",
              "      <td>Total number of U.S. states that begin and end with the same letter; (hint: they all start with vowels)</td>\n",
              "      <td>The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                           over the moon  \\\n",
              "amount                                                                                                                     \n",
              "1200                                                                                     High-aiming standard heard here   \n",
              "1400      It's the name shared by an Irish revolutionary & the pilot of the command module during the first moon landing   \n",
              "1600             The lunar kind means the light of the Moon is obscured because  the Earth is between the Moon & the sun   \n",
              "1800    On Feb. 1, 1958 the Detroit Free Press said, \"U.S. Fires Moon!\"; they meant the USA's first of these, Explorer 1   \n",
              "2000                                                                           Lunar object over a Gulf of Guinea nation   \n",
              "\n",
              "                                                                                                  united states  \\\n",
              "amount                                                                                                            \n",
              "1200                                                                                          The third largest   \n",
              "1400                                                         Number of U.S. states that begin with the letter I   \n",
              "1600                                    On a map of the U.S., 1 of the 2 states that each border 8 other states   \n",
              "1800                                        The Rio Grande forms part of the border between these 2 U.S. states   \n",
              "2000    Total number of U.S. states that begin and end with the same letter; (hint: they all start with vowels)   \n",
              "\n",
              "                                                                                                       invention of computer  \n",
              "amount                                                                                                                        \n",
              "1200                                             Long before it was the name of a computer, it was Brit speak for a raincoat  \n",
              "1400    In 1896 Herman Hollerith, born on Leap Day 1860, organized the Tabulating Machine Co., which evolved into this giant  \n",
              "1600           As a student at Eton, he did have his own laptop computer, despite being second in line to the British throne  \n",
              "1800                       This metal used to make semiconductors was discovered by Clemens Winkler & named for his homeland  \n",
              "2000                The Fly, the first pentop computer, is made by this company that \"jumped\" on the educational toys market  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "double_jeopardy_board"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb50b53e",
      "metadata": {},
      "source": [
        "### Looking Up Answers\n",
        "Before running the next cell, see if you can look up the answer to this question: \"Over the moon for 400\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7f4b69",
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "query, amount = \"over the moon\", 400\n",
        "id_ = jeopardy_questions.index[(jeopardy_questions['query'] == query) & (jeopardy_questions['amount'] == amount)][0]\n",
        "jeopardy_questions.at[id_, 'answer']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}